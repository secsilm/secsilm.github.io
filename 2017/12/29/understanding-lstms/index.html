<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        理解 LSTM 网络 · Lee&#39;s Space Station
        
    </title>
    <link rel="icon" href= /avatar/Bastion_cute.png>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.8);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171218 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <script>
        var _hmt = _hmt || [];
        (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?6e8b0c627bf164f809ee4346796b1952";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
    <!-- 谷歌统计  -->
    
    <script>
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'UA-111553531-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>
    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Lee&#39;s Space Station</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">理解 LSTM 网络</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Lee's Space Station</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(https://i.imgur.com/NSng4cv.png)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            理解 LSTM 网络
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = Machine Learning>Machine Learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = Translation>Translation</a>
    
</div>
            
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2017/12/29</span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <blockquote>
<p>译者注：</p>
<ul>
<li>本文原文为 Christopher Olah 于 2015 年发表在自己<a href="http://colah.github.io/" target="_blank" rel="noopener">博客</a>上的经典文章：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks — colah’s blog</a>。</li>
<li>没有翻译原文中的 Acknowledgments 部分，此部分为致谢，私以为无关。</li>
<li>文中括号或者引用块中的 <em>斜体字</em> 为对应的英文原文或者我自己注释的话（会标明 <em>译者注</em>），否则为原文中本来就有的话。</li>
<li>本人水平有限，如有错误欢迎指出。</li>
</ul>
</blockquote>
<h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>人们不会每一秒都从头开始思考。当你阅读这篇文章的时候，你根据前面的单词来理解后面的单词。你不会扔掉他们然后重新开始思考。你的思考具有持续性。</p>
<p>传统的神经网络做不到这一点，而且这似乎是一个主要缺点。例如，想象一下你想要对一个电影中每一帧所发生的事件类型进行分类，而传统神经网络却不能够利用先前的事情来推理后来的事情。</p>
<p>Recurrent Neural Networks（<em>译者注：以下简称 RNN</em>）解决了这个问题，他们在网络内部有循环，可以让信息具有持续性。</p>
<p><img src="https://i.imgur.com/A8appFv.png" height="180" alt="Recurrent Neural Networks have loops." )=""></p>
<center><font color="gray">RNN 中有循环</font></center>

<p>上图是神经网络的一部分，其中 $A$ 观察输入 $x_t$ 并输出一个值 $h_t$，循环使得信息可以从网络中的一步传递到下一步。</p>
<p>这些循环使得 RNN 看起来有些神秘。然而如果你再进一步想想，就会发现他们与普通的神经网络并不是完全不同。一个 RNN 可以被想成是对同一个网络的多次复制，每次都把信息传递给下一个。考虑一下如果我们把循环展开（<em>unroll</em>）会发生什么：</p>
<p><img src="https://i.imgur.com/0Wik9NF.png" alt="rnn-unrolled"></p>
<center><font color="gray">一个展开的 RNN</font></center>

<p>这种链状性质表明 RNN 与序列（<em>sequences</em>）和列表（<em>lists</em>）密切相关，在处理这种数据时他们是很自然的神经网络架构。</p>
<p>而且他们也确实在被使用！过去几年中，RNN 被应用于一系列的任务并取得了令人难以置信的成功：语音识别，语言模型，翻译，看图说话（<em>image captioning</em>）等等。你可以阅读 Andrej Karpathy 的精彩博文 —— <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 来了解通过 RNN 可以取得的惊人成绩。他们真的很棒。</p>
<p>这些成功的基础是使用 LSTMs，一种非常特别地 RNN，在许多任务中要比标准版本好。几乎所有基于 RNN 的令人激动的成绩都是通过他们获得的。这篇文章讨论的就是这些 LSTMs。</p>
<h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>RNNs 其中一个有吸引力的地方是能够将以前的信息和现在的任务联系起来，例如使用视频中的前几帧信息可能会对当前帧的理解有帮助。如果 RNNs 能够做到这些，那么他们就是非常有用的。但是他们能吗？这不一定。</p>
<p>有时，我们可能只需要最近的信息来完成当前任务。例如，考虑一个试图基于前面的词来预测下一个词的语言模型，如果我们试图预测 “the clouds are in the <em>sky</em>” 这句话中的最后一个词，那么我们就不需要更多的信息，很明显下一个词就是 sky。在这种情况下，相关信息和需要的地方（<em>the place that it’s needed</em>）之间的差距很小，那么这时候 RNNs 就可以学习到使用过去的信息。（<em>译者注：也就是短期依赖</em>）</p>
<p><img src="https://i.imgur.com/HAvvUQV.png" height="180)"></p>
<p>但是也有其他情况是我们需要更多信息的。考虑我们需要预测 “I grew up in France… I speak fluent <em>French</em>” 这句话中的最后一个词。最近的信息表明这个词应该是一个语言的名字，但是如果我们想要知道哪个语言，那么我们需要结合更前面的 France 这个背景。这时相关信息和需要的点（<em>the point where it is needed</em>）之间的差距就会变得非常大。</p>
<p>然而不幸的是，随着这个差距的增大，RNNs 越来越难以学习使用以前的信息。</p>
<p><img src="https://i.imgur.com/Whfo6UB.png" height="180)"></p>
<p>理论上来说，RNNs 完全可以处理这种“长期依赖”（<em>long-term dependencies</em>）。一个人可以很仔细的选择参数来解决这种形式的小问题（<em>toy problems</em>）。不过实际上，RNNs 似乎并不能学习到这种长期依赖。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter (1991) [German]</a> 和 <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a> 曾经深入探讨了这个问题，发现了一些相当根本的原因。</p>
<p>幸运的是，LSTMs 并没有这个问题！</p>
<h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>长短期记忆网络 —— 通常简称为“LSTMs” —— 是一种特别的 RNN，能够学习到长期依赖。LSTMs 由 <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> 提出，其后有很多人都对其进行了改善和推广<sup><a href="#fn_1" id="reffn_1">1</a></sup>。LSTMs 在许多任务上效果都非常好，现在也被广泛使用。</p>
<p>LSTMs 就是被设计用来避免长期依赖问题的。记住长时间的信息实际上是他们的默认行为，而不需要可以去这样做！</p>
<p>所有的 RNN 都是在将一个神经网络的模块重复好多次并链式连接起来。在标准的 RNNs 中，这个被重复的模块是一个非常简单的结构，例如一个单层的 tanh 层。</p>
<p><img src="https://i.imgur.com/KfFor4n.png" height="180)"></p>
<center><font color="gray">标准 RNN 中的重复模块都含有一层</font></center>

<p>LSTMs 也有这样一个链式结构，但是那个重复模块的结构是不一样的。与仅有一个单层神经网络不同的是，LSTMs 有 4 层，以一种非常特殊的方式连接起来。</p>
<p><img src="https://i.imgur.com/UydD8qN.png" height="180)"></p>
<center><font color="gray">LSTMs 中的重复模块含有 4 层</font></center>

<p>不要担心这里面的细节。稍后我们将会逐步深入这个 LSTMs 图。现在，我们先来熟悉下我们将要使用的符号：</p>
<p><img src="https://i.imgur.com/CbUDJll.png" alt=""></p>
<p>上图中，每一条线都表示一个向量从一个输出节点传到其他节点作为输入。粉色圆圈表示的是 pointwise 操作，例如向量加法，而黄色举行表示的是可学习的神经网络层。线合在一起表示连接（<em>concatenation</em>），线分叉表示其内容被复制成多份并且这些复制品流向不同的方向。</p>
<h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p>LSTMs 的核心是单元状态（<em>cell state</em>），就是顶部那条水平贯穿整个图的线。</p>
<p>单元状态就像是一个传送带，直接穿过整个链式结构，与其他部分仅有一些次要的线性交互，可以非常容易的传送信息而保持其不变。</p>
<p><img src="https://i.imgur.com/n7yh66F.png" height="180)"></p>
<p>LSTMs 可以给单元状态移除或者增加信息，由一个称为门（<em>gate</em>）的结构来控制。</p>
<p>门是一种让信息选择性通过的方式，由一个 sigmoid 层和一个 pointwise 乘法操作组成。</p>
<p><img src="https://i.imgur.com/1yDnZiD.png" height="180)"></p>
<p>sigmoid 层输出一个介于 0 和 1 之间的数字，表示每一个组件有多少信息可以穿过。0 意味着不让任何信息穿过，1 则意味着让所有信息穿过。</p>
<p>一个 LSTM 有 3 个这样的门，来保护和空值单元状态。</p>
<h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>LSTM 的第一步就是决定我们要从单元状态中扔掉什么信息，这由一个叫失忆门（<em>forget gate layer</em>）的 sigmoid 层来控制。失忆门的输入为 $h<em>{t-1}$ 和 $x_t$，然后为单元状态 $C</em>{t-1}$ 中的每个数字输出一个 0 和 1 之间的数，1 表示完全保留信息，而 0 表示完全丢去信息。</p>
<p>让我们回到语言模型，我们想要基于所有前面的词来预测下一个词。在这样一个问题中，单元状态可能包括了当前主体（<em>subject</em>）的性别，因此可以使用正确的代词。当我们看到一个新的主体时，我们想要忘记旧主体的性别。</p>
<p><img src="https://i.imgur.com/e8fiEJg.png" alt=""></p>
<p>下一步就是我们要决定要在单元状态中存入什么信息。这包括两部分。首先，一个叫做输入门（<em>input gate layer</em>）的 sigmoid 层决定我们要更新哪些值。然后一个 tanh 层创建一个新的候选向量 $\tilde{C}_t$，这个值会加到单元状态中。下一步我们将会利用这两个向量来更新单元状态。</p>
<p>在语言模型的例子中，我们想要用新主体的性别替换掉旧主体的性别，并加到单元状态中。</p>
<p><img src="https://i.imgur.com/8JE0JQW.png" alt="LSTM3-focus-i"></p>
<p>现在可以把旧的单元状态 $\tilde{C}_{t-1}$ 更新为新的单元状态 $\tilde{C}_t$ 了，前面的步骤已经决定了要做什么，现在我们只需要真正的去做就行了。</p>
<p>我们要乘以 $f_t$，也就是之前在失忆门我们决定的要忘记的东西。然后我们加上 $i_t*\tilde{C}_t$，这是新的候选值，乘上我们决定的要为每个单元状态值更新多少。</p>
<p>在语言模型的例子中，这里实际做的就是丢弃旧主体的性别信息，加进新主体的性别信息，就像我们之前要做的那样。</p>
<p><img src="https://i.imgur.com/1FXS5pB.png" alt=""></p>
<p>最后，我们要决定输出什么。这个输出将会是基于我们的单元状态的，但是是一个过滤版本（<em>filtered version</em>）。首先，我们运行一个 sigmoid 层来决定单元状态的哪些部分会被输出。然后，我们把单元状态输入给一个 tanh 层（把值映射到 $[-1,1]$ 区间内），并乘上 sigmoid 层的输出，然后这就是我们的输出。</p>
<p>在语言模型的例子中，由于仅仅有一个主体，以防接下来会发生什么事，LSTM 可能会输出与一个动词相关的信息。例如，可能会输出表示这个主体是单数还是复数的信息，以便我们如果知道接下来要发生什么，我们应该使用动词的什么形式。</p>
<p><img src="https://i.imgur.com/lLxeyBy.png" alt="LSTM3-focus-o"></p>
<h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>我目前描述的是非常普通的 LSTM，但不是所有的 LSTMs 都和上面的一样。事实上，几乎所有与 LSTMs 相关的论文都会使用一个稍微不同的版本。这些区别是很小的，但是其中一些区别值得我们注意。</p>
<p>其中一个比较流行的 LSTM 变体由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 引入，增加了一个「猫眼连接」（<em>peephole connections</em>），这意味着我们让门可以看到单元状态。</p>
<p><img src="https://i.imgur.com/APS3oDp.png" alt="LSTM3-var-peepholes"></p>
<p>上图给所有的门都增加了「猫眼」，但是一些论文只会加一些。</p>
<p>另一个变体是使用耦合的（<em>coupled</em>）失忆门和输入门。与独立的决定我们要丢弃和增加哪些信息不同的是，我们一起做这两个决定。只有当我们输入一些新信息的时候才会丢弃一些信息，只有当我们丢弃一些旧信息的时候才会输入新信息。</p>
<p><img src="https://i.imgur.com/gdm03LC.png" alt="LSTM3-var-tied"></p>
<p>稍微好点的（<em>dramatic</em>）LSTM 变体是 Gated Recurrent Unit，简称 GRU，由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a> 提出。GRU 将失忆门和输入门组合成一个更新门（<em>update gate</em>）。同时也合并了单元状态和隐藏状态，也做了一些其他改变。最终的模型比标准的 LSTM 模型更简单，而且越来越受欢迎。</p>
<p><img src="https://i.imgur.com/UEiY58u.png" alt="LSTM3-var-GRU"></p>
<p>这只是一些最显著的 LSTM 变体，还有很多其他的，例如 <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao, et al. (2015)</a> 提出的 Depth Gated RNNs。也有一些方法用完全不同的方式来处理长期依赖的问题，比如 <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik, et al. (2014)</a> 提出的 Clockwork RNNs。</p>
<p>哪个变体是最好的？这些差别影响大吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> 对流行的变体做了一个很好的比较，发现他们都是一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a> 测试了超过 10000 种 RNN 架构，发现其中一些在某些具体任务上的表现优于 LSTMs。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>前面我提到人们使用 RNNs 取得了显著成果，基本上都是用的 LSTMs。他们在大多数任务上都表现的很好。</p>
<p>写下一堆有关 LSTMs 的方程，这让 LSTMs 看起来很吓人。希望这篇文章一步一步的走进 LSTMs 可以让你更好的理解他们。</p>
<p>在可以用 RNNs 完成的任务上使用 LSTMs 是前进的一大步。很自然我们会问：还有另外一个一大步吗？研究者中的一个普遍观点是“是的！有下一步而且这个下一步是注意力机制（<em>attention</em>）”。这个想法是让 RNN 的每一步都从更多的信息中挑选信息。例如，如果你用一个 RNN 来为一个图片加一些注释，每个输出的词都可能对应的是图片中的一部分。<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, et al. (2015)</a> 就是这样做的，如果你想要继续探索注意力的话，那么这是一个很好地起点。使用注意力已经有了一些很好地结果，而且似乎还有更多的结果出来。注意力机制并不是 RNN 研究中唯一令人兴奋的部分，例如 <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, et al. (2015)</a> 提出的 Grid LSTMs 似乎也非常有前途。在生成模型中使用 RNNs 看起来也非常有趣，例如 <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, et al. (2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, et al. (2015)</a> 和 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a>。过去几年对于 RNNs 来说是一个激动人心的时刻，接下来几年只会更是如此！</p>
<blockquote id="fn_1">
<sup>1</sup>. 除了原作者，许多人都对现在的 LSTM 做出了贡献。 一个不完整名单：Felix Gers，Fred Cummins，Santiago Fernandez，Justin Bayer，Daan Wierstra，Julian Togelius，Faustino Gomez，Matteo Gagliolo，和 <a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" target="_blank" rel="noopener">Alex Graves</a>。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
        
            <li class="previous">
                <a href= "/2017/12/25/export-csdn-blogs-to-md/" title= 批量导出 CSDN 博客为 Markdown 文件 >
                    <span>Previous Post</span>
                    <span>批量导出 CSDN 博客为 Markdown 文件</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    <div id="disqus_thread"></div>
    <script>
        /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        
        var disqus_config = function () {
        this.page.url = "http://yoursite.com/2017/12/29/understanding-lstms/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "理解 LSTM 网络"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        
        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://secsilm.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();

    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

    
    <!--PC版-->

    <!--PC版-->


    
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:secsilm@outlook.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/secsilm" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
            
                <a href="https://twitter.com/SilverSecondMan" class="iconfont-archer twitter" target="_blank" title="twitter"></a>
            
        
    
        
            
                <a href="https://www.instagram.com/secsilm/" class="iconfont-archer instagram" target="_blank" title="instagram"></a>
            
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
        <span id="busuanzi_container_site_pv">Welcome, No. <span id="busuanzi_value_site_pv"></span> visitor.
        </span>
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Recurrent-Neural-Networks"><span class="toc-number">1.</span> <span class="toc-text">Recurrent Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Problem-of-Long-Term-Dependencies"><span class="toc-number">2.</span> <span class="toc-text">The Problem of Long-Term Dependencies</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-Networks"><span class="toc-number">3.</span> <span class="toc-text">LSTM Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Core-Idea-Behind-LSTMs"><span class="toc-number">4.</span> <span class="toc-text">The Core Idea Behind LSTMs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Step-by-Step-LSTM-Walk-Through"><span class="toc-number">5.</span> <span class="toc-text">Step-by-Step LSTM Walk Through</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Variants-on-Long-Short-Term-Memory"><span class="toc-number">6.</span> <span class="toc-text">Variants on Long Short Term Memory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">7.</span> <span class="toc-text">Conclusion</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 43 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/29</span><a class="archive-post-title" href= "/2017/12/29/understanding-lstms/" >理解 LSTM 网络</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/25</span><a class="archive-post-title" href= "/2017/12/25/export-csdn-blogs-to-md/" >批量导出 CSDN 博客为 Markdown 文件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span><a class="archive-post-title" href= "/2017/12/22/understanding-estimators-datasets/" >理解 Estimators 和 Datasets</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/26</span><a class="archive-post-title" href= "/2017/10/26/windows10-dark-theme/" >Windows 10 资源管理器黑色风格</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/08</span><a class="archive-post-title" href= "/2017/10/08/gradient-descent-methods/" >梯度下降优化算法概述</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span><a class="archive-post-title" href= "/2017/08/30/ensemble-methods/" >使用集成学习提升机器学习算法性能</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/20</span><a class="archive-post-title" href= "/2017/08/20/understanding-tensorboard/" >理解 TensorBoard</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span><a class="archive-post-title" href= "/2017/06/17/numpy-shuffle-permutation/" >Numpy 中的 shuffle VS permutation</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2017/06/02/tensorflow-DNNRegressor/" >TensorFlow DNNRegressor 的简单使用</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/18</span><a class="archive-post-title" href= "/2017/05/18/installing-xgboost/" >XGBoost 在 Windows 10 和 Ubuntu 上的安装</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/22</span><a class="archive-post-title" href= "/2017/04/22/python-fire/" >Python 自动生成命令行工具 - fire 简介</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/29</span><a class="archive-post-title" href= "/2017/03/29/understanding-svd/" >奇异值分解 SVD 的数学解释</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/2017/03/25/python-string-count/" >使用 Python 统计字符串中英文、空格、数字、标点个数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/17</span><a class="archive-post-title" href= "/2017/03/17/tensorflow-cnn-tensorboard/" >TensorFlow 中的卷积神经网络 CNN - TensorBoard 版</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span><a class="archive-post-title" href= "/2017/03/15/using-tree/" >使用 tree 命令格式化输出目录结构</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/2017/03/11/vscode-pdf-error/" >VSCode Markdown PDF 导出成PDF报 phantomjs binary does not exist 错误的解决办法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span><a class="archive-post-title" href= "/2017/03/01/numpy-copy/" >Numpy 中的 copy 问题详解</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/26</span><a class="archive-post-title" href= "/2017/02/26/tensorflow-cudnn-version/" >Check failed stream->parent()->GetConvolveAlgorithms(&algorithms)解决办法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/16</span><a class="archive-post-title" href= "/2017/02/16/tensorflow-1.0/" >TensorFlow 1.0 发布</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/28</span><a class="archive-post-title" href= "/2016/12/28/tensorflow-cnn-no-tensorboard/" >TensorFlow 中的卷积神经网络 CNN - 无TensorBoard版</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/10</span><a class="archive-post-title" href= "/2016/12/10/tensorboard-mnist-pca/" >在 TensorBoard 中用 PCA 可视化 MNIST 手写数字识别数据集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span><a class="archive-post-title" href= "/2016/12/01/installing-tensorflow/" >Windows10 64 位下安装 TensorFlow - 官方原生支持</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/30</span><a class="archive-post-title" href= "/2016/11/30/numpy-array-memory/" >小谈 Numpy 数组占用内存空间问题</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/19</span><a class="archive-post-title" href= "/2016/11/19/tensorflow-mlp/" >TensorFlow 中的多层感知器（MLP）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/06</span><a class="archive-post-title" href= "/2016/11/06/tensorflow-logistic-regression/" >TensorFlow 中的 Logistic Regression</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2016/11/03/pandas-apply/" >Pandas 中的 apply 函数使用示例</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/29</span><a class="archive-post-title" href= "/2016/10/29/tensorflow-hyperparams/" >学习率、迭代次数和初始化方式对模型准确率的影响</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/26</span><a class="archive-post-title" href= "/2016/10/26/sklearn-logistics-regression-parameters/" >sklearn 中 Logistics Regression 的 coef_ 和 intercept_ 的具体意义</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/24</span><a class="archive-post-title" href= "/2016/10/24/plt-save/" >解决使用 plt.savefig 保存图片时一片空白</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/22</span><a class="archive-post-title" href= "/2016/10/22/tensorflow-linear-regression/" >TensorFlow 中的线性回归</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/16</span><a class="archive-post-title" href= "/2016/10/16/python-email/" >用 Python 发电子邮件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/27</span><a class="archive-post-title" href= "/2016/08/27/ubuntu-installing-tensorflow/" >Ubuntu 14.04 64 位安装 Google 的 TensorFlow</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/22</span><a class="archive-post-title" href= "/2016/08/22/ubuntu-update-issue/" >Ubuntu 14.04 64 位系统更新重启后无法进入系统，光标不停闪烁</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/27</span><a class="archive-post-title" href= "/2016/06/27/python-strptime/" >Python 中 strptime 的简单使用</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/03</span><a class="archive-post-title" href= "/2016/04/03/python-numpy/" >Python NumPy 基础</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/24</span><a class="archive-post-title" href= "/2016/03/24/ubuntu-gedit-change-theme/" >Ubuntu14.04 gnome3 下 gedit 首选项消失时如何修改 gedit 主题</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/18</span><a class="archive-post-title" href= "/2016/01/18/matlab-plot-parallelepiped/" >MATLAB绘制平行六面体</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span><a class="archive-post-title" href= "/2016/01/11/matlab-mat2cell/" >MATLAB 矩阵分块函数 mat2cell 及 cellfun 函数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span><a class="archive-post-title" href= "/2016/01/11/machine-learning-book-note/" >《机器学习》学习笔记1——绪论 机器学习概述</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2015 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/21</span><a class="archive-post-title" href= "/2015/12/21/matlab-fitlm/" >使用 MATLAB 的 fitlm 函数进行线性回归</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/19</span><a class="archive-post-title" href= "/2015/11/19/matlab-rename-files/" >MATLAB 批量文件重命名（详细解释）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/05</span><a class="archive-post-title" href= "/2015/11/05/matlab-callback/" >MATLAB GUI 中 Edit Text 的 Callback 函数何时执行</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2015/09/22/matlab-move-files/" >用 MATLAB 将多个文件夹内的某些文件汇总到另一个文件夹</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">Python</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Machine Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Translation</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">TensorFlow</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Data Science</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">MATLAB</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">sklearn</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Ubuntu</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">IDE</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Windows</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


