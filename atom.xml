<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Alan Lee Space Station</title>
  
  <subtitle>Always Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://secsilm.github.io/"/>
  <updated>2018-03-15T12:12:43.091Z</updated>
  <id>https://secsilm.github.io/</id>
  
  <author>
    <name>Alan Lee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>嵌入 bokeh 绘图到 hexo 博客中</title>
    <link href="https://secsilm.github.io/2018/03/15/embed-bokeh-plot/"/>
    <id>https://secsilm.github.io/2018/03/15/embed-bokeh-plot/</id>
    <published>2018-03-15T07:57:45.000Z</published>
    <updated>2018-03-15T12:12:43.091Z</updated>
    
    <content type="html"><![CDATA[<p><link href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css" rel="stylesheet" type="text/css"></p><p><link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css" type="text/css"></p><script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js"></script><script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js"></script><p><a href="https://bokeh.pydata.org/en/latest/" target="_blank" rel="noopener">Bokeh</a> 是一个 Python 交互式可视化库，主要用于绘制面向浏览器的交互式图表，D3.js 的风格，Anaconda 出品。关注这个库很久了，我知道的 Python 中用于创建这种交互式图表的库好用的不是很多，还有个 <a href="https://plot.ly/" target="_blank" rel="noopener">plotly</a> 也不错，同时支持 Python 和 R，只不过是收费的，免费的有很多限制。</p><p>使用 bokeh 你可以绘制很多具有交互效果的图，例如直接拖动图、hover 效果、滚轮缩放、box 缩放、按钮和滑块等其他丰富的组件。这里放一个我写本文的时候发现的以 Stephen Hawking （R.I.P）的论文和书籍成就数据为基础绘制的图，这也是使用 bokeh 绘制的，完整文章可见 <a href="http://michael.szell.net/hawking/" target="_blank" rel="noopener">Stephen Hawking - An interactive history of his career</a>：</p><p><img src="https://i.imgur.com/VbaGgC1.gif" alt="sh"></p><p>话说正题，本文的目的不是让你学会怎么使用 bokeh 画图，而是如何把画好的图嵌入到网页或者博客中，例如 hexo 博客。而这一部分我将分两部分说：带有 widgets 的图（<strong>No Widgets</strong>）和不带 widgets 的图（<strong>With Widgets</strong>）。这里说的 widgets 指按钮、复选框、单选框、下拉菜单和滑块等小部件。引用 <a href="https://bokeh.pydata.org/en/latest/docs/user_guide/concepts.html" target="_blank" rel="noopener">Bokeh 官方</a>对 widgets 的解释：</p><blockquote><p>User interface elements outside of a Bokeh plot such as sliders, drop down menus, buttons, etc. Events and updates from widgets can inform additional computations, or cause Bokeh plots to update. Widgets can be used in both standalone applications or with the Bokeh server. For examples and information, see <a href="https://bokeh.pydata.org/en/latest/docs/user_guide/interaction.html#userguide-interaction" target="_blank" rel="noopener">Adding Interactions</a>.</p></blockquote><p>根据 <a href="https://bokeh.pydata.org/en/latest/docs/user_guide/embed.html" target="_blank" rel="noopener">Bokeh 官方</a>的描述，我们有三种方法可以将图嵌入到网页中：</p><ul><li><strong>HTML files</strong></li><li><strong>Components</strong></li><li>Autoload Scripts</li></ul><blockquote><p>Note：</p><ul><li><a href="https://bokeh.pydata.org/en/latest/docs/user_guide/server.html#userguide-server" target="_blank" rel="noopener">Bokeh Server</a> 不在此文讨论范围中。</li><li>关于如何使用 Bokeh，可能的话以后再写篇来说。</li></ul></blockquote><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>先来看下嵌入后的效果：</p><p>No widgets：</p><div class="bk-root">    <div class="bk-plotdiv" id="c3e3c5d7-6d0f-4253-aa32-4d4b05636220"></div></div><script type="application/json" id="0c06bc21-28fe-4fcb-ba43-91c7166610c6">    {"0c8888dd-d728-4892-a09c-12106d1d56c8":{"roots":{"references":[{"attributes":{"formatter":{"id":"2e33a094-8602-4d4c-97df-714b0c01c3ba","type":"BasicTickFormatter"},"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"}},"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"},{"attributes":{},"id":"2e33a094-8602-4d4c-97df-714b0c01c3ba","type":"BasicTickFormatter"},{"attributes":{},"id":"a398d591-409e-45bd-8152-dbcc472b8636","type":"PanTool"},{"attributes":{},"id":"e0464e1f-a698-4c36-ac8c-8f2c5407a417","type":"WheelZoomTool"},{"attributes":{"formatter":{"id":"50165239-a8a4-4f1a-bd1c-9cb9f945d866","type":"BasicTickFormatter"},"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"}},"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"},{"attributes":{},"id":"da3e277b-51fd-405f-aa8f-94206ec769a4","type":"BasicTickFormatter"},{"attributes":{"dimension":1,"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"}},"id":"d75cca78-1048-4a3e-ab43-0c0e98df5b36","type":"Grid"},{"attributes":{"source":{"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"}},"id":"868d4f51-2b41-47f2-acf2-f1d160fd0999","type":"CDSView"},{"attributes":{"callback":null},"id":"1ffa00c4-f9c2-4816-a98e-c66beabc4b70","type":"DataRange1d"},{"attributes":{},"id":"b85908c0-106f-49cd-8f4a-83e92d958074","type":"LinearScale"},{"attributes":{},"id":"f484f0c7-d10b-4813-83ee-dd4ac86f610d","type":"WheelZoomTool"},{"attributes":{"below":[{"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"}],"left":[{"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"}],"plot_height":400,"renderers":[{"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"},{"id":"413b7a2a-58cd-4570-a15b-4774445bc075","type":"Grid"},{"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"},{"id":"ced5469e-a711-4588-afca-f1dabd241d07","type":"Grid"},{"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"},{"id":"1db047e9-5900-4f6d-b5a7-d5aff16b4770","type":"GlyphRenderer"}],"title":{"id":"cebf0126-7d05-4538-b776-62af024d3b43","type":"Title"},"toolbar":{"id":"7c64d5aa-6215-4c24-9014-113dfecba9b1","type":"Toolbar"},"x_range":{"id":"da0aa644-9956-41e5-9bf9-151dca8613bc","type":"DataRange1d"},"x_scale":{"id":"f5769855-4441-4181-82d3-14e5e1cbfd66","type":"LinearScale"},"y_range":{"id":"167f9a5d-fa71-4e1b-885f-1625d9f29b1b","type":"DataRange1d"},"y_scale":{"id":"b85908c0-106f-49cd-8f4a-83e92d958074","type":"LinearScale"}},"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},{"attributes":{"data_source":{"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"},"glyph":{"id":"3556bfe2-bab1-45cc-a066-591cab89b812","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"04f140b8-754a-4261-98cb-26b1ff1f2b06","type":"Line"},"selection_glyph":null,"view":{"id":"868d4f51-2b41-47f2-acf2-f1d160fd0999","type":"CDSView"}},"id":"0b8ad283-579c-4eee-bd94-57f8629d95f9","type":"GlyphRenderer"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"a398d591-409e-45bd-8152-dbcc472b8636","type":"PanTool"},{"id":"e0464e1f-a698-4c36-ac8c-8f2c5407a417","type":"WheelZoomTool"},{"id":"cfa6b691-70d9-4e1b-985f-97fd83304cfd","type":"BoxZoomTool"},{"id":"fc40d6d3-72cf-4cd1-baab-78e4b807bb1a","type":"SaveTool"},{"id":"30635d51-4c94-4436-b4dc-5152211127e1","type":"ResetTool"},{"id":"3b7a65ea-a301-4bc1-9f72-04a488858860","type":"HelpTool"}]},"id":"e7743f55-fb9c-42de-a5fc-920330134fc9","type":"Toolbar"},{"attributes":{"callback":null},"id":"da0aa644-9956-41e5-9bf9-151dca8613bc","type":"DataRange1d"},{"attributes":{},"id":"50165239-a8a4-4f1a-bd1c-9cb9f945d866","type":"BasicTickFormatter"},{"attributes":{"overlay":{"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"}},"id":"59715e61-a1b9-4efe-a0b0-c39b10a530e9","type":"BoxZoomTool"},{"attributes":{"plot":null,"text":""},"id":"cebf0126-7d05-4538-b776-62af024d3b43","type":"Title"},{"attributes":{},"id":"6b502fcf-944d-4f79-b0d7-7ff8c02404b7","type":"ResetTool"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"04f140b8-754a-4261-98cb-26b1ff1f2b06","type":"Line"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"d953b8ce-b1cc-47f4-8a1d-742f35c86af4","type":"Line"},{"attributes":{"below":[{"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"}],"left":[{"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"}],"renderers":[{"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"},{"id":"12f2c781-5f88-46a4-ad7a-70be9f1965fd","type":"Grid"},{"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"},{"id":"d75cca78-1048-4a3e-ab43-0c0e98df5b36","type":"Grid"},{"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"},{"id":"0b8ad283-579c-4eee-bd94-57f8629d95f9","type":"GlyphRenderer"},{"id":"20419767-e365-43ca-81e6-9825076ab4bb","type":"GlyphRenderer"}],"title":{"id":"ccbe5382-ecae-4e55-8ac3-610208a92c7d","type":"Title"},"toolbar":{"id":"e7743f55-fb9c-42de-a5fc-920330134fc9","type":"Toolbar"},"x_range":{"id":"1ffa00c4-f9c2-4816-a98e-c66beabc4b70","type":"DataRange1d"},"x_scale":{"id":"b2182d83-08ec-40e7-b28e-650592227237","type":"LinearScale"},"y_range":{"id":"5765487d-b1a0-424f-9e36-8dbc469f05a1","type":"DataRange1d"},"y_scale":{"id":"1f614c03-87c2-404d-b31b-5d8d82114e92","type":"LinearScale"}},"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},{"attributes":{},"id":"b712048b-35ad-4935-ae1c-ead54b79e489","type":"SaveTool"},{"attributes":{},"id":"1f614c03-87c2-404d-b31b-5d8d82114e92","type":"LinearScale"},{"attributes":{},"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"},{"attributes":{"callback":null},"id":"5765487d-b1a0-424f-9e36-8dbc469f05a1","type":"DataRange1d"},{"attributes":{"dimension":1,"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"}},"id":"ced5469e-a711-4588-afca-f1dabd241d07","type":"Grid"},{"attributes":{"source":{"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"}},"id":"1d22a5ff-7969-43d0-a244-d06d00adb013","type":"CDSView"},{"attributes":{},"id":"b2182d83-08ec-40e7-b28e-650592227237","type":"LinearScale"},{"attributes":{"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"}},"id":"12f2c781-5f88-46a4-ad7a-70be9f1965fd","type":"Grid"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"dcd07e66-8b65-4886-a0e6-0187c3783c46","type":"Line"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"},{"attributes":{"callback":null},"id":"167f9a5d-fa71-4e1b-885f-1625d9f29b1b","type":"DataRange1d"},{"attributes":{},"id":"44d69179-0506-48ef-8872-3e74205932d5","type":"PanTool"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"41aabb79-37b6-4ae7-b4b6-11b70ca9fc6d","type":"Line"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"44d69179-0506-48ef-8872-3e74205932d5","type":"PanTool"},{"id":"f484f0c7-d10b-4813-83ee-dd4ac86f610d","type":"WheelZoomTool"},{"id":"59715e61-a1b9-4efe-a0b0-c39b10a530e9","type":"BoxZoomTool"},{"id":"b712048b-35ad-4935-ae1c-ead54b79e489","type":"SaveTool"},{"id":"6b502fcf-944d-4f79-b0d7-7ff8c02404b7","type":"ResetTool"},{"id":"ccb6843f-9ce1-4b27-8a84-a987a24a630b","type":"HelpTool"}]},"id":"7c64d5aa-6215-4c24-9014-113dfecba9b1","type":"Toolbar"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"67cac86c-72b8-4585-a942-54988072032f","type":"Line"},{"attributes":{"data_source":{"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"},"glyph":{"id":"d953b8ce-b1cc-47f4-8a1d-742f35c86af4","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"dcd07e66-8b65-4886-a0e6-0187c3783c46","type":"Line"},"selection_glyph":null,"view":{"id":"1d22a5ff-7969-43d0-a244-d06d00adb013","type":"CDSView"}},"id":"1db047e9-5900-4f6d-b5a7-d5aff16b4770","type":"GlyphRenderer"},{"attributes":{"data_source":{"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"},"glyph":{"id":"41aabb79-37b6-4ae7-b4b6-11b70ca9fc6d","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"67cac86c-72b8-4585-a942-54988072032f","type":"Line"},"selection_glyph":null,"view":{"id":"a8e6f99a-34ab-43c7-a0d4-b3e541f0d288","type":"CDSView"}},"id":"20419767-e365-43ca-81e6-9825076ab4bb","type":"GlyphRenderer"},{"attributes":{},"id":"ccb6843f-9ce1-4b27-8a84-a987a24a630b","type":"HelpTool"},{"attributes":{"source":{"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"}},"id":"a8e6f99a-34ab-43c7-a0d4-b3e541f0d288","type":"CDSView"},{"attributes":{"overlay":{"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"}},"id":"cfa6b691-70d9-4e1b-985f-97fd83304cfd","type":"BoxZoomTool"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"},{"attributes":{"plot":null,"text":""},"id":"ccbe5382-ecae-4e55-8ac3-610208a92c7d","type":"Title"},{"attributes":{"formatter":{"id":"c6af5d8e-5988-4f6a-a22a-789a6223bedf","type":"BasicTickFormatter"},"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"}},"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"},{"attributes":{},"id":"fc40d6d3-72cf-4cd1-baab-78e4b807bb1a","type":"SaveTool"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"3556bfe2-bab1-45cc-a066-591cab89b812","type":"Line"},{"attributes":{},"id":"30635d51-4c94-4436-b4dc-5152211127e1","type":"ResetTool"},{"attributes":{"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"}},"id":"413b7a2a-58cd-4570-a15b-4774445bc075","type":"Grid"},{"attributes":{},"id":"f5769855-4441-4181-82d3-14e5e1cbfd66","type":"LinearScale"},{"attributes":{},"id":"3b7a65ea-a301-4bc1-9f72-04a488858860","type":"HelpTool"},{"attributes":{},"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"},{"attributes":{},"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"},{"attributes":{},"id":"c6af5d8e-5988-4f6a-a22a-789a6223bedf","type":"BasicTickFormatter"},{"attributes":{"formatter":{"id":"da3e277b-51fd-405f-aa8f-94206ec769a4","type":"BasicTickFormatter"},"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"}},"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"},{"attributes":{},"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"}],"root_ids":["025f068b-88d7-4051-9a74-8b3cf4ca3322","b90f0883-95b6-4cd2-b378-ceab8964a7b5"]},"title":"Bokeh Application","version":"0.12.14"}}</script><script type="text/javascript">    (function() {    var fn = function() {        Bokeh.safely(function() {        (function(root) {            function embed_document(root) {            var docs_json = document.getElementById('0c06bc21-28fe-4fcb-ba43-91c7166610c6').textContent;            var render_items = [{"docid":"0c8888dd-d728-4892-a09c-12106d1d56c8","elementid":"c3e3c5d7-6d0f-4253-aa32-4d4b05636220","modelid":"b90f0883-95b6-4cd2-b378-ceab8964a7b5"}];            root.Bokeh.embed.embed_items(docs_json, render_items);            }            if (root.Bokeh !== undefined) {            embed_document(root);            } else {            var attempts = 0;            var timer = setInterval(function(root) {                if (root.Bokeh !== undefined) {                embed_document(root);                clearInterval(timer);                }                attempts++;                if (attempts > 100) {                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing")                clearInterval(timer);                }            }, 10, root)            }        })(window);        });    };    if (document.readyState != "loading") fn();    else document.addEventListener("DOMContentLoaded", fn);    })();</script><p>With widgets：</p><div class="bk-root">    <div class="bk-plotdiv" id="d5b7c7a5-8fef-4f8f-9395-7643af0ca730"></div></div><script type="application/json" id="51bd30f2-f016-428d-bf01-dfa13a5641fa">  {"c735e38e-6c46-4737-ab01-75ad9c85e84a":{"roots":{"references":[{"attributes":{},"id":"e0464e1f-a698-4c36-ac8c-8f2c5407a417","type":"WheelZoomTool"},{"attributes":{"line_alpha":0.6,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"442517bd-b021-469d-b8af-5c72c7db7f83","type":"Line"},{"attributes":{},"id":"da3e277b-51fd-405f-aa8f-94206ec769a4","type":"BasicTickFormatter"},{"attributes":{},"id":"0692b593-a76c-4214-babe-81612f4ce5f7","type":"LinearScale"},{"attributes":{"callback":null},"id":"8b69b140-f0dd-4b12-a781-d309af814089","type":"DataRange1d"},{"attributes":{"callback":null},"id":"1ffa00c4-f9c2-4816-a98e-c66beabc4b70","type":"DataRange1d"},{"attributes":{},"id":"b85908c0-106f-49cd-8f4a-83e92d958074","type":"LinearScale"},{"attributes":{"plot":{"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},"ticker":{"id":"534a5016-02e3-4506-b467-e615025ea194","type":"BasicTicker"}},"id":"8170b6df-ae63-4908-8d4b-7ef6d93029cc","type":"Grid"},{"attributes":{"plot":null,"text":""},"id":"41609255-7240-4f85-bf8d-4d5ab18763f3","type":"Title"},{"attributes":{"below":[{"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"}],"left":[{"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"}],"plot_height":400,"renderers":[{"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"},{"id":"413b7a2a-58cd-4570-a15b-4774445bc075","type":"Grid"},{"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"},{"id":"ced5469e-a711-4588-afca-f1dabd241d07","type":"Grid"},{"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"},{"id":"1db047e9-5900-4f6d-b5a7-d5aff16b4770","type":"GlyphRenderer"}],"title":{"id":"cebf0126-7d05-4538-b776-62af024d3b43","type":"Title"},"toolbar":{"id":"7c64d5aa-6215-4c24-9014-113dfecba9b1","type":"Toolbar"},"x_range":{"id":"da0aa644-9956-41e5-9bf9-151dca8613bc","type":"DataRange1d"},"x_scale":{"id":"f5769855-4441-4181-82d3-14e5e1cbfd66","type":"LinearScale"},"y_range":{"id":"167f9a5d-fa71-4e1b-885f-1625d9f29b1b","type":"DataRange1d"},"y_scale":{"id":"b85908c0-106f-49cd-8f4a-83e92d958074","type":"LinearScale"}},"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},{"attributes":{"formatter":{"id":"6f18901b-1365-4a7f-ad6c-3d848374a721","type":"BasicTickFormatter"},"plot":{"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},"ticker":{"id":"534a5016-02e3-4506-b467-e615025ea194","type":"BasicTicker"}},"id":"efd02180-9db1-4cf3-86b6-c38a49736567","type":"LinearAxis"},{"attributes":{"source":{"id":"959803c4-225a-452d-8e89-58193d81cb7d","type":"ColumnDataSource"}},"id":"0bcd2a4a-eb70-4bc2-85c7-f1adfa2fdc5f","type":"CDSView"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"a398d591-409e-45bd-8152-dbcc472b8636","type":"PanTool"},{"id":"e0464e1f-a698-4c36-ac8c-8f2c5407a417","type":"WheelZoomTool"},{"id":"cfa6b691-70d9-4e1b-985f-97fd83304cfd","type":"BoxZoomTool"},{"id":"fc40d6d3-72cf-4cd1-baab-78e4b807bb1a","type":"SaveTool"},{"id":"30635d51-4c94-4436-b4dc-5152211127e1","type":"ResetTool"},{"id":"3b7a65ea-a301-4bc1-9f72-04a488858860","type":"HelpTool"}]},"id":"e7743f55-fb9c-42de-a5fc-920330134fc9","type":"Toolbar"},{"attributes":{"callback":null},"id":"da0aa644-9956-41e5-9bf9-151dca8613bc","type":"DataRange1d"},{"attributes":{},"id":"742c1cc8-2c58-40a8-8573-65a22a141647","type":"HelpTool"},{"attributes":{},"id":"534a5016-02e3-4506-b467-e615025ea194","type":"BasicTicker"},{"attributes":{"overlay":{"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"}},"id":"59715e61-a1b9-4efe-a0b0-c39b10a530e9","type":"BoxZoomTool"},{"attributes":{"formatter":{"id":"6915490c-1b04-4f3e-a128-fa1bca0fed20","type":"BasicTickFormatter"},"plot":{"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},"ticker":{"id":"ed361074-98f1-4b32-bd19-206c9718de67","type":"BasicTicker"}},"id":"e86a27e5-7feb-4207-8105-4517c6552cda","type":"LinearAxis"},{"attributes":{},"id":"13beec82-da72-4954-b0a4-bd02a03353eb","type":"SaveTool"},{"attributes":{},"id":"ed361074-98f1-4b32-bd19-206c9718de67","type":"BasicTicker"},{"attributes":{},"id":"68617b5b-9e89-496a-a9d8-747c6aa11595","type":"BasicTickFormatter"},{"attributes":{},"id":"6b502fcf-944d-4f79-b0d7-7ff8c02404b7","type":"ResetTool"},{"attributes":{"dimension":1,"plot":{"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},"ticker":{"id":"ed361074-98f1-4b32-bd19-206c9718de67","type":"BasicTicker"}},"id":"3da1cb0e-cdd6-4dea-b510-d6c277174994","type":"Grid"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"959803c4-225a-452d-8e89-58193d81cb7d","type":"ColumnDataSource"},{"attributes":{"below":[{"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"}],"left":[{"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"}],"renderers":[{"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"},{"id":"12f2c781-5f88-46a4-ad7a-70be9f1965fd","type":"Grid"},{"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"},{"id":"d75cca78-1048-4a3e-ab43-0c0e98df5b36","type":"Grid"},{"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"},{"id":"0b8ad283-579c-4eee-bd94-57f8629d95f9","type":"GlyphRenderer"},{"id":"20419767-e365-43ca-81e6-9825076ab4bb","type":"GlyphRenderer"}],"title":{"id":"ccbe5382-ecae-4e55-8ac3-610208a92c7d","type":"Title"},"toolbar":{"id":"e7743f55-fb9c-42de-a5fc-920330134fc9","type":"Toolbar"},"x_range":{"id":"1ffa00c4-f9c2-4816-a98e-c66beabc4b70","type":"DataRange1d"},"x_scale":{"id":"b2182d83-08ec-40e7-b28e-650592227237","type":"LinearScale"},"y_range":{"id":"5765487d-b1a0-424f-9e36-8dbc469f05a1","type":"DataRange1d"},"y_scale":{"id":"1f614c03-87c2-404d-b31b-5d8d82114e92","type":"LinearScale"}},"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},{"attributes":{},"id":"b712048b-35ad-4935-ae1c-ead54b79e489","type":"SaveTool"},{"attributes":{},"id":"ef916a0f-17f9-47fa-9b91-cd39031324f7","type":"PanTool"},{"attributes":{"below":[{"id":"ebd39a2b-e7d4-4ded-910a-41e56914f3d4","type":"LinearAxis"}],"left":[{"id":"1ac25c06-4428-445c-a046-b807fcd61bfd","type":"LinearAxis"}],"plot_height":400,"plot_width":400,"renderers":[{"id":"ebd39a2b-e7d4-4ded-910a-41e56914f3d4","type":"LinearAxis"},{"id":"d5154776-fcea-4fce-9125-bd1d04a03ab5","type":"Grid"},{"id":"1ac25c06-4428-445c-a046-b807fcd61bfd","type":"LinearAxis"},{"id":"b1ee50c1-13e3-46bb-9c77-97a9bd12d632","type":"Grid"},{"id":"00afcf2f-3d63-4ad5-8c64-b31a05e86987","type":"BoxAnnotation"},{"id":"9fefb425-b24c-40fc-b282-d970e388b4f5","type":"GlyphRenderer"}],"title":{"id":"41609255-7240-4f85-bf8d-4d5ab18763f3","type":"Title"},"toolbar":{"id":"629bbb70-4153-413e-8f47-6bdab5ca6943","type":"Toolbar"},"x_range":{"id":"8b69b140-f0dd-4b12-a781-d309af814089","type":"DataRange1d"},"x_scale":{"id":"baedce77-8087-4a39-bfbc-a2ff936e6d8f","type":"LinearScale"},"y_range":{"id":"337f4e1c-9d06-49d2-9fc7-13725bb39f04","type":"Range1d"},"y_scale":{"id":"5a5eb415-fe8a-4f0e-aa66-b927d1ea45e8","type":"LinearScale"}},"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},{"attributes":{"source":{"id":"08c4a922-77b4-4250-a13a-9fddf147787c","type":"ColumnDataSource"}},"id":"23b5c7bb-c0ba-4894-9e08-60789d7136bb","type":"CDSView"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"f5d8b6ef-a1d6-4550-a985-4d3a707ba7bb","type":"PanTool"},{"id":"bded7dd3-fb10-4032-9eab-349a43ba917b","type":"WheelZoomTool"},{"id":"3818aea3-a7b5-4ca5-a0a9-0b682541f99b","type":"BoxZoomTool"},{"id":"b2187cb4-e540-49ce-b81a-4be158a4879f","type":"SaveTool"},{"id":"af97642d-848a-4657-86c8-18caef9acc80","type":"ResetTool"},{"id":"e0596d28-fa72-4739-b79d-ecb0a0733236","type":"HelpTool"}]},"id":"629bbb70-4153-413e-8f47-6bdab5ca6943","type":"Toolbar"},{"attributes":{},"id":"1f614c03-87c2-404d-b31b-5d8d82114e92","type":"LinearScale"},{"attributes":{},"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"},{"attributes":{"callback":{"id":"00c622b4-2925-4ca5-af9a-bcc7cf7a390d","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Frequency","value":1},"id":"e0dcba8c-25ec-4af7-a915-3f5aa718acf1","type":"Slider"},{"attributes":{"data_source":{"id":"08c4a922-77b4-4250-a13a-9fddf147787c","type":"ColumnDataSource"},"glyph":{"id":"be28ff81-13f5-4a6a-87cf-cee63a353dc3","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"e3e96b56-e938-4dd9-b2c0-1d6f68b11997","type":"Line"},"selection_glyph":null,"view":{"id":"23b5c7bb-c0ba-4894-9e08-60789d7136bb","type":"CDSView"}},"id":"cb2f4e04-6c88-4f2b-bfd3-2f48d68738e8","type":"GlyphRenderer"},{"attributes":{},"id":"1195cf55-a404-443b-a6a9-38988b72673b","type":"BasicTickFormatter"},{"attributes":{"children":[{"id":"b3b1d5ac-0b55-42d5-9ecc-be3f1ccd9ec0","type":"Slider"},{"id":"537d39b8-3d33-4eb3-8650-611264eae516","type":"Slider"},{"id":"e6080249-a875-443f-8600-f850fcebe08c","type":"Slider"},{"id":"a7a30a9b-f1e5-4a9e-a1a9-a59b1f79ed41","type":"Slider"}]},"id":"0d2048b6-7525-490c-9e28-db36bd628f34","type":"WidgetBox"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"00afcf2f-3d63-4ad5-8c64-b31a05e86987","type":"BoxAnnotation"},{"attributes":{"callback":null},"id":"5765487d-b1a0-424f-9e36-8dbc469f05a1","type":"DataRange1d"},{"attributes":{"callback":{"id":"00c622b4-2925-4ca5-af9a-bcc7cf7a390d","type":"CustomJS"},"end":6.4,"start":0,"step":0.1,"title":"Phase","value":0},"id":"4a21206c-ec74-4810-ada3-8a2fff22c271","type":"Slider"},{"attributes":{},"id":"5a5eb415-fe8a-4f0e-aa66-b927d1ea45e8","type":"LinearScale"},{"attributes":{"callback":{"id":"d73793a0-67f5-4fdc-9dfb-90a371dc035e","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Amplitude","value":1},"id":"b3b1d5ac-0b55-42d5-9ecc-be3f1ccd9ec0","type":"Slider"},{"attributes":{"callback":{"id":"397a958b-44c9-4ed9-9006-60f700662505","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Amplitude","value":1},"id":"84284e10-679f-4deb-a411-414704175019","type":"Slider"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"e3e96b56-e938-4dd9-b2c0-1d6f68b11997","type":"Line"},{"attributes":{"callback":null,"end":10,"start":-10},"id":"337f4e1c-9d06-49d2-9fc7-13725bb39f04","type":"Range1d"},{"attributes":{},"id":"b2182d83-08ec-40e7-b28e-650592227237","type":"LinearScale"},{"attributes":{"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"}},"id":"12f2c781-5f88-46a4-ad7a-70be9f1965fd","type":"Grid"},{"attributes":{},"id":"baedce77-8087-4a39-bfbc-a2ff936e6d8f","type":"LinearScale"},{"attributes":{"formatter":{"id":"68617b5b-9e89-496a-a9d8-747c6aa11595","type":"BasicTickFormatter"},"plot":{"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},"ticker":{"id":"7e767e1e-00b8-4ef7-b921-687287801292","type":"BasicTicker"}},"id":"1ac25c06-4428-445c-a046-b807fcd61bfd","type":"LinearAxis"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"},{"attributes":{"callback":null},"id":"167f9a5d-fa71-4e1b-885f-1625d9f29b1b","type":"DataRange1d"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"9d8287f0-9b3f-47de-94a5-591adc59a0ea","type":"Line"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"},{"attributes":{},"id":"284bbf91-0288-49ee-976a-f78b81bfa7c7","type":"ResetTool"},{"attributes":{"line_alpha":0.6,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"75e04be5-e6e9-470f-b298-592fa6db3071","type":"Line"},{"attributes":{"data_source":{"id":"959803c4-225a-452d-8e89-58193d81cb7d","type":"ColumnDataSource"},"glyph":{"id":"75e04be5-e6e9-470f-b298-592fa6db3071","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"9d8287f0-9b3f-47de-94a5-591adc59a0ea","type":"Line"},"selection_glyph":null,"view":{"id":"0bcd2a4a-eb70-4bc2-85c7-f1adfa2fdc5f","type":"CDSView"}},"id":"9fefb425-b24c-40fc-b282-d970e388b4f5","type":"GlyphRenderer"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"41aabb79-37b6-4ae7-b4b6-11b70ca9fc6d","type":"Line"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"44d69179-0506-48ef-8872-3e74205932d5","type":"PanTool"},{"id":"f484f0c7-d10b-4813-83ee-dd4ac86f610d","type":"WheelZoomTool"},{"id":"59715e61-a1b9-4efe-a0b0-c39b10a530e9","type":"BoxZoomTool"},{"id":"b712048b-35ad-4935-ae1c-ead54b79e489","type":"SaveTool"},{"id":"6b502fcf-944d-4f79-b0d7-7ff8c02404b7","type":"ResetTool"},{"id":"ccb6843f-9ce1-4b27-8a84-a987a24a630b","type":"HelpTool"}]},"id":"7c64d5aa-6215-4c24-9014-113dfecba9b1","type":"Toolbar"},{"attributes":{"data_source":{"id":"0df2dffe-f023-469d-a8e7-d31674abbb0a","type":"ColumnDataSource"},"glyph":{"id":"442517bd-b021-469d-b8af-5c72c7db7f83","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"39bb4328-6aee-4ff1-baad-97a894968cdb","type":"Line"},"selection_glyph":null,"view":{"id":"5c23c9ca-3e0e-4330-83f0-ffa23cf4b5c0","type":"CDSView"}},"id":"bd13252c-9074-4e4b-b799-cbaa60b06ae9","type":"GlyphRenderer"},{"attributes":{},"id":"6915490c-1b04-4f3e-a128-fa1bca0fed20","type":"BasicTickFormatter"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"67cac86c-72b8-4585-a942-54988072032f","type":"Line"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"4c18dd2c-53ba-4f1c-a21e-6899582b243f","type":"BoxAnnotation"},{"attributes":{"dimension":1,"plot":{"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},"ticker":{"id":"7e767e1e-00b8-4ef7-b921-687287801292","type":"BasicTicker"}},"id":"b1ee50c1-13e3-46bb-9c77-97a9bd12d632","type":"Grid"},{"attributes":{"data_source":{"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"},"glyph":{"id":"41aabb79-37b6-4ae7-b4b6-11b70ca9fc6d","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"67cac86c-72b8-4585-a942-54988072032f","type":"Line"},"selection_glyph":null,"view":{"id":"a8e6f99a-34ab-43c7-a0d4-b3e541f0d288","type":"CDSView"}},"id":"20419767-e365-43ca-81e6-9825076ab4bb","type":"GlyphRenderer"},{"attributes":{},"id":"ccb6843f-9ce1-4b27-8a84-a987a24a630b","type":"HelpTool"},{"attributes":{},"id":"1bfc486c-93df-4d5b-930e-f9da5882e048","type":"WheelZoomTool"},{"attributes":{},"id":"35b56544-6da7-4271-8908-4f3fce307d92","type":"PanTool"},{"attributes":{"source":{"id":"8c3c4789-9ae9-42d3-80f7-27e80d58a21a","type":"ColumnDataSource"}},"id":"a8e6f99a-34ab-43c7-a0d4-b3e541f0d288","type":"CDSView"},{"attributes":{"overlay":{"id":"4c18dd2c-53ba-4f1c-a21e-6899582b243f","type":"BoxAnnotation"}},"id":"35d9a612-6554-4ad5-b26d-a0d65c2f377e","type":"BoxZoomTool"},{"attributes":{},"id":"7e767e1e-00b8-4ef7-b921-687287801292","type":"BasicTicker"},{"attributes":{"overlay":{"id":"37305650-8c49-45b1-9455-e0c8644fb37b","type":"BoxAnnotation"}},"id":"cfa6b691-70d9-4e1b-985f-97fd83304cfd","type":"BoxZoomTool"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"481e6e58-04f8-41c1-bdf2-aef7c1fca52f","type":"BoxAnnotation"},{"attributes":{"callback":null},"id":"8df8a96f-3cd7-4f95-83dd-c3825e64a567","type":"DataRange1d"},{"attributes":{},"id":"6f18901b-1365-4a7f-ad6c-3d848374a721","type":"BasicTickFormatter"},{"attributes":{},"id":"6d5f612b-a75c-4fce-9ee8-76858bce184d","type":"SaveTool"},{"attributes":{},"id":"c8f97c04-e883-4f1a-95b8-a696eae9a182","type":"LinearScale"},{"attributes":{"plot":{"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},"ticker":{"id":"e46e1c2b-1de3-4d31-851e-b78c6d56f88c","type":"BasicTicker"}},"id":"d5154776-fcea-4fce-9125-bd1d04a03ab5","type":"Grid"},{"attributes":{},"id":"0d5bb837-9dad-4302-903e-214ed8fae797","type":"BasicTickFormatter"},{"attributes":{},"id":"b2654f60-ef14-4c29-bf49-d0bec3fbc6eb","type":"WheelZoomTool"},{"attributes":{},"id":"8c423f85-7a8e-4981-b5a4-04ccb0b92e24","type":"ResetTool"},{"attributes":{"callback":{"id":"397a958b-44c9-4ed9-9006-60f700662505","type":"CustomJS"},"end":6.4,"start":0,"step":0.1,"title":"Phase","value":0},"id":"e0459d87-b4f8-4913-a8e8-5e5170aecc84","type":"Slider"},{"attributes":{"formatter":{"id":"c6af5d8e-5988-4f6a-a22a-789a6223bedf","type":"BasicTickFormatter"},"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"}},"id":"e15d54ce-0bbe-44f2-9634-c60f9414f4c4","type":"LinearAxis"},{"attributes":{"callback":null},"id":"feefab8e-27cc-4128-8f39-63df5dd3f66f","type":"DataRange1d"},{"attributes":{},"id":"3c7b2e62-5e5c-4a78-bd0b-2bcad77fc253","type":"HelpTool"},{"attributes":{},"id":"fc40d6d3-72cf-4cd1-baab-78e4b807bb1a","type":"SaveTool"},{"attributes":{"overlay":{"id":"6f5a4156-ca05-409e-ae10-312aeb1e4628","type":"BoxAnnotation"}},"id":"20ad2131-fbf2-451a-b088-cf33c3427790","type":"BoxZoomTool"},{"attributes":{"children":[{"id":"84284e10-679f-4deb-a411-414704175019","type":"Slider"},{"id":"f9d9e39b-1171-4cea-a527-88a635f89582","type":"Slider"},{"id":"e0459d87-b4f8-4913-a8e8-5e5170aecc84","type":"Slider"},{"id":"be8072dc-1e67-48b1-b4e2-adc17f65ae29","type":"Slider"}]},"id":"432f4674-bf4a-451d-9dee-6688059fbb55","type":"WidgetBox"},{"attributes":{},"id":"b2187cb4-e540-49ce-b81a-4be158a4879f","type":"SaveTool"},{"attributes":{"plot":null,"text":""},"id":"147df0ea-ce83-469e-a051-f5fe73fea531","type":"Title"},{"attributes":{"args":{"amp":{"id":"b3b1d5ac-0b55-42d5-9ecc-be3f1ccd9ec0","type":"Slider"},"freq":{"id":"537d39b8-3d33-4eb3-8650-611264eae516","type":"Slider"},"offset":{"id":"a7a30a9b-f1e5-4a9e-a1a9-a59b1f79ed41","type":"Slider"},"phase":{"id":"e6080249-a875-443f-8600-f850fcebe08c","type":"Slider"},"source":{"id":"959803c4-225a-452d-8e89-58193d81cb7d","type":"ColumnDataSource"}},"code":"\n    var data = source.data;\n    var A = amp.value;\n    var k = freq.value;\n    var phi = phase.value;\n    var B = offset.value;\n    x = data['x']\n    y = data['y']\n    for (i = 0; i &lt; x.length; i++) {\n        y[i] = B + A*Math.sin(k*x[i]+phi);\n    }\n    source.change.emit();\n"},"id":"d73793a0-67f5-4fdc-9dfb-90a371dc035e","type":"CustomJS"},{"attributes":{},"id":"30635d51-4c94-4436-b4dc-5152211127e1","type":"ResetTool"},{"attributes":{"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"}},"id":"413b7a2a-58cd-4570-a15b-4774445bc075","type":"Grid"},{"attributes":{},"id":"8cfc810e-d9d0-4b5a-97fb-5c2256f8a6f6","type":"LinearScale"},{"attributes":{"children":[{"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},{"id":"432f4674-bf4a-451d-9dee-6688059fbb55","type":"WidgetBox"}]},"id":"fef30824-891d-4ef9-b09e-8566f6fd3b41","type":"Row"},{"attributes":{},"id":"f5769855-4441-4181-82d3-14e5e1cbfd66","type":"LinearScale"},{"attributes":{"callback":{"id":"397a958b-44c9-4ed9-9006-60f700662505","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Frequency","value":1},"id":"f9d9e39b-1171-4cea-a527-88a635f89582","type":"Slider"},{"attributes":{},"id":"e46e1c2b-1de3-4d31-851e-b78c6d56f88c","type":"BasicTicker"},{"attributes":{},"id":"3b7a65ea-a301-4bc1-9f72-04a488858860","type":"HelpTool"},{"attributes":{},"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"},{"attributes":{},"id":"2720b520-cac6-4614-bf27-4386d6b120bd","type":"BasicTicker"},{"attributes":{},"id":"e0596d28-fa72-4739-b79d-ecb0a0733236","type":"HelpTool"},{"attributes":{"overlay":{"id":"00afcf2f-3d63-4ad5-8c64-b31a05e86987","type":"BoxAnnotation"}},"id":"3818aea3-a7b5-4ca5-a0a9-0b682541f99b","type":"BoxZoomTool"},{"attributes":{},"id":"bded7dd3-fb10-4032-9eab-349a43ba917b","type":"WheelZoomTool"},{"attributes":{},"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"},{"attributes":{"formatter":{"id":"0d5bb837-9dad-4302-903e-214ed8fae797","type":"BasicTickFormatter"},"plot":{"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},"ticker":{"id":"2720b520-cac6-4614-bf27-4386d6b120bd","type":"BasicTicker"}},"id":"8d92c29c-89c3-40d6-9e49-651a4c13ac41","type":"LinearAxis"},{"attributes":{"callback":{"id":"397a958b-44c9-4ed9-9006-60f700662505","type":"CustomJS"},"end":5,"start":-5,"step":0.1,"title":"Offset","value":0},"id":"be8072dc-1e67-48b1-b4e2-adc17f65ae29","type":"Slider"},{"attributes":{"callback":{"id":"d73793a0-67f5-4fdc-9dfb-90a371dc035e","type":"CustomJS"},"end":5,"start":-5,"step":0.1,"title":"Offset","value":0},"id":"a7a30a9b-f1e5-4a9e-a1a9-a59b1f79ed41","type":"Slider"},{"attributes":{},"id":"f5d8b6ef-a1d6-4550-a985-4d3a707ba7bb","type":"PanTool"},{"attributes":{},"id":"f484f0c7-d10b-4813-83ee-dd4ac86f610d","type":"WheelZoomTool"},{"attributes":{"plot":null,"text":""},"id":"ccbe5382-ecae-4e55-8ac3-610208a92c7d","type":"Title"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"3556bfe2-bab1-45cc-a066-591cab89b812","type":"Line"},{"attributes":{"formatter":{"id":"da3e277b-51fd-405f-aa8f-94206ec769a4","type":"BasicTickFormatter"},"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"}},"id":"dd362f70-a96b-41c1-91d0-784da0cd0205","type":"LinearAxis"},{"attributes":{},"id":"a5a931e1-9bb1-496f-a27d-6a4c12de23b0","type":"BasicTicker"},{"attributes":{},"id":"fb163469-2e57-43e6-bdd8-23f69cfd8b8a","type":"BasicTickFormatter"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"plot":null,"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"6f5a4156-ca05-409e-ae10-312aeb1e4628","type":"BoxAnnotation"},{"attributes":{"formatter":{"id":"2e33a094-8602-4d4c-97df-714b0c01c3ba","type":"BasicTickFormatter"},"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"4e280448-750a-46cb-870c-92475291a7db","type":"BasicTicker"}},"id":"8741a13f-2118-4351-9d91-504a45f7fe04","type":"LinearAxis"},{"attributes":{},"id":"2e33a094-8602-4d4c-97df-714b0c01c3ba","type":"BasicTickFormatter"},{"attributes":{},"id":"a398d591-409e-45bd-8152-dbcc472b8636","type":"PanTool"},{"attributes":{"formatter":{"id":"50165239-a8a4-4f1a-bd1c-9cb9f945d866","type":"BasicTickFormatter"},"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"}},"id":"6aedd8c6-8bc2-48c0-b336-f90cf146a6c3","type":"LinearAxis"},{"attributes":{},"id":"7943efd9-9aaa-4f4b-acef-c8f430b76616","type":"LinearScale"},{"attributes":{"callback":{"id":"d73793a0-67f5-4fdc-9dfb-90a371dc035e","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Frequency","value":1},"id":"537d39b8-3d33-4eb3-8650-611264eae516","type":"Slider"},{"attributes":{"callback":{"id":"d73793a0-67f5-4fdc-9dfb-90a371dc035e","type":"CustomJS"},"end":6.4,"start":0,"step":0.1,"title":"Phase","value":0},"id":"e6080249-a875-443f-8600-f850fcebe08c","type":"Slider"},{"attributes":{"dimension":1,"plot":{"id":"025f068b-88d7-4051-9a74-8b3cf4ca3322","subtype":"Figure","type":"Plot"},"ticker":{"id":"51485646-f97e-46bb-8111-2cff98015aae","type":"BasicTicker"}},"id":"d75cca78-1048-4a3e-ab43-0c0e98df5b36","type":"Grid"},{"attributes":{"source":{"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"}},"id":"868d4f51-2b41-47f2-acf2-f1d160fd0999","type":"CDSView"},{"attributes":{"formatter":{"id":"fb163469-2e57-43e6-bdd8-23f69cfd8b8a","type":"BasicTickFormatter"},"plot":{"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},"ticker":{"id":"0811dd43-c233-4225-b002-115ac341ec22","type":"BasicTicker"}},"id":"500c3f32-3892-4bfc-9e8e-8ad7e1a8d123","type":"LinearAxis"},{"attributes":{"children":[{"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},{"id":"0d2048b6-7525-490c-9e28-db36bd628f34","type":"WidgetBox"}]},"id":"49f6f981-c30b-4abe-a9c5-eb13a8d64ac1","type":"Row"},{"attributes":{"data_source":{"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"},"glyph":{"id":"3556bfe2-bab1-45cc-a066-591cab89b812","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"04f140b8-754a-4261-98cb-26b1ff1f2b06","type":"Line"},"selection_glyph":null,"view":{"id":"868d4f51-2b41-47f2-acf2-f1d160fd0999","type":"CDSView"}},"id":"0b8ad283-579c-4eee-bd94-57f8629d95f9","type":"GlyphRenderer"},{"attributes":{"plot":null,"text":""},"id":"e309628c-db02-475a-986b-14c76d3a98ab","type":"Title"},{"attributes":{},"id":"50165239-a8a4-4f1a-bd1c-9cb9f945d866","type":"BasicTickFormatter"},{"attributes":{"plot":null,"text":""},"id":"cebf0126-7d05-4538-b776-62af024d3b43","type":"Title"},{"attributes":{"children":[{"id":"ab96f988-f867-4e43-8162-160adc103d5e","type":"Slider"},{"id":"e0dcba8c-25ec-4af7-a915-3f5aa718acf1","type":"Slider"},{"id":"4a21206c-ec74-4810-ada3-8a2fff22c271","type":"Slider"},{"id":"bda10ed0-05e2-41e3-9fc2-cbe8f5ac3624","type":"Slider"}]},"id":"d02badee-2f93-4932-a5c9-566da115a4a2","type":"WidgetBox"},{"attributes":{},"id":"0811dd43-c233-4225-b002-115ac341ec22","type":"BasicTicker"},{"attributes":{"children":[{"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},{"id":"d02badee-2f93-4932-a5c9-566da115a4a2","type":"WidgetBox"}]},"id":"beee17a2-17b5-40c8-b43d-aea7cf3aa1eb","type":"Row"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"04f140b8-754a-4261-98cb-26b1ff1f2b06","type":"Line"},{"attributes":{"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"d953b8ce-b1cc-47f4-8a1d-742f35c86af4","type":"Line"},{"attributes":{"dimension":1,"plot":{"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},"ticker":{"id":"0811dd43-c233-4225-b002-115ac341ec22","type":"BasicTicker"}},"id":"a8f59fb6-efb9-4211-b60b-4fc603487242","type":"Grid"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"39bb4328-6aee-4ff1-baad-97a894968cdb","type":"Line"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"2a31d9c6-d0f0-4d27-882a-06494c043151","type":"ColumnDataSource"},{"attributes":{"dimension":1,"plot":{"id":"b90f0883-95b6-4cd2-b378-ceab8964a7b5","subtype":"Figure","type":"Plot"},"ticker":{"id":"c159ca20-b2e9-4e75-9ee1-8ce417051b28","type":"BasicTicker"}},"id":"ced5469e-a711-4588-afca-f1dabd241d07","type":"Grid"},{"attributes":{"source":{"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"}},"id":"1d22a5ff-7969-43d0-a244-d06d00adb013","type":"CDSView"},{"attributes":{"source":{"id":"0df2dffe-f023-469d-a8e7-d31674abbb0a","type":"ColumnDataSource"}},"id":"5c23c9ca-3e0e-4330-83f0-ffa23cf4b5c0","type":"CDSView"},{"attributes":{"line_alpha":0.1,"line_color":"#1f77b4","line_width":2,"x":{"field":"x"},"y":{"field":"y"}},"id":"dcd07e66-8b65-4886-a0e6-0187c3783c46","type":"Line"},{"attributes":{},"id":"44d69179-0506-48ef-8872-3e74205932d5","type":"PanTool"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"},{"attributes":{"callback":{"id":"00c622b4-2925-4ca5-af9a-bcc7cf7a390d","type":"CustomJS"},"end":5,"start":-5,"step":0.1,"title":"Offset","value":0},"id":"bda10ed0-05e2-41e3-9fc2-cbe8f5ac3624","type":"Slider"},{"attributes":{},"id":"af97642d-848a-4657-86c8-18caef9acc80","type":"ResetTool"},{"attributes":{"data_source":{"id":"d29c79fa-76e7-49d7-86af-613b8a30e436","type":"ColumnDataSource"},"glyph":{"id":"d953b8ce-b1cc-47f4-8a1d-742f35c86af4","type":"Line"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"dcd07e66-8b65-4886-a0e6-0187c3783c46","type":"Line"},"selection_glyph":null,"view":{"id":"1d22a5ff-7969-43d0-a244-d06d00adb013","type":"CDSView"}},"id":"1db047e9-5900-4f6d-b5a7-d5aff16b4770","type":"GlyphRenderer"},{"attributes":{"plot":{"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},"ticker":{"id":"2720b520-cac6-4614-bf27-4386d6b120bd","type":"BasicTicker"}},"id":"f8895768-5b0c-40b9-a4ec-c5cd3c2f707c","type":"Grid"},{"attributes":{"below":[{"id":"8d92c29c-89c3-40d6-9e49-651a4c13ac41","type":"LinearAxis"}],"left":[{"id":"500c3f32-3892-4bfc-9e8e-8ad7e1a8d123","type":"LinearAxis"}],"plot_height":400,"plot_width":400,"renderers":[{"id":"8d92c29c-89c3-40d6-9e49-651a4c13ac41","type":"LinearAxis"},{"id":"f8895768-5b0c-40b9-a4ec-c5cd3c2f707c","type":"Grid"},{"id":"500c3f32-3892-4bfc-9e8e-8ad7e1a8d123","type":"LinearAxis"},{"id":"a8f59fb6-efb9-4211-b60b-4fc603487242","type":"Grid"},{"id":"6f5a4156-ca05-409e-ae10-312aeb1e4628","type":"BoxAnnotation"},{"id":"bd13252c-9074-4e4b-b799-cbaa60b06ae9","type":"GlyphRenderer"}],"title":{"id":"e309628c-db02-475a-986b-14c76d3a98ab","type":"Title"},"toolbar":{"id":"376694f2-29b5-4cc5-bf57-d2c898627bb0","type":"Toolbar"},"x_range":{"id":"8df8a96f-3cd7-4f95-83dd-c3825e64a567","type":"DataRange1d"},"x_scale":{"id":"7943efd9-9aaa-4f4b-acef-c8f430b76616","type":"LinearScale"},"y_range":{"id":"0726e76d-9395-4a17-812a-85dcc87c740a","type":"Range1d"},"y_scale":{"id":"8cfc810e-d9d0-4b5a-97fb-5c2256f8a6f6","type":"LinearScale"}},"id":"f5e39ffe-00ae-43f4-9577-6cbf5a46e7bf","subtype":"Figure","type":"Plot"},{"attributes":{"args":{"amp":{"id":"ab96f988-f867-4e43-8162-160adc103d5e","type":"Slider"},"freq":{"id":"e0dcba8c-25ec-4af7-a915-3f5aa718acf1","type":"Slider"},"offset":{"id":"bda10ed0-05e2-41e3-9fc2-cbe8f5ac3624","type":"Slider"},"phase":{"id":"4a21206c-ec74-4810-ada3-8a2fff22c271","type":"Slider"},"source":{"id":"0df2dffe-f023-469d-a8e7-d31674abbb0a","type":"ColumnDataSource"}},"code":"\n    var data = source.data;\n    var A = amp.value;\n    var k = freq.value;\n    var phi = phase.value;\n    var B = offset.value;\n    x = data['x']\n    y = data['y']\n    for (i = 0; i &lt; x.length; i++) {\n        y[i] = B + A*Math.sin(k*x[i]+phi);\n    }\n    source.change.emit();\n"},"id":"00c622b4-2925-4ca5-af9a-bcc7cf7a390d","type":"CustomJS"},{"attributes":{"formatter":{"id":"1195cf55-a404-443b-a6a9-38988b72673b","type":"BasicTickFormatter"},"plot":{"id":"d06a2e27-c4ed-4e08-9ca6-dec70fc2c593","subtype":"Figure","type":"Plot"},"ticker":{"id":"e46e1c2b-1de3-4d31-851e-b78c6d56f88c","type":"BasicTicker"}},"id":"ebd39a2b-e7d4-4ded-910a-41e56914f3d4","type":"LinearAxis"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"ef916a0f-17f9-47fa-9b91-cd39031324f7","type":"PanTool"},{"id":"1bfc486c-93df-4d5b-930e-f9da5882e048","type":"WheelZoomTool"},{"id":"20ad2131-fbf2-451a-b088-cf33c3427790","type":"BoxZoomTool"},{"id":"13beec82-da72-4954-b0a4-bd02a03353eb","type":"SaveTool"},{"id":"284bbf91-0288-49ee-976a-f78b81bfa7c7","type":"ResetTool"},{"id":"742c1cc8-2c58-40a8-8573-65a22a141647","type":"HelpTool"}]},"id":"376694f2-29b5-4cc5-bf57-d2c898627bb0","type":"Toolbar"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"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","dtype":"float64","shape":[500]},"y":{"__ndarray__":"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","dtype":"float64","shape":[500]}}},"id":"0df2dffe-f023-469d-a8e7-d31674abbb0a","type":"ColumnDataSource"},{"attributes":{"args":{"amp":{"id":"84284e10-679f-4deb-a411-414704175019","type":"Slider"},"freq":{"id":"f9d9e39b-1171-4cea-a527-88a635f89582","type":"Slider"},"offset":{"id":"be8072dc-1e67-48b1-b4e2-adc17f65ae29","type":"Slider"},"phase":{"id":"e0459d87-b4f8-4913-a8e8-5e5170aecc84","type":"Slider"},"source":{"id":"08c4a922-77b4-4250-a13a-9fddf147787c","type":"ColumnDataSource"}},"code":"\n    var data = source.data;\n    var A = amp.value;\n    var k = freq.value;\n    var phi = phase.value;\n    var B = offset.value;\n    x = data['x']\n    y = data['y']\n    for (i = 0; i &lt; x.length; i++) {\n        y[i] = B + A*Math.sin(k*x[i]+phi);\n    }\n    source.change.emit();\n"},"id":"397a958b-44c9-4ed9-9006-60f700662505","type":"CustomJS"},{"attributes":{"callback":null,"column_names":["x","y"],"data":{"x":{"__ndarray__":"AAAAAAAAFMCamZmZmZkTwDQzMzMzMxPAzszMzMzMEsBoZmZmZmYSwAIAAAAAABLAnJmZmZmZEcA2MzMzMzMRwNDMzMzMzBDAamZmZmZmEMAEAAAAAAAQwDwzMzMzMw/AcGZmZmZmDsCkmZmZmZkNwNjMzMzMzAzADAAAAAAADMBAMzMzMzMLwHRmZmZmZgrAqJmZmZmZCcDczMzMzMwIwBAAAAAAAAjARDMzMzMzB8B4ZmZmZmYGwKyZmZmZmQXA4MzMzMzMBMAUAAAAAAAEwEgzMzMzMwPAfGZmZmZmAsCwmZmZmZkBwOTMzMzMzADAGAAAAAAAAMCYZmZmZmb+vwDNzMzMzPy/aDMzMzMz+7/QmZmZmZn5vzgAAAAAAPi/oGZmZmZm9r8IzczMzMz0v3AzMzMzM/O/2JmZmZmZ8b9AAAAAAADwv1DNzMzMzOy/IJqZmZmZ6b/wZmZmZmbmv8AzMzMzM+O/kAAAAAAA4L/AmpmZmZnZv2A0MzMzM9O/AJyZmZmZyb+AnpmZmZm5vwAAAAAAABS9gJSZmZmZuT8Al5mZmZnJP+AxMzMzM9M/QJiZmZmZ2T+g/v/////fP4AyMzMzM+M/sGVmZmZm5j/gmJmZmZnpPxDMzMzMzOw/QP//////7z84mZmZmZnxP9AyMzMzM/M/aMzMzMzM9D8AZmZmZmb2P5j///////c/MJmZmZmZ+T/IMjMzMzP7P2DMzMzMzPw/+GVmZmZm/j+Q////////P5TMzMzMzABAYJmZmZmZAUAsZmZmZmYCQPgyMzMzMwNAxP//////A0CQzMzMzMwEQFyZmZmZmQVAKGZmZmZmBkD0MjMzMzMHQMD//////wdAjMzMzMzMCEBYmZmZmZkJQCRmZmZmZgpA8DIzMzMzC0C8//////8LQIjMzMzMzAxAVJmZmZmZDUAgZmZmZmYOQOwyMzMzMw9AuP//////D0BCZmZmZmYQQKjMzMzMzBBADjMzMzMzEUB0mZmZmZkRQNr//////xFAQGZmZmZmEkCmzMzMzMwSQAwzMzMzMxNAcpmZmZmZE0A=","dtype":"float64","shape":[100]},"y":{"__ndarray__":"M5ng9YGv7j+k6yJ2QHDvP/89DJqU4O8/U+w3D1//7z+77F0TUczvP4RAxz3tR+8/9ChFMYZz7j8vigE6O1HtP+MS0N/y4+s/y8jielMv6j/EHtzduTfoP+WzHy0uAuY/vcDs/1aU4z9iGy7savTgP0DS8ENBUtw/eWK6fjtz1j/RTP8QyVrQP2QNt8oCMcQ/sTDoOTfjrT9vDJiCD0qlv17Utm04EMK/2i9I1rifzr9A5V0jcnDVv4qmJCQxWtu/eoWgu/x+4L9wys8Nrybjv13Rw/VkneW/6i5YLdHc57/U1qrZM9/pv81y6z9pn+u/HrTR6vYY7b83OBofF0juv9kVvYHCKe+/qZgv17e777/CQuDHgfzvv5NtLJt66++/FuFE382I779rHsX5d9Xuv/FVGqJD0+2/cGMtTMWE7L8zDQmPVO3qv5KoXZgDEem/Kv+/wpT05r8iGkZpbp3kv81BpReNEeK/7QZLdOiu3r96aCvpOuzYv1O7upXN6dK/FtU9I/9tyb8M0IvLro65vwAAAAAAABS9GcaLy66OuT8w0D0j/23JP++4upXN6dI/LGYr6Trs2D+7BEt06K7eP8VApReNEeI/LRlGaW6d5D9M/r/ClPTmP8unXZgDEek/hgwJj1Tt6j/fYi1MxYTsP31VGqJD0+0/FR7F+XfV7j/g4ETfzYjvP3xtLJt66+8/zELgx4H87z/SmC/Xt7vvPyIWvYHCKe8/nzgaHxdI7j+jtNHq9hjtP25z6z9pn+s/kNeq2TPf6T+/L1gt0dznP0nSw/VkneU/ccvPDa8m4z+MhqC7/H7gP8yoJCQxWts/m+ddI3Jw1T+1NEjWuJ/OP1HZtm04EMI/aiCYgg9KpT+5HOg5N+Otv3QIt8oCMcS/Zkr/EMla0L8iYLp+O3PWvwLQ8ENBUty/Uxou7Gr04L/Av+z/VpTjv/yyHy0uAua/8h3c3bk36L8TyOJ6Uy/qv0cS0N/y4+u/r4kBOjtR7b+SKEUxhnPuv0BAxz3tR++/l+xdE1HM779P7DcPX//vvxs+DJqU4O+/3+sidkBw778=","dtype":"float64","shape":[100]}}},"id":"08c4a922-77b4-4250-a13a-9fddf147787c","type":"ColumnDataSource"},{"attributes":{"below":[{"id":"efd02180-9db1-4cf3-86b6-c38a49736567","type":"LinearAxis"}],"left":[{"id":"e86a27e5-7feb-4207-8105-4517c6552cda","type":"LinearAxis"}],"plot_height":400,"renderers":[{"id":"efd02180-9db1-4cf3-86b6-c38a49736567","type":"LinearAxis"},{"id":"8170b6df-ae63-4908-8d4b-7ef6d93029cc","type":"Grid"},{"id":"e86a27e5-7feb-4207-8105-4517c6552cda","type":"LinearAxis"},{"id":"3da1cb0e-cdd6-4dea-b510-d6c277174994","type":"Grid"},{"id":"4c18dd2c-53ba-4f1c-a21e-6899582b243f","type":"BoxAnnotation"},{"id":"cb2f4e04-6c88-4f2b-bfd3-2f48d68738e8","type":"GlyphRenderer"}],"title":{"id":"147df0ea-ce83-469e-a051-f5fe73fea531","type":"Title"},"toolbar":{"id":"90aef6d6-4259-45f0-9d9b-b02a2b553fbb","type":"Toolbar"},"x_range":{"id":"feefab8e-27cc-4128-8f39-63df5dd3f66f","type":"DataRange1d"},"x_scale":{"id":"c8f97c04-e883-4f1a-95b8-a696eae9a182","type":"LinearScale"},"y_range":{"id":"ac914a19-279f-4e09-9c5c-80b2713edf08","type":"Range1d"},"y_scale":{"id":"0692b593-a76c-4214-babe-81612f4ce5f7","type":"LinearScale"}},"id":"9e85073b-3834-446f-a92c-decb9137e4a7","subtype":"Figure","type":"Plot"},{"attributes":{"callback":{"id":"00c622b4-2925-4ca5-af9a-bcc7cf7a390d","type":"CustomJS"},"end":10,"start":0.1,"step":0.1,"title":"Amplitude","value":1},"id":"ab96f988-f867-4e43-8162-160adc103d5e","type":"Slider"},{"attributes":{"line_alpha":0.6,"line_color":"#1f77b4","line_width":3,"x":{"field":"x"},"y":{"field":"y"}},"id":"be28ff81-13f5-4a6a-87cf-cee63a353dc3","type":"Line"},{"attributes":{},"id":"c6af5d8e-5988-4f6a-a22a-789a6223bedf","type":"BasicTickFormatter"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_scroll":"auto","active_tap":"auto","tools":[{"id":"35b56544-6da7-4271-8908-4f3fce307d92","type":"PanTool"},{"id":"b2654f60-ef14-4c29-bf49-d0bec3fbc6eb","type":"WheelZoomTool"},{"id":"35d9a612-6554-4ad5-b26d-a0d65c2f377e","type":"BoxZoomTool"},{"id":"6d5f612b-a75c-4fce-9ee8-76858bce184d","type":"SaveTool"},{"id":"8c423f85-7a8e-4981-b5a4-04ccb0b92e24","type":"ResetTool"},{"id":"3c7b2e62-5e5c-4a78-bd0b-2bcad77fc253","type":"HelpTool"}]},"id":"90aef6d6-4259-45f0-9d9b-b02a2b553fbb","type":"Toolbar"},{"attributes":{"callback":null,"end":10,"start":-10},"id":"0726e76d-9395-4a17-812a-85dcc87c740a","type":"Range1d"},{"attributes":{"callback":null,"end":10,"start":-10},"id":"ac914a19-279f-4e09-9c5c-80b2713edf08","type":"Range1d"}],"root_ids":["025f068b-88d7-4051-9a74-8b3cf4ca3322","b90f0883-95b6-4cd2-b378-ceab8964a7b5","beee17a2-17b5-40c8-b43d-aea7cf3aa1eb","fef30824-891d-4ef9-b09e-8566f6fd3b41","49f6f981-c30b-4abe-a9c5-eb13a8d64ac1"]},"title":"Bokeh Application","version":"0.12.14"}}</script><script type="text/javascript">  (function() {    var fn = function() {      Bokeh.safely(function() {        (function(root) {          function embed_document(root) {          var docs_json = document.getElementById('51bd30f2-f016-428d-bf01-dfa13a5641fa').textContent;          var render_items = [{"docid":"c735e38e-6c46-4737-ab01-75ad9c85e84a","elementid":"d5b7c7a5-8fef-4f8f-9395-7643af0ca730","modelid":"49f6f981-c30b-4abe-a9c5-eb13a8d64ac1"}];          root.Bokeh.embed.embed_items(docs_json, render_items);          }          if (root.Bokeh !== undefined) {            embed_document(root);          } else {            var attempts = 0;            var timer = setInterval(function(root) {              if (root.Bokeh !== undefined) {                embed_document(root);                clearInterval(timer);              }              attempts++;              if (attempts > 100) {                console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing")                clearInterval(timer);              }            }, 10, root)          }        })(window);      });    };    if (document.readyState != "loading") fn();    else document.addEventListener("DOMContentLoaded", fn);  })();</script><h2 id="HTML-files"><a href="#HTML-files" class="headerlink" title="HTML files"></a>HTML files</h2><p>这是最直接不容易出错的方法，就是直接使用生成的 HTML 文件。</p><p>假设我们的博客文章使用 Markdown 编写，我们首先使用下面代码生成所需要的 HTML 文件（来自 <a href="https://bokeh.pydata.org/en/latest/docs/gallery/slider.html" target="_blank" rel="noopener">slider.py — Bokeh 0.12.14 documentation</a>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bokeh.layouts <span class="keyword">import</span> row, widgetbox</span><br><span class="line"><span class="keyword">from</span> bokeh.models <span class="keyword">import</span> CustomJS, Slider</span><br><span class="line"><span class="keyword">from</span> bokeh.plotting <span class="keyword">import</span> figure, output_file, show, ColumnDataSource</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line">source = ColumnDataSource(data=dict(x=x, y=y))</span><br><span class="line"></span><br><span class="line">plot = figure(y_range=(<span class="number">-10</span>, <span class="number">10</span>), plot_width=<span class="number">400</span>, plot_height=<span class="number">400</span>)</span><br><span class="line"></span><br><span class="line">plot.line(<span class="string">'x'</span>, <span class="string">'y'</span>, source=source, line_width=<span class="number">3</span>, line_alpha=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line">callback = CustomJS(args=dict(source=source), code=<span class="string">"""</span></span><br><span class="line"><span class="string">    var data = source.data;</span></span><br><span class="line"><span class="string">    var A = amp.value;</span></span><br><span class="line"><span class="string">    var k = freq.value;</span></span><br><span class="line"><span class="string">    var phi = phase.value;</span></span><br><span class="line"><span class="string">    var B = offset.value;</span></span><br><span class="line"><span class="string">    x = data['x']</span></span><br><span class="line"><span class="string">    y = data['y']</span></span><br><span class="line"><span class="string">    for (i = 0; i &lt; x.length; i++) &#123;</span></span><br><span class="line"><span class="string">        y[i] = B + A*Math.sin(k*x[i]+phi);</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    source.change.emit();</span></span><br><span class="line"><span class="string">"""</span>)</span><br><span class="line"></span><br><span class="line">amp_slider = Slider(start=<span class="number">0.1</span>, end=<span class="number">10</span>, value=<span class="number">1</span>, step=<span class="number">.1</span>,</span><br><span class="line">                    title=<span class="string">"Amplitude"</span>, callback=callback)</span><br><span class="line">callback.args[<span class="string">"amp"</span>] = amp_slider</span><br><span class="line"></span><br><span class="line">freq_slider = Slider(start=<span class="number">0.1</span>, end=<span class="number">10</span>, value=<span class="number">1</span>, step=<span class="number">.1</span>,</span><br><span class="line">                     title=<span class="string">"Frequency"</span>, callback=callback)</span><br><span class="line">callback.args[<span class="string">"freq"</span>] = freq_slider</span><br><span class="line"></span><br><span class="line">phase_slider = Slider(start=<span class="number">0</span>, end=<span class="number">6.4</span>, value=<span class="number">0</span>, step=<span class="number">.1</span>,</span><br><span class="line">                      title=<span class="string">"Phase"</span>, callback=callback)</span><br><span class="line">callback.args[<span class="string">"phase"</span>] = phase_slider</span><br><span class="line"></span><br><span class="line">offset_slider = Slider(start=<span class="number">-5</span>, end=<span class="number">5</span>, value=<span class="number">0</span>, step=<span class="number">.1</span>,</span><br><span class="line">                       title=<span class="string">"Offset"</span>, callback=callback)</span><br><span class="line">callback.args[<span class="string">"offset"</span>] = offset_slider</span><br><span class="line"></span><br><span class="line">layout = row(</span><br><span class="line">    plot,</span><br><span class="line">    widgetbox(amp_slider, freq_slider, phase_slider, offset_slider),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output_file(<span class="string">"slider.html"</span>, title=<span class="string">"slider.py example"</span>)</span><br><span class="line"></span><br><span class="line">show(layout)</span><br></pre></td></tr></table></figure><p>运行结束后，会在当前目录生成 <code>slider.html</code>，并在浏览器中自动打开这个网页。我们所需要的代码是这个 HTML 文件中 <code>&lt;div class=&quot;bk-root&quot;&gt;</code> 这个 div 块以及 <code>&lt;script type=&quot;application/json&quot; id=&quot;id&quot;&gt;</code> 和 <code>&lt;script type=&quot;text/javascript&quot;&gt;</code> 中的内容，也是 <code>body</code> 中的内容，如下图，内容太长，我就只圈出了标签名字：</p><p><img src="https://i.imgur.com/o40boQm.png" alt=""></p><p><img src="https://i.imgur.com/vkXqGvS.png" alt=""></p><p>然后我们直接复制这三个标签中的内容到博文的 md 文件中即可。</p><h2 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h2><p>这个方法是直接生成绘图所需用到的 <code>script</code> 和 <code>div</code> 块，然后直接复制到博文的 md 文件中，只不过这里得到的是一个 <code>script</code> 和一个 <code>div</code>。但是这种方法极其容易出错，所得到的 <code>script</code> 和 <code>div</code> 内容不太对，在我处理了一个又一个报错后依然不行，错误率很高，说明还不太完善，错误多地我都不知道怎么去提 issue，从何提起 😆。最后果断放弃了这个方法，有成功的同学可以在下方留言告知，谢谢 😄</p><p>简单说下这个方法的程序逻辑：</p><ol><li>使用 Bokeh 绘图。</li><li><p>使用 <code>bokeh.embed.components</code> 方法得到绘图对象对应的 <code>script</code> 和 <code>div</code> 标签，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># some other codes</span></span><br><span class="line">p = figure()</span><br><span class="line">p.line(x, y)</span><br><span class="line"><span class="comment"># some other plot codes</span></span><br><span class="line">script, div = components(p)</span><br></pre></td></tr></table></figure></li><li><p>复制得到的 <code>script</code> 和 <code>div</code> 到博文的 md 文件中，Done。</p></li></ol><h2 id="Autoload-Scripts"><a href="#Autoload-Scripts" class="headerlink" title="Autoload Scripts"></a>Autoload Scripts</h2><p>这是使用自动加载的脚本，需要把绘图脚本放在自己的服务器上。这里给出一个官方的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bokeh.resources <span class="keyword">import</span> CDN</span><br><span class="line"><span class="keyword">from</span> bokeh.plotting <span class="keyword">import</span> figure</span><br><span class="line"><span class="keyword">from</span> bokeh.embed <span class="keyword">import</span> autoload_static</span><br><span class="line"></span><br><span class="line">plot = figure()</span><br><span class="line">plot.circle([<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">js, tag = autoload_static(plot, CDN, <span class="string">"some/path"</span>)</span><br></pre></td></tr></table></figure><p>所以你需要把生成的 <code>js</code> 放到 <code>some/path</code> 里。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>总的来说，还是第一种方法靠谱。通用的步骤就是：</p><ol><li>创建绘图对象。</li><li>得到绘图所需的 HTML 代码。</li><li>复制这些代码到 md 文件。</li></ol><p>最终的 md 文件大概是下面这个样子：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 你的标题</span><br><span class="line">date: 日期</span><br><span class="line">tags: 你的标签</span><br><span class="line">---</span><br><span class="line">&lt;link</span><br><span class="line">    href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css"</span><br><span class="line">    rel="stylesheet" type="text/css"&gt;</span><br><span class="line">&lt;link</span><br><span class="line">    href="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css"</span><br><span class="line">    rel="stylesheet" type="text/css"&gt;</span><br><span class="line">&lt;link</span><br><span class="line">    href="https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css"</span><br><span class="line">    rel="stylesheet" type="text/css"&gt;</span><br><span class="line"></span><br><span class="line">&lt;script src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js"&gt;&lt;/script&gt;</span><br><span class="line">&lt;script src="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js"&gt;&lt;/script&gt;</span><br><span class="line">&lt;script src="https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js"&gt;&lt;/script&gt;</span><br><span class="line">……</span><br><span class="line">这里是你的一些博客内容</span><br><span class="line">……</span><br><span class="line"></span><br><span class="line">在需要显示图的时候，把绘图所需代码放复制到这</span><br><span class="line"></span><br><span class="line">……</span><br><span class="line">这里是你的一些博客内容</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>其中 <code>-widgets</code> 在图中有 widgets 的情况使用，<code>-tables</code> 在有 table 的情况下使用。</p><p>写的不太好，我也是琢磨了好长时间，有问题欢迎在下面留言交流 😄。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://bokeh.pydata.org/en/latest/docs/user_guide/embed.html" target="_blank" rel="noopener">Embedding Plots and Apps — Bokeh 0.12.14 documentation</a></li><li><a href="https://bokeh.pydata.org/en/latest/docs/user_guide/concepts.html" target="_blank" rel="noopener">Defining Key Concepts — Bokeh 0.12.14 documentation</a></li><li><a href="https://bokeh.pydata.org/en/latest/docs/user_guide/interaction.html" target="_blank" rel="noopener">Adding Interactions — Bokeh 0.12.14 documentation</a></li><li><a href="https://bokeh.pydata.org/en/latest/docs/user_guide/notebook.html" target="_blank" rel="noopener">Working in the Notebook — Bokeh 0.12.14 documentation</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;link href=&quot;https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;link rel=&quot;stylesheet&quot; h
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://secsilm.github.io/tags/hexo/"/>
    
      <category term="Python" scheme="https://secsilm.github.io/tags/Python/"/>
    
      <category term="Data Science" scheme="https://secsilm.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>曾经我也有一位 Stephen Hawking</title>
    <link href="https://secsilm.github.io/2018/03/14/stephen-hawking/"/>
    <id>https://secsilm.github.io/2018/03/14/stephen-hawking/</id>
    <published>2018-03-14T05:28:27.000Z</published>
    <updated>2018-03-14T07:04:35.171Z</updated>
    
    <content type="html"><![CDATA[<p>今天去河西找我老师说论文和签字，结束后就去等校车，本以为 12 点半的校车结果我们 11 点 50 到的时候校车已经停在那了，于是上车找到座位坐下，没多少人，看到手机即刻有个 Breaking News 的通知：据卫报等多家英媒报道，史蒂芬·霍金逝世，享年 76 岁。</p><p><img src="https://i.imgur.com/5idgZ2l.png" alt="jike"></p><p>感觉先是震惊，不敢相信，然后是不信，遂去 Twitter 上看了看，果然。</p><p>眼泪要流出来，因为他是我目前的生命中的一个重要精神依赖和偶像，由于一些特殊原因，我总是可以在他身上找到我要的坚持。</p><p>车马上要开了，我看手机容易晕车，于是迅速打开网易云音乐，戴上耳机，点了一首生活大爆炸主题曲 <a href="http://music.163.com/#/song?id=2193514" target="_blank" rel="noopener">The Big Bang Theory Theme</a> 和一首纯钢琴版本。</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27510197&auto=0&height=66"></iframe><p>钢琴是深沉的。原本欢快活泼的音乐，现在听起来是那么悲伤。我在朋友圈说：依稀记得电影中你年轻的模样，宇宙为你荡漾。是啊，宇宙为你荡漾……</p><p>记得不久前又看了一遍的的<a href="https://zh.wikipedia.org/zh-cn/%E6%84%9B%E7%9A%84%E8%90%AC%E7%89%A9%E8%AB%96" target="_blank" rel="noopener">《万物理论》</a>，依稀还记得电影中他青涩年轻的模样，恋爱的单纯，脑力的非同寻常。</p><p>从河西到泰达，一个小时的车程，原本会在车上睡觉的我，看着窗外想了一路，想的太多，很多次眼泪差些出来，但是到现在写的时候，我却写不出来。可能，在路上，更适合怀念和忧伤。</p><p>曾经，我也有一位 Stephen Hawking。他是我父亲。现在，他们都不在了。我父亲也是一位伟大的人，也曾与命运抗争，也曾战胜命运。</p><p>满肚子的话，现在写不下去了，原来自己不如父亲那样坚强。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天去河西找我老师说论文和签字，结束后就去等校车，本以为 12 点半的校车结果我们 11 点 50 到的时候校车已经停在那了，于是上车找到座位坐下，没多少人，看到手机即刻有个 Breaking News 的通知：据卫报等多家英媒报道，史蒂芬·霍金逝世，享年 76 岁。&lt;/p
      
    
    </summary>
    
    
      <category term="Living" scheme="https://secsilm.github.io/tags/Living/"/>
    
  </entry>
  
  <entry>
    <title>使用 CloudFlare 为 hexo 博客实现 HTTPS</title>
    <link href="https://secsilm.github.io/2018/03/09/using-https-namecheap-hexo/"/>
    <id>https://secsilm.github.io/2018/03/09/using-https-namecheap-hexo/</id>
    <published>2018-03-09T13:34:53.000Z</published>
    <updated>2018-03-12T13:57:58.016Z</updated>
    
    <content type="html"><![CDATA[<p>过了个年，好久没更博客了，忙着各种事。</p><p>博客地址之前一直都是默认的 secsilm.github.io，一直想换个自己的域名，正巧看到 <a href="https://namebeta.com/" target="_blank" rel="noopener">NameBeta</a> 上好几个域名都好便宜，就准备买个搞搞，最后挑中了 <code>alanlee.fun</code> 这个域名。</p><p>把自己的博客域名换成自己买的域名并实现 HTTPS（GitHub Pages 不支持自定义域名 HTTPS），大概需要这么几个步骤（以在 <a href="https://www.namecheap.com/" target="_blank" rel="noopener">namecheap</a> 上购买域名为例）：</p><ol><li>在 namecheap 购买自己心仪的域名</li><li>更新 GitHub 仓库</li><li>CloudFlare 端 DNS 配置</li><li>namecheap 端更改 nameservers</li><li>CloudFlare 端设置使用 HTTPS</li></ol><p>第一步购买域名我就不赘述了，只是需要注意的是付款不支持支付宝，支持 PayPal。</p><p>以下假设已经购买到 <code>alanlee.fun</code> 这个域名。</p><h2 id="更新-GitHub-仓库"><a href="#更新-GitHub-仓库" class="headerlink" title="更新 GitHub 仓库"></a>更新 GitHub 仓库</h2><p>在你的 hexo 博客<strong>本地</strong>根目录中的 <code>source</code> 目录下增加一个 <a href="https://en.wikipedia.org/wiki/CNAME_record" target="_blank" rel="noopener"><code>CNAME</code> 文件</a>（无后缀名），里面写上购买的域名，本文中即 <code>alanlee.fun</code>：</p><p><img src="https://i.imgur.com/q5k9tYD.png" alt="cname"></p><p>然后把更新部署到 GitHub 上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure><p>此时你应该可以在 GitHub 上看到 <code>CNAME</code> 文件了，并且在你的仓库设置中也可以看到你的自定义域名也已经加上了，此时访问 <code>alanlee.fun</code> 应该可以进入博客了，但是此时网址栏是没有「小绿锁」的（HTTPS），要想让这个「小绿锁」出现，我们需要使用 <a href="https://www.cloudflare.com/" target="_blank" rel="noopener">CloudFlare</a> 的服务。</p><p><img src="https://i.imgur.com/ilN6DSD.png" alt="cname-github"></p><p><img src="https://i.imgur.com/ZKHOZxa.png" alt="settings"></p><h2 id="CloudFlare-端-DNS-配置"><a href="#CloudFlare-端-DNS-配置" class="headerlink" title="CloudFlare 端 DNS 配置"></a>CloudFlare 端 DNS 配置</h2><p>首先你需要注册一个 CloudFlare 账号并把自己的域名添加上去，这个过程很简单，我就不赘述了。注册完登陆之后，点击下图中的 <code>DNS</code> 选项卡，另一个 <code>Page Rules</code>  是我们一会儿要用到的。</p><p><img src="https://i.imgur.com/xPpJM44.png" alt=""></p><p>然后如下图一样添加两条记录，<code>Value</code> 处都写自己原来的博客地址，即是 <code>your-github-username.github.com</code> 这样的形式，例如我的 <code>secsilm.github.com</code>：</p><p><img src="https://i.imgur.com/a2lz085.png" alt=""></p><p>按理说我们下一步要在 <code>Page Rules</code> 那设置让我们的域名使用 HTTPS 链接，但是你如果现在就这么做的话你就会发现你设置不了。这是因为你必须在域名提供商那把 nameservers 设为 CloudFlare 的 DNS 才能使用他的 HTTPS 服务，所以我们现在得去 namecheap 那设置一下 nameservers，否则在 CloudFlare 上 <code>Overview</code> 处你的域名会显示未激活状态，而不是下面的 active 状态：</p><p><img src="https://i.imgur.com/4kCLOCM.png" alt=""></p><h2 id="namecheap-端更改-nameservers"><a href="#namecheap-端更改-nameservers" class="headerlink" title="namecheap 端更改 nameservers"></a>namecheap 端更改 nameservers</h2><p>登录进 namecheap，点击右上角 <code>Account</code> → <code>Domain List</code>，在 <code>NAMESERVERS</code> 处选择 <code>Custom DNS</code>，然后填上 CloudFlare 的nameservers（可以在 CloudFlare 上 <code>Overview</code> 选项卡页面找到）：</p><p><img src="https://i.imgur.com/2yZ7ZKs.png" alt=""></p><p>至此，namecheap 端的更改就结束了。</p><h2 id="CloudFlare-端设置使用-HTTPS"><a href="#CloudFlare-端设置使用-HTTPS" class="headerlink" title="CloudFlare 端设置使用 HTTPS"></a>CloudFlare 端设置使用 HTTPS</h2><p>现在就只需要在 <code>Page Rules</code> 选项卡页面创建几个 page rules 就行了，免费用户最多创建 3 个：</p><p>第一个 👇：<br><img src="https://i.imgur.com/zNcNL0y.png" alt="rule1"></p><p>第二个 👇，可选，让所有非 WWW 的链接安全地转到带 WWW 的链接：<br><img src="https://i.imgur.com/QlYf80u.png" alt="rule2"></p><p>第三个 👇，可选，这会在 CloudFlare 的 CDN 中缓存你的静态页面：<br><img src="https://i.imgur.com/Roo2Uk0.png" alt="rule3"></p><p>至此，<a href="https://alanlee.fun/" target="_blank" rel="noopener"><code>alanlee.fun</code></a> 已经可以安全的访问了，「小绿锁」已经出现了：</p><p><img src="https://i.imgur.com/rfMJBdS.jpg" alt="alanlee.fun"></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>其实这种方法并不是百分百的 HTTPS 链接，你的博客和 CloudFlare 之间还是 HTTP 链接，浏览器和 CloudFlare 之间则是 HTTPS 链接，引用 CloudFlare 官方说明：</p><blockquote><p>Firstly a word on security. If you are deploying a JavaScript app which communicates with remote APIs, be sure not to use this for sensitive data submissions. As <a href="https://help.github.com/articles/what-are-github-pages/" target="_blank" rel="noopener">GitHub themselves put it</a>: “GitHub Pages sites shouldn’t be used for sensitive transactions like sending passwords or credit card numbers.” Also bear in mind your website source files are publicly accessible in a Git repository, so be extra careful about what you put there.  </p><p>There are some things we can’t do; GitHub Pages doesn’t let us set custom headers, which unfortunately means we can’t do HTTP/2 Server Push right now.</p></blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://blog.cloudflare.com/secure-and-fast-github-pages-with-cloudflare/" target="_blank" rel="noopener">Secure and fast GitHub Pages with CloudFlare</a></li><li><a href="https://madao.science/useful_tools/Hexo-6-%E4%BD%BF%E7%94%A8Cloudflare%E5%85%8D%E8%B4%B9HTTPS.html" target="_blank" rel="noopener">Hexo-6-使用Cloudflare免费HTTPS | Madao No More</a></li><li><a href="https://steffan.cn/2017/03/22/use-cloudflare-to-implement-HTTPS-for-GithubPages-with-custom-domain-names/" target="_blank" rel="noopener">使用Cloudflare为自定义域名的GithubPages实现HTTPS化 | Steffan’s Blog</a></li></ul><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过了个年，好久没更博客了，忙着各种事。&lt;/p&gt;
&lt;p&gt;博客地址之前一直都是默认的 secsilm.github.io，一直想换个自己的域名，正巧看到 &lt;a href=&quot;https://namebeta.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://secsilm.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>我的网易云音乐 2017 年度听歌报告</title>
    <link href="https://secsilm.github.io/2018/01/03/neteasemusic-year-report-2017/"/>
    <id>https://secsilm.github.io/2018/01/03/neteasemusic-year-report-2017/</id>
    <published>2018-01-03T08:15:30.000Z</published>
    <updated>2018-01-03T11:21:09.205Z</updated>
    
    <content type="html"><![CDATA[<p>又是一年，最近网易云音乐推出了年度听歌报告，来看看我的报告吧，也是记录下来和以后做个比较 😄</p><h2 id="温暖同行"><a href="#温暖同行" class="headerlink" title="温暖同行"></a>温暖同行</h2><p><img src="https://i.imgur.com/DR9eHED.jpg" height="600"></p><p>报告首页，「网易云音乐陪你温暖同行」，我的 ID<code>secsilm</code>，确实音乐是我的陪伴，以前每次晚上睡不着的时候都会听歌，但是最近发现越听越睡不着……</p><h2 id="1550"><a href="#1550" class="headerlink" title="1550"></a>1550</h2><p><img src="https://i.imgur.com/8diWAYp.jpg" height="600"></p><p>2017 年里共在云村听了 1550 首歌，这里应该说的是听歌量，关于云村听歌量的官方解释：</p><blockquote><p>听歌量是指累计播放的<strong>歌曲数量</strong>而非播放次数，并且实际播放时间过短的歌曲将不纳入计算，每天最多计算300首。</p></blockquote><p>说实话这个听歌量对于我这个老用户来说应该不小了，毕竟我大部分时间还是听我自己收集的创建的歌单（温故而知新，总是想记住旋律与歌曲信息的对应关系），听新歌一般是在看电视的时候听到好听的才会去听，而且关注的歌手基本也只有我伦，而且现在我的听歌量曲线应该是处于 S 曲线的后面了。</p><p>「风格成谜」…… 确实我听得挺杂的，毕竟我伦的曲风就挺多变的 😏 深夜听我伦，已经成了一个后知后觉的习惯。</p><p>「热爱分享」，喜欢在我伦的下面刷评论，看着那些刚编的故事，或者一笑而过，或者深夜凝思，有时也会深有感触的写下「这首歌真吊」的惊世感叹，外加一个惊叹号！</p><h2 id="永远"><a href="#永远" class="headerlink" title="永远"></a>永远</h2><p><img src="https://i.imgur.com/3rPnFvY.jpg" height="600"></p><p>这个应该是今年新加的一个总结，去年我记得没有这个，我见很多人都是「永远」，甚至有人觉得云村的分词有 bug：<a href="https://www.v2ex.com/t/419399" target="_blank" rel="noopener">网易云音乐的分词是不是有 bug 报告都是离开？ - V2EX</a></p><p>分词，顾名思义就是把一句话切分成有意义的词，有很多现成工具可以做这个比如 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">fxsjy/jieba: 结巴中文分词</a>，而这次网易云音乐应该就是把你所听过的歌的歌词做了分词，然后做了个词频统计，词频最高的那个就是你的关键词了，而且从上图来看，把外文歌曲的翻译也算上了……</p><h2 id="美丽的神话"><a href="#美丽的神话" class="headerlink" title="美丽的神话"></a>美丽的神话</h2><p><img src="https://i.imgur.com/ZUVNs29.jpg" height="600"></p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=449454051&auto=0&height=66"></iframe></center><p>为什么呢？因为那天去看了<a href="https://movie.douban.com/subject/26182910/" target="_blank" rel="noopener">《功夫瑜伽》</a>，然后这首《美丽的神话》是电影主题曲和片尾曲，觉得挺好听的，从那之后就停了很多遍，欢快有活力。</p><p>另外，真心觉得豆瓣上这电影的分打低了。</p><h2 id="UH"><a href="#UH" class="headerlink" title="UH"></a>UH</h2><p><img src="https://i.imgur.com/tVTLvjZ.jpg" height="600"></p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=17852504&auto=0&height=66"></iframe></center><p>这是因为那几天在追<a href="https://movie.douban.com/subject/26772013/" target="_blank" rel="noopener">《杀不死》</a>，超迷，这歌是里面的插曲，第一遍听就喜欢上了，配合着画面，简直无敌。</p><p>至于 3 时 2 分，可能只是因为我睡着了吧……</p><h2 id="133"><a href="#133" class="headerlink" title="133"></a>133</h2><p><img src="https://i.imgur.com/Cl1uqgc.jpg" height="600"></p><p>133 天，一年中的 36.4% 都是在深夜 12 点以后还听着歌，借此立个 flag：<del>2018 年要早点睡</del>（<em>这一句发布的时候删掉</em>）。</p><h2 id="Oh-My-God"><a href="#Oh-My-God" class="headerlink" title="Oh My God"></a>Oh My God</h2><p><img src="https://i.imgur.com/0j57n8X.jpg" height="600"></p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=432509017&auto=0&height=66"></iframe></center><p><a href="https://baike.baidu.com/item/%E9%BB%84%E6%98%8E%E5%BF%97/10276475" target="_blank" rel="noopener">黄明志</a>（<a href="https://zh.wikipedia.org/wiki/%E9%BB%84%E6%98%8E%E5%BF%97#%E8%BE%B1%E9%A7%A1%E6%9E%97%E4%B8%B9%E4%BA%8B%E4%BB%B6" target="_blank" rel="noopener">黄明志 - 维基百科，自由的百科全书</a>）真是个人才，写了很多关于亚洲不同国家文化的歌曲，这首《Oh My God》就是他的一首歌，放荡不羁，此外再推荐他的 Tokyo Bon 東京盆踊り2020：</p><iframe width="640" height="360" src="https://www.youtube.com/embed/zhGnuWwpNxI?ecver=1" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe><h2 id="有何不可"><a href="#有何不可" class="headerlink" title="有何不可"></a>有何不可</h2><p><img src="https://i.imgur.com/tSpS73M.jpg" height="600"></p><p>《有何不可》这首歌在我这里真是复活了好多次，不过这歌确实挺不错的，旋律朗朗上口，有青春的感觉，在我这里最近一次复活应该是这个鬼畜了：<a href="https://www.bilibili.com/video/av14617344/?from=search&amp;seid=11975469206993589461" target="_blank" rel="noopener">【波澜哥】为你做一只扑火的肥鹅【有何不可】<em>人力VOCALOID</em>鬼畜<em>bilibili</em>哔哩哔哩</a></p><p>不过说我对这首歌最专一有点过分了啊。</p><h2 id="舍てるほどの爱でいいから"><a href="#舍てるほどの爱でいいから" class="headerlink" title="舍てるほどの爱でいいから"></a>舍てるほどの爱でいいから</h2><p><img src="https://i.imgur.com/4jMNMZe.jpg" height="600"></p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=22773678&auto=0&height=66"></iframe></center><p>这首歌是我最讶异的一首，说实话我看到的时候我真想不起来这是哪首歌了，一查原来是中岛美雪的歌，听了一下发现还是不知道啥时候听过 😕</p><p>为了方便复制，我特地把歌名放这：舍てるほどの爱でいいから。</p><h2 id="夢灯笼"><a href="#夢灯笼" class="headerlink" title="夢灯笼"></a>夢灯笼</h2><p><img src="https://i.imgur.com/8sIcKRi.jpg" height="600"></p><p>我觉得这是云村最没资格对我说的话了，还说我似乎把这首歌忘了，其实不然，其实是这首歌现在需要付费听了，超喜欢这首歌的，推荐一个打 call 的视频，背景音乐是这首，超级酷帅：<a href="https://www.bilibili.com/video/av6639433/" target="_blank" rel="noopener">【你的名字。】夢灯笼 / RADWIMPS<em>宅舞</em>舞蹈<em>bilibili</em>哔哩哔哩</a>。</p><p>再次吐槽一下云村的版权问题。</p><h2 id="五五开"><a href="#五五开" class="headerlink" title="五五开"></a>五五开</h2><p><img src="https://i.imgur.com/iz4JFjY.jpg" height="600"></p><p>最开始来云村的一个重要原因是云村的日推真的不错，但也只是最开始而已，越到后面你越会发现，这日推有严重的过度推荐的嫌疑，你听了一首某种类型的歌，隔天就给你狂推，简直智障！现在我使用日推的频率直线下降。</p><h2 id="周杰伦"><a href="#周杰伦" class="headerlink" title="周杰伦"></a>周杰伦</h2><p><img src="https://i.imgur.com/yby104Z.jpg" height="600"></p><p>这个没毛病，<strong>无与伦比，为杰沉伦</strong>。</p><h2 id="862"><a href="#862" class="headerlink" title="862"></a>862</h2><p><img src="https://i.imgur.com/1yvXQPl.jpg" height="600"></p><p>862 小时，大约 36 天，相当于连续一个月 24 小时不间断都在听歌，OMG 😲</p><h2 id="十大"><a href="#十大" class="headerlink" title="十大"></a>十大</h2><p><img src="https://i.imgur.com/ZvXFWen.jpg" height="600"></p><p>总的来说还可以吧，只是希望新的一年云村能够拿到更多版权！</p><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;又是一年，最近网易云音乐推出了年度听歌报告，来看看我的报告吧，也是记录下来和以后做个比较 😄&lt;/p&gt;
&lt;h2 id=&quot;温暖同行&quot;&gt;&lt;a href=&quot;#温暖同行&quot; class=&quot;headerlink&quot; title=&quot;温暖同行&quot;&gt;&lt;/a&gt;温暖同行&lt;/h2&gt;&lt;p&gt;&lt;img sr
      
    
    </summary>
    
    
      <category term="Living" scheme="https://secsilm.github.io/tags/Living/"/>
    
  </entry>
  
  <entry>
    <title>Google sitemap 不允许的网址的解决办法</title>
    <link href="https://secsilm.github.io/2017/12/30/google-sitemap/"/>
    <id>https://secsilm.github.io/2017/12/30/google-sitemap/</id>
    <published>2017-12-30T03:20:47.000Z</published>
    <updated>2018-03-15T07:57:20.481Z</updated>
    
    <content type="html"><![CDATA[<p><link href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css" rel="stylesheet" type="text/css"></p><p><link href="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css" rel="stylesheet" type="text/css"></p><link href="https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css" rel="stylesheet" type="text/css"><script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js"></script><script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js"></script><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>这段时间搭建好自己的<a href="https://secsilm.github.io/">博客</a>后，想要让别人可以在 Google 中找到我的文章，于是就得添加 sitemap，怎么生成 sitemap 的相关文章有很多，我在这里就不赘述了。我在<code>npm install hexo-generator-sitemap --save</code>装了插件后，确实生成了<code>sitemap.xml</code>文件，但是提交到 <a href="https://www.google.com/webmasters/tools/" target="_blank" rel="noopener">Google Search Console</a> 的时候就出错了：不允许的网址（忘了截图）。</p><blockquote><p>Note：本文方法只是许多解决方法中的一个，造成这个问题的原因也不止一个（见<a href="https://support.google.com/webmasters/answer/183669?hl=zh-Hans&amp;visit_id=1-636501984739210099-2033365588&amp;rd=1#errors" target="_blank" rel="noopener">完整错误列表</a>），因此本文方法可能并不能解决你的问题，仅供参考。</p></blockquote><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>网上搜了好多文章，千篇一律，一个偶然的机会我仔细看了下我生成的<code>sitemap.xml</code>文件，结果发现：</p><p><img src="https://i.imgur.com/nOZWc4R.png" alt="错误的 sitemap"></p><p>可以看到地址都是错的，都是<code>yoursite.com</code>开头，这很明显不是我的网址啊 😲，肯定是我犯了个 stupid 错误，哪里的默认配置没有改。</p><p>后来在检查我的<code>_config.yml</code>文件的时候发现：</p><p><img src="https://i.imgur.com/QJUu4zq.png" alt="config.yml"></p><p><code>url</code>竟然写的是默认值，没有改！</p><p>那么把<code>url</code>改成你的地址就可以了，例如我的</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="attr">https://secsilm.github.io/</span></span><br></pre></td></tr></table></figure><p>然后<code>hexo g -d</code>更新网站，再在 Google Search Console 重新提交一次 sitemap 就可以了。</p><p><img src="https://i.imgur.com/O4OhpQw.png" alt="success"></p><p>Problem solved！</p><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;link href=&quot;https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;link href=&quot;https://cdn.
      
    
    </summary>
    
    
      <category term="hexo" scheme="https://secsilm.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>理解 LSTM 网络</title>
    <link href="https://secsilm.github.io/2017/12/29/understanding-lstms/"/>
    <id>https://secsilm.github.io/2017/12/29/understanding-lstms/</id>
    <published>2017-12-29T10:58:00.000Z</published>
    <updated>2017-12-29T12:37:51.845Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>译者注：</p><ul><li>本文原文为 Christopher Olah 于 2015 年发表在自己<a href="http://colah.github.io/" target="_blank" rel="noopener">博客</a>上的经典文章：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks — colah’s blog</a>。</li><li>没有翻译原文中的 Acknowledgments 部分，此部分为致谢，私以为无关。</li><li>文中括号或者引用块中的 <em>斜体字</em> 为对应的英文原文或者我自己注释的话（会标明 <em>译者注</em>），否则为原文中本来就有的话。</li><li>本人水平有限，如有错误欢迎指出。</li></ul></blockquote><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>人们不会每一秒都从头开始思考。当你阅读这篇文章的时候，你根据前面的单词来理解后面的单词。你不会扔掉他们然后重新开始思考。你的思考具有持续性。</p><p>传统的神经网络做不到这一点，而且这似乎是一个主要缺点。例如，想象一下你想要对一个电影中每一帧所发生的事件类型进行分类，而传统神经网络却不能够利用先前的事情来推理后来的事情。</p><p>Recurrent Neural Networks（<em>译者注：以下简称 RNN</em>）解决了这个问题，他们在网络内部有循环，可以让信息具有持续性。</p><p><img src="https://i.imgur.com/A8appFv.png" height="180" alt="Recurrent Neural Networks have loops." )=""></p><center><font color="gray">RNN 中有循环</font></center><p>上图是神经网络的一部分，其中 $A$ 观察输入 $x_t$ 并输出一个值 $h_t$，循环使得信息可以从网络中的一步传递到下一步。</p><p>这些循环使得 RNN 看起来有些神秘。然而如果你再进一步想想，就会发现他们与普通的神经网络并不是完全不同。一个 RNN 可以被想成是对同一个网络的多次复制，每次都把信息传递给下一个。考虑一下如果我们把循环展开（<em>unroll</em>）会发生什么：</p><p><img src="https://i.imgur.com/0Wik9NF.png" alt="rnn-unrolled"></p><center><font color="gray">一个展开的 RNN</font></center><p>这种链状性质表明 RNN 与序列（<em>sequences</em>）和列表（<em>lists</em>）密切相关，在处理这种数据时他们是很自然的神经网络架构。</p><p>而且他们也确实在被使用！过去几年中，RNN 被应用于一系列的任务并取得了令人难以置信的成功：语音识别，语言模型，翻译，看图说话（<em>image captioning</em>）等等。你可以阅读 Andrej Karpathy 的精彩博文 —— <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 来了解通过 RNN 可以取得的惊人成绩。他们真的很棒。</p><p>这些成功的基础是使用 LSTMs，一种非常特别地 RNN，在许多任务中要比标准版本好。几乎所有基于 RNN 的令人激动的成绩都是通过他们获得的。这篇文章讨论的就是这些 LSTMs。</p><h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>RNNs 其中一个有吸引力的地方是能够将以前的信息和现在的任务联系起来，例如使用视频中的前几帧信息可能会对当前帧的理解有帮助。如果 RNNs 能够做到这些，那么他们就是非常有用的。但是他们能吗？这不一定。</p><p>有时，我们可能只需要最近的信息来完成当前任务。例如，考虑一个试图基于前面的词来预测下一个词的语言模型，如果我们试图预测 “the clouds are in the <em>sky</em>” 这句话中的最后一个词，那么我们就不需要更多的信息，很明显下一个词就是 sky。在这种情况下，相关信息和需要的地方（<em>the place that it’s needed</em>）之间的差距很小，那么这时候 RNNs 就可以学习到使用过去的信息。（<em>译者注：也就是短期依赖</em>）</p><p><img src="https://i.imgur.com/HAvvUQV.png" height="180)"></p><p>但是也有其他情况是我们需要更多信息的。考虑我们需要预测 “I grew up in France… I speak fluent <em>French</em>” 这句话中的最后一个词。最近的信息表明这个词应该是一个语言的名字，但是如果我们想要知道哪个语言，那么我们需要结合更前面的 France 这个背景。这时相关信息和需要的点（<em>the point where it is needed</em>）之间的差距就会变得非常大。</p><p>然而不幸的是，随着这个差距的增大，RNNs 越来越难以学习使用以前的信息。</p><p><img src="https://i.imgur.com/Whfo6UB.png" height="180)"></p><p>理论上来说，RNNs 完全可以处理这种“长期依赖”（<em>long-term dependencies</em>）。一个人可以很仔细的选择参数来解决这种形式的小问题（<em>toy problems</em>）。不过实际上，RNNs 似乎并不能学习到这种长期依赖。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter (1991) [German]</a> 和 <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a> 曾经深入探讨了这个问题，发现了一些相当根本的原因。</p><p>幸运的是，LSTMs 并没有这个问题！</p><h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>长短期记忆网络 —— 通常简称为“LSTMs” —— 是一种特别的 RNN，能够学习到长期依赖。LSTMs 由 <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> 提出，其后有很多人都对其进行了改善和推广<sup><a href="#fn_1" id="reffn_1">1</a></sup>。LSTMs 在许多任务上效果都非常好，现在也被广泛使用。</p><p>LSTMs 就是被设计用来避免长期依赖问题的。记住长时间的信息实际上是他们的默认行为，而不需要可以去这样做！</p><p>所有的 RNN 都是在将一个神经网络的模块重复好多次并链式连接起来。在标准的 RNNs 中，这个被重复的模块是一个非常简单的结构，例如一个单层的 tanh 层。</p><p><img src="https://i.imgur.com/KfFor4n.png" height="180)"></p><center><font color="gray">标准 RNN 中的重复模块都含有一层</font></center><p>LSTMs 也有这样一个链式结构，但是那个重复模块的结构是不一样的。与仅有一个单层神经网络不同的是，LSTMs 有 4 层，以一种非常特殊的方式连接起来。</p><p><img src="https://i.imgur.com/UydD8qN.png" height="180)"></p><center><font color="gray">LSTMs 中的重复模块含有 4 层</font></center><p>不要担心这里面的细节。稍后我们将会逐步深入这个 LSTMs 图。现在，我们先来熟悉下我们将要使用的符号：</p><p><img src="https://i.imgur.com/CbUDJll.png" alt=""></p><p>上图中，每一条线都表示一个向量从一个输出节点传到其他节点作为输入。粉色圆圈表示的是 pointwise 操作，例如向量加法，而黄色举行表示的是可学习的神经网络层。线合在一起表示连接（<em>concatenation</em>），线分叉表示其内容被复制成多份并且这些复制品流向不同的方向。</p><h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p>LSTMs 的核心是单元状态（<em>cell state</em>），就是顶部那条水平贯穿整个图的线。</p><p>单元状态就像是一个传送带，直接穿过整个链式结构，与其他部分仅有一些次要的线性交互，可以非常容易的传送信息而保持其不变。</p><p><img src="https://i.imgur.com/n7yh66F.png" height="180)"></p><p>LSTMs 可以给单元状态移除或者增加信息，由一个称为门（<em>gate</em>）的结构来控制。</p><p>门是一种让信息选择性通过的方式，由一个 sigmoid 层和一个 pointwise 乘法操作组成。</p><p><img src="https://i.imgur.com/1yDnZiD.png" height="180)"></p><p>sigmoid 层输出一个介于 0 和 1 之间的数字，表示每一个组件有多少信息可以穿过。0 意味着不让任何信息穿过，1 则意味着让所有信息穿过。</p><p>一个 LSTM 有 3 个这样的门，来保护和空值单元状态。</p><h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>LSTM 的第一步就是决定我们要从单元状态中扔掉什么信息，这由一个叫失忆门（<em>forget gate layer</em>）的 sigmoid 层来控制。失忆门的输入为 $h<em>{t-1}$ 和 $x_t$，然后为单元状态 $C</em>{t-1}$ 中的每个数字输出一个 0 和 1 之间的数，1 表示完全保留信息，而 0 表示完全丢去信息。</p><p>让我们回到语言模型，我们想要基于所有前面的词来预测下一个词。在这样一个问题中，单元状态可能包括了当前主体（<em>subject</em>）的性别，因此可以使用正确的代词。当我们看到一个新的主体时，我们想要忘记旧主体的性别。</p><p><img src="https://i.imgur.com/e8fiEJg.png" alt=""></p><p>下一步就是我们要决定要在单元状态中存入什么信息。这包括两部分。首先，一个叫做输入门（<em>input gate layer</em>）的 sigmoid 层决定我们要更新哪些值。然后一个 tanh 层创建一个新的候选向量 $\tilde{C}_t$，这个值会加到单元状态中。下一步我们将会利用这两个向量来更新单元状态。</p><p>在语言模型的例子中，我们想要用新主体的性别替换掉旧主体的性别，并加到单元状态中。</p><p><img src="https://i.imgur.com/8JE0JQW.png" alt="LSTM3-focus-i"></p><p>现在可以把旧的单元状态 $\tilde{C}_{t-1}$ 更新为新的单元状态 $\tilde{C}_t$ 了，前面的步骤已经决定了要做什么，现在我们只需要真正的去做就行了。</p><p>我们要乘以 $f_t$，也就是之前在失忆门我们决定的要忘记的东西。然后我们加上 $i_t*\tilde{C}_t$，这是新的候选值，乘上我们决定的要为每个单元状态值更新多少。</p><p>在语言模型的例子中，这里实际做的就是丢弃旧主体的性别信息，加进新主体的性别信息，就像我们之前要做的那样。</p><p><img src="https://i.imgur.com/1FXS5pB.png" alt=""></p><p>最后，我们要决定输出什么。这个输出将会是基于我们的单元状态的，但是是一个过滤版本（<em>filtered version</em>）。首先，我们运行一个 sigmoid 层来决定单元状态的哪些部分会被输出。然后，我们把单元状态输入给一个 tanh 层（把值映射到 $[-1,1]$ 区间内），并乘上 sigmoid 层的输出，然后这就是我们的输出。</p><p>在语言模型的例子中，由于仅仅有一个主体，以防接下来会发生什么事，LSTM 可能会输出与一个动词相关的信息。例如，可能会输出表示这个主体是单数还是复数的信息，以便我们如果知道接下来要发生什么，我们应该使用动词的什么形式。</p><p><img src="https://i.imgur.com/lLxeyBy.png" alt="LSTM3-focus-o"></p><h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>我目前描述的是非常普通的 LSTM，但不是所有的 LSTMs 都和上面的一样。事实上，几乎所有与 LSTMs 相关的论文都会使用一个稍微不同的版本。这些区别是很小的，但是其中一些区别值得我们注意。</p><p>其中一个比较流行的 LSTM 变体由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 引入，增加了一个「猫眼连接」（<em>peephole connections</em>），这意味着我们让门可以看到单元状态。</p><p><img src="https://i.imgur.com/APS3oDp.png" alt="LSTM3-var-peepholes"></p><p>上图给所有的门都增加了「猫眼」，但是一些论文只会加一些。</p><p>另一个变体是使用耦合的（<em>coupled</em>）失忆门和输入门。与独立的决定我们要丢弃和增加哪些信息不同的是，我们一起做这两个决定。只有当我们输入一些新信息的时候才会丢弃一些信息，只有当我们丢弃一些旧信息的时候才会输入新信息。</p><p><img src="https://i.imgur.com/gdm03LC.png" alt="LSTM3-var-tied"></p><p>稍微好点的（<em>dramatic</em>）LSTM 变体是 Gated Recurrent Unit，简称 GRU，由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a> 提出。GRU 将失忆门和输入门组合成一个更新门（<em>update gate</em>）。同时也合并了单元状态和隐藏状态，也做了一些其他改变。最终的模型比标准的 LSTM 模型更简单，而且越来越受欢迎。</p><p><img src="https://i.imgur.com/UEiY58u.png" alt="LSTM3-var-GRU"></p><p>这只是一些最显著的 LSTM 变体，还有很多其他的，例如 <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao, et al. (2015)</a> 提出的 Depth Gated RNNs。也有一些方法用完全不同的方式来处理长期依赖的问题，比如 <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik, et al. (2014)</a> 提出的 Clockwork RNNs。</p><p>哪个变体是最好的？这些差别影响大吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> 对流行的变体做了一个很好的比较，发现他们都是一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a> 测试了超过 10000 种 RNN 架构，发现其中一些在某些具体任务上的表现优于 LSTMs。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>前面我提到人们使用 RNNs 取得了显著成果，基本上都是用的 LSTMs。他们在大多数任务上都表现的很好。</p><p>写下一堆有关 LSTMs 的方程，这让 LSTMs 看起来很吓人。希望这篇文章一步一步的走进 LSTMs 可以让你更好的理解他们。</p><p>在可以用 RNNs 完成的任务上使用 LSTMs 是前进的一大步。很自然我们会问：还有另外一个一大步吗？研究者中的一个普遍观点是“是的！有下一步而且这个下一步是注意力机制（<em>attention</em>）”。这个想法是让 RNN 的每一步都从更多的信息中挑选信息。例如，如果你用一个 RNN 来为一个图片加一些注释，每个输出的词都可能对应的是图片中的一部分。<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, et al. (2015)</a> 就是这样做的，如果你想要继续探索注意力的话，那么这是一个很好地起点。使用注意力已经有了一些很好地结果，而且似乎还有更多的结果出来。注意力机制并不是 RNN 研究中唯一令人兴奋的部分，例如 <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, et al. (2015)</a> 提出的 Grid LSTMs 似乎也非常有前途。在生成模型中使用 RNNs 看起来也非常有趣，例如 <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, et al. (2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, et al. (2015)</a> 和 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a>。过去几年对于 RNNs 来说是一个激动人心的时刻，接下来几年只会更是如此！</p><blockquote id="fn_1"><sup>1</sup>. 除了原作者，许多人都对现在的 LSTM 做出了贡献。 一个不完整名单：Felix Gers，Fred Cummins，Santiago Fernandez，Justin Bayer，Daan Wierstra，Julian Togelius，Faustino Gomez，Matteo Gagliolo，和 <a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" target="_blank" rel="noopener">Alex Graves</a>。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;译者注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文原文为 Christopher Olah 于 2015 年发表在自己&lt;a href=&quot;http://colah.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客&lt;/a
      
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
      <category term="Translation" scheme="https://secsilm.github.io/tags/Translation/"/>
    
  </entry>
  
  <entry>
    <title>批量导出 CSDN 博客为 Markdown 文件</title>
    <link href="https://secsilm.github.io/2017/12/25/export-csdn-blogs-to-md/"/>
    <id>https://secsilm.github.io/2017/12/25/export-csdn-blogs-to-md/</id>
    <published>2017-12-25T01:27:18.000Z</published>
    <updated>2017-12-25T04:02:17.276Z</updated>
    
    <content type="html"><![CDATA[<p>CSDN 真是💊，经常性的不能访问，而且如果不用广告插件的话页面简直不能看，前段时间又挂了，发了个帖子吐槽了一下：<a href="https://www.v2ex.com/t/416648#reply35" target="_blank" rel="noopener">CSDN 博客又挂了。。。 - V2EX</a>。本来也考虑过自己做个，看了好久，但是由于自己对网站相关的不太懂，看着教程又复杂，就没弄了。CSDN 这次挂真让我铁了心弄自己的网站，然后用了一个下午搭起来了，随后又陆陆续续改了改主题配置之类的，弄这个还是我最近第一次一两点才睡，后来还感冒了 😞</p><p>内容为王，站搭起来了，就需要内容来充实。但是我的文章都在 CSDN 上，手动一篇一篇导过来太慢，而且 hexo 的文章需要在头部添加 <a href="https://hexo.io/zh-cn/docs/front-matter.html" target="_blank" rel="noopener">Front-matter</a>，所以这样一来工作量就更大了，所以考虑用 Python 把文章都爬过来，然后统一添加 Front-matter。</p><p>完整程序源码在 <a href="https://github.com/secsilm/csdn2md" target="_blank" rel="noopener">secsilm/csdn2md: Export csdn blogs to markdown files.</a>。</p><blockquote><p>Note：你的文章必须是使用 markdown 写的，不然无法使用本文方法导出。</p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>对于单篇文章，CSDN 有导出为 markdown 的功能，如下图：</p><p><img src="https://i.imgur.com/6aLaZPS.gif" alt="导出单篇文章"></p><p>但是这样费时费力，所以我们需要找到当我们点击<code>编辑</code>按钮时，网页向服务器发送的请求地址，这样我们就可以直接获取到博客内容，也就可以重新写入到 markdown 文件了。</p><p>我们在文章页按<code>F12</code>调出开发者工具，切换到<code>Network</code>选项卡，然后点击<code>编辑</code>，可以看到如下几个请求：</p><p><img src="https://i.imgur.com/Sg4Bb5w.png" alt="三个请求"></p><p><code>webspeeds</code>无关，那么剩下两个，从名字可以很快看出来我们需要的是第二个：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://write.blog.csdn.net/mdeditor/getArticle?id=53418159&amp;username=u010099080</span><br></pre></td></tr></table></figure><p>可以看到需要两个参数：<code>id</code>和<code>username</code>，<code>id</code>就是文章 id，这个很好获取，可以在文章地址中找到。<code>username</code>则是你的用户名，也可以在你的博客地址中找到。</p><p>下面我们看下请求头：</p><p><img src="https://i.imgur.com/gAkWlzs.png" alt="请求头"></p><p>可以看到其实也没啥特别的，其中<code>Cookie</code>是一会儿我们需要的，你可以将此复制到一个 txt 文件（假设名字叫<code>cookies.txt</code>）中以备后用。由于 cookie 字符串是存在一个文件中的，所以需要一个函数来解析成<code>dict</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_and_parse_cookies</span><span class="params">(cookie_file)</span>:</span></span><br><span class="line">    <span class="string">"""读取并解析 cookies。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cookie_file: 含有 cookies 字符串的 txt 文件名</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        一个字典形式的 cookies</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> open(cookie_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        cookies_str = f.readline()</span><br><span class="line">    cookies_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> cookies_str.split(<span class="string">";"</span>):</span><br><span class="line">        k, v = item.split(<span class="string">"="</span>, maxsplit=<span class="number">1</span>)</span><br><span class="line">        cookies_dict[k.strip()] = v.strip()</span><br><span class="line">    <span class="keyword">return</span> cookies_dict</span><br></pre></td></tr></table></figure><p>那么现在我们就可以使用<code>requests</code>来发起请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">base_url = <span class="string">'http://write.blog.csdn.net/mdeditor/getArticle'</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'id'</span>: article_id,</span><br><span class="line">    <span class="string">'username'</span>: username</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 根据文章 id 获取文章数据</span></span><br><span class="line">r = requests.get(base_url, params=params, cookies=cookies)</span><br></pre></td></tr></table></figure><p>再从开发者工具看下请求返回值：</p><p><img src="https://i.imgur.com/VPMrb7Q.png" alt="Response"></p><p>基本上囊括了一篇文章的所有信息了，而我们只需要里面的<code>markdowncontent</code>、<code>title</code>和<code>create</code>。而且返回的数据是 json 格式，我们可以用<code>json.loads()</code>来解析成<code>dict</code>格式，方便我们写入文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = json.loads(r.text, strict=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>最后由于我是要导入到 hexo 站点中，所以需要加 Front-matter，所以在获取到文章内容后，需要先把定义好的 Front-matter 加到文章内容里，一起写入文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> hexo:</span><br><span class="line">hexo_str = <span class="string">'---\ntitle: &#123;title&#125;\ndate: &#123;date&#125;\ntags:\n---\n\n'</span>.format(</span><br><span class="line">    title=title, date=data[<span class="string">'data'</span>][<span class="string">'create'</span>])</span><br><span class="line"><span class="comment"># Windows 下文件名中的非法字符</span></span><br><span class="line">forbidden = [<span class="string">'\\'</span>, <span class="string">'/'</span>, <span class="string">':'</span>, <span class="string">'*'</span>, <span class="string">'?'</span>, <span class="string">'"'</span>, <span class="string">'&lt;'</span>, <span class="string">'&gt;'</span>, <span class="string">'|'</span>]</span><br><span class="line"><span class="comment"># 如果文章名含有非法字符，那么使用其 id 作为 md 文件名</span></span><br><span class="line"><span class="keyword">if</span> any([c <span class="keyword">in</span> repr(title) <span class="keyword">for</span> c <span class="keyword">in</span> forbidden]):</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(md_dir, article_id + <span class="string">'.md'</span>), <span class="string">'w'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(hexo_str + data[<span class="string">'data'</span>][<span class="string">'markdowncontent'</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(md_dir, title + <span class="string">'.md'</span>), <span class="string">'w'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(hexo_str + data[<span class="string">'data'</span>][<span class="string">'markdowncontent'</span>])</span><br></pre></td></tr></table></figure><p>至于如何通过文章列表获取文章 id，这个比较简单，我就不赘述了，大家可以直接看源码。</p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>程序使用<code>fire</code>来从命令行接收参数，如何使用<code>fire</code>请参考 <a href="https://secsilm.github.io/2017/04/22/python-fire/#%E6%9B%B4%E6%96%B0">Python 自动生成命令行工具 - fire 简介 · Lee’s Space Station</a>。我们可以使用如下命令来查看帮助：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python csdn2md.py -- --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Type:        <span class="keyword">function</span></span><br><span class="line">String form: &lt;<span class="keyword">function</span> to_md_files at 0x000001CACC8C5E18&gt;</span><br><span class="line">File:        d:\chromedownload\mdfiles\csdn2md.py</span><br><span class="line">Line:        47</span><br><span class="line">Docstring:   导出为 Markdown 文件。</span><br><span class="line"></span><br><span class="line">Args:</span><br><span class="line">    total_pages: 博客文章在摘要模式下的总页数</span><br><span class="line">    filename: 含有 cookies 字符串的 txt 文件名</span><br><span class="line">    start: 从 start 页开始导出 (default: &#123;1&#125;)</span><br><span class="line">    stop: 到 stop 页停止 (default: &#123;None&#125;)</span><br><span class="line">    hexo: 是否添加 hexo 文章头部字符串（default: &#123;True&#125;）</span><br><span class="line">    md_dir: md 文件导出目录，默认为当前目录（default: .）</span><br><span class="line"></span><br><span class="line">Usage:       csdn2md.py USERNAME TOTAL_PAGES COOKIE_FILE [START] [STOP] [HEXO] [MD_DIR]</span><br><span class="line">             csdn2md.py --username USERNAME --total-pages TOTAL_PAGES --cookie-file COOKIE_FILE [--start START] [--stop STOP] [--hexo HEXO] [--md-dir MD_DIR]</span><br></pre></td></tr></table></figure><p>从帮助可以看出，你需要提供用户名<code>username</code>、博客总页数<code>total-pages</code>和 cookie 文件名<code>cookie-file</code>，可选参数为<code>start</code>、<code>stop</code>（默认为<code>None</code>，即一直到博客文章最后一页）、<code>hexo</code>和<code>md-dir</code>。</p><p>例如，你需要导出博客的第一页文章到当前目录下的<code>mdfiles</code>目录，你的博客总页数为<code>3</code>，那么只需要执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python csdn2md.py --username u010099080 --total-pages 3 --cookie-file cookies.txt --stop 1 --md-dir mdfiles</span><br></pre></td></tr></table></figure><p>控制台输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">2017-12-25 11:54:08 INFO: Page 1</span><br><span class="line">2017-12-25 11:54:08 INFO: Exporting 【TensorFlow】理解 Estimators 和 Datasets ...</span><br><span class="line">2017-12-25 11:54:08 INFO: Exporting Windows 10 资源管理器黑色风格 ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting 梯度下降优化算法概述 ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting 使用集成学习提升机器学习算法性能 ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting 【TensorFlow | TensorBoard】理解 TensorBoard ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting 【Python】Numpy 中的 shuffle VS permutation ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting 【TensorFlow】DNNRegressor 的简单使用 ...</span><br><span class="line">2017-12-25 11:54:09 INFO: Exporting XGBoost 在 Windows 10 和 Ubuntu 上的安装 ...</span><br><span class="line">2017-12-25 11:54:10 INFO: Exporting 【Python】自动生成命令行工具 - fire 简介 ...</span><br><span class="line">2017-12-25 11:54:10 INFO: Exporting 奇异值分解 SVD 的数学解释 ...</span><br><span class="line">2017-12-25 11:54:10 INFO: Exporting 【Python】统计字符串中英文、空格、数字、标点个数 ...</span><br><span class="line">2017-12-25 11:54:10 INFO: Exporting 【TensorFlow】TensorFlow 的卷积神经网络 CNN - TensorBoard版 ...</span><br><span class="line">2017-12-25 11:54:10 INFO: Exporting 使用 tree 命令格式化输出目录结构 ...</span><br><span class="line">2017-12-25 11:54:11 INFO: Exporting VSCode Markdown PDF 导出成PDF报 phantomjs binary does not exist 错误的解决办法 ...</span><br><span class="line">2017-12-25 11:54:11 INFO: Exporting 【Python】numpy 中的 copy 问题详解 ...</span><br><span class="line">2017-12-25 11:54:11 INFO: Done!</span><br></pre></td></tr></table></figure><p><code>mdfiles</code>文件夹：</p><p><img src="https://i.imgur.com/UK5pcCz.png" alt="mdfiles"></p><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>当然，导出的文件中 Front-matter 中的 <code>tags</code> 还是需要自己添加，因为我想自己再重新加标签而不是使用原来的标签，如果你想使用原来的标签，那么只需把请求返回 json 数据中的<code>categories</code>提取出来加进去即可。</p><p>Merry Christmas! 😄</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CSDN 真是💊，经常性的不能访问，而且如果不用广告插件的话页面简直不能看，前段时间又挂了，发了个帖子吐槽了一下：&lt;a href=&quot;https://www.v2ex.com/t/416648#reply35&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
      
    
    </summary>
    
    
      <category term="Python" scheme="https://secsilm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>理解 Estimators 和 Datasets</title>
    <link href="https://secsilm.github.io/2017/12/22/understanding-estimators-datasets/"/>
    <id>https://secsilm.github.io/2017/12/22/understanding-estimators-datasets/</id>
    <published>2017-12-22T10:30:15.000Z</published>
    <updated>2017-12-24T15:08:35.711Z</updated>
    
    <content type="html"><![CDATA[<p>Google 在 2017 年 9 月 12 号的博文 <a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" target="_blank" rel="noopener">Introduction to TensorFlow Datasets and Estimators</a> 中介绍了新引入的两个新特性 <a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank" rel="noopener">Datasets</a> 和 <a href="https://www.tensorflow.org/programmers_guide/estimators" target="_blank" rel="noopener">Estimators</a>：</p><ul><li><p>Datasets：创建一个输入管道（input pipelines）来为你的模型读取数据，在这个 pipelines 中你可以做一些数据预处理，尽量都使用 TensorFlow 自己的函数，即 <code>tf</code> 开头的函数（比如 <code>tf.reshape</code>），这样可以提高程序执行效率。</p></li><li><p>Estimators：这是模型的核心部分，而 Estimators 的核心部分则是一个 <code>model_fn</code> 函数（后面会细讲），你在这个函数中定义你的模型架构，输入是特征和标签，输出是一个定义好的 estimator。</p></li></ul><p><img src="https://i.imgur.com/CSw2x5g.png" alt="tensorflow architecture"><br><em>TensorFlow 架构，图自 <a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" target="_blank" rel="noopener">Google Developers Blog</a></em></p><p>实际上这两个特性并不是第一次引入，只不过之前是放在 <code>tf.contrib</code> 里，而这次是引入到了 TensorFlow 核心组件中，意味着可以在生产环境中使用。我 6 月份的时候也写过一篇<a href="http://blog.csdn.net/u010099080/article/details/72824899" target="_blank" rel="noopener">博文</a>简单说了下 <code>tf.contrib.learn.DNNRegressor</code> 的使用，实际上这就是 Estimators 内置的一个模型（estimator）。这两个都是高层 API，也就是说为了创建一个模型你不用再写一些很底层的代码（比如定义权重偏置项），可以像 scikit-learn 和 Keras 那样很轻松的几行代码创建一个模型，便于快速实现。</p><p>本篇博文就是试图将这两个高层 API 结合起来，使用 TensorFlow 的数据格式 TFRecords 来实现一个在 <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a> 数据集上的 CNN 模型。完整代码可在<a href="">我的 GitHub</a> 上找到。</p><blockquote><p>Note：本篇博文中的模型并不是结果最好的模型，仅仅是为了展示如何将 Estimators 和 Datasets 结合起来使用。</p></blockquote><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>你可以使用 <code>python cifar10-estimator-dataset.py --help</code> 来查看可选参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">       USAGE: cifar10-estimator-dataset.py [flags]</span><br><span class="line">flags:</span><br><span class="line"></span><br><span class="line">cifar10-estimator-dataset.py:</span><br><span class="line">  --batch_size: Batch size</span><br><span class="line">    (default: &apos;64&apos;)</span><br><span class="line">    (an integer)</span><br><span class="line">  --dropout_rate: Dropout rate</span><br><span class="line">    (default: &apos;0.5&apos;)</span><br><span class="line">    (a number)</span><br><span class="line">  --eval_dataset: Filename of evaluation dataset</span><br><span class="line">    (default: &apos;eval.tfrecords&apos;)</span><br><span class="line">  --learning_rate: Learning rate</span><br><span class="line">    (default: &apos;0.001&apos;)</span><br><span class="line">    (a number)</span><br><span class="line">  --model_dir: Filename of testing dataset</span><br><span class="line">    (default: &apos;models/cifar10_cnn_model&apos;)</span><br><span class="line">  --num_epochs: Number of training epochs</span><br><span class="line">    (default: &apos;10&apos;)</span><br><span class="line">    (an integer)</span><br><span class="line">  --test_dataset: Filename of testing dataset</span><br><span class="line">    (default: &apos;test.tfrecords&apos;)</span><br><span class="line">  --train_dataset: Filename of training dataset</span><br><span class="line">    (default: &apos;train.tfrecords&apos;)</span><br></pre></td></tr></table></figure><p>TFRecords 和 TensorBoard 文件（包括我做的所有 run）较大，没有放到 GitHub 上，你可以从百度盘上获取：</p><ul><li><a href="https://pan.baidu.com/s/1jImZOGY" target="_blank" rel="noopener">TFRecords</a>（133.4 MB），密码：<code>dp7u</code></li><li><a href="https://pan.baidu.com/s/1dFew7SH" target="_blank" rel="noopener">TensorBoard</a>（1.45 GB），密码：<code>6885</code></li></ul><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>为了让大家对模型架构先有个清晰地了解，我先把 TensorBoard （不熟悉 TensorBoard 的话可以参考<a href="http://blog.csdn.net/u010099080/article/details/77426577" target="_blank" rel="noopener">这里</a>）中显示的模型架构图贴出来（数据集我也就不介绍了，这是个很常用的数据集，如有不熟悉的可以参看<a href="http://blog.csdn.net/u010099080/article/details/53906810#%E6%95%B0%E6%8D%AE%E9%9B%86" target="_blank" rel="noopener">这里</a>）：</p><p><img src="https://i.imgur.com/D1Qz5it.png" alt="cnn model"><em>模型架构</em></p><p>可以看到两层卷积层，两层池化层，两层 BN 层，一层 dropout，三层全连接层（<code>DENSE</code>）。</p><h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><p>在给模型「喂」数据的时候，我们的流程大概是这样的：</p><ol><li><p>创建一个 <code>Dataset</code> 对象来表示我们的数据集，有多种方法可以创建一个 <code>Dataset</code> 对象，我说几个比较常用的：</p><ul><li><code>tf.data.Dataset.from_tensor_slices()</code>：这种方法适合于你的数据集是 numpy 数组类型的。</li><li><strong><code>tf.data.TFRecordDataset()</code></strong>：<strong>这是本文所使用的方法</strong>，适合于你的数据集是 TFRecords 类型的。</li><li><code>tf.data.TextLineDataset()</code>：适合于你的数据集是 txt 格式的。</li></ul></li><li><p>对数据集进行一些预处理：</p><ul><li><code>Dataset.map()</code>：和普通的 <code>map</code> 函数一样，对数据集进行一些变换，例如图像数据集的类型转换（uint8 -&gt; float32）以及 <code>reshape</code> 等。</li><li><code>Dataset.shuffle()</code>：打乱数据集</li><li><code>Dataset.batch()</code>：将数据集切分为特定大小的 batch</li><li><code>Dataset.repeat()</code>：将数据集重复多次。如果不使用这个方法，在第一次遍历到数据集的结尾的时候，会抛出一个 <code>tf.errors.OutOfRangeError</code> 异常，表示数据集已经遍历完毕。但是实际中我们可能需要对数据集迭代训练不止一次，这时候就要用 <code>repeat()</code> 来重复数据集多次。如果不加任何参数，那么表示重复数据集无穷多次。</li></ul></li><li><p>使用 <code>Iterator</code> 的 <code>get_next()</code> 方法来每次获取一个 batch 的数据（假如你是使用 mini-batch 训练的话）。目前 TensorFlow 提供四种 <code>Iterator</code>（详细见 <a href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" target="_blank" rel="noopener">Creating an iterator</a>）：</p><ul><li><strong>one-shot</strong>：<strong>这是本文程序所使用的方法</strong>，使用 <code>Dataset.make_one_shot_iterator()</code> 来创建，不需要初始化。官方有这么一句话：<em>Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator.</em> 不过呢，我也发现外国友人 Peter Roelants 写了个例子将下面的 initializable Iterator 和 Estimator 一起使用，见 <a href="https://gist.github.com/peterroelants/9956ec93a07ca4e9ba5bc415b014bcca" target="_blank" rel="noopener">Example using TensorFlow Estimator, Experiment &amp; Dataset on MNIST data</a>。</li><li>initializable：使用 <code>Dataset.make_initializable_iterator()</code> 创建，需要使用 <code>iterator.initializer</code> 初始化。该方式可以允许你自定义数据集，例如你的数据集是 <code>range(0, max_value)</code>，这里面 <code>max_value</code> 是一个 <code>Tensor</code>，在初始化的时候你需要赋值。</li><li>reinitializable：这是种比较复杂的方式，简单来说也就是使你可以从多个不同的 <code>Dataset</code> 对象获取数据，详细可见 <a href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" target="_blank" rel="noopener">Creating an iterator</a>。</li><li>feedable：同样比较复杂，当然更灵活，可以针对不同的 <code>Dataset</code> 对象和 <code>tf.Session.run</code> 使用不同的 <code>Iterator</code>，详细可见 <a href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" target="_blank" rel="noopener">Creating an iterator</a>。</li></ul></li></ol><p>在 Estimator 中，我们输入必须是一个函数，这个函数必须返回特征和标签（或者只有特征），所以我们需要把上面的内容写到一个函数中。因为训练输入和验证输入是不一样的，所以需要两个输入函数：<code>train_input_fn</code> 和 <code>eval_input_fn</code>。为了保持文章简洁，我下面只列出 <code>train_input_fn</code>，<code>eval_input_fn</code> 和其大同小异。</p><blockquote><p>此处我使用了 <code>tf.data.TFRecordDataset</code>，所以你需要将你的数据集写成 TFRecords 格式，比如 <code>train.tfrecords</code>。TFRecords 格式每行表示一个样本（record），关于如何将数据集写成 TFRecords 格式，我将在另一篇博文中说明。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    训练输入函数，返回一个 batch 的 features 和 labels</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_dataset = tf.data.TFRecordDataset(FLAGS.train_dataset)</span><br><span class="line">    train_dataset = train_dataset.map(parser)</span><br><span class="line">    <span class="comment"># num_epochs 为整个数据集的迭代次数</span></span><br><span class="line">    train_dataset = train_dataset.repeat(FLAGS.num_epochs)</span><br><span class="line">    train_dataset = train_dataset.batch(FLAGS.batch_size)</span><br><span class="line">    train_iterator = train_dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">    features, labels = train_iterator.get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure><p>而其中的 <code>map</code> 函数的参数 <code>parser</code> 也是一个函数，用于将图片和标签从 TFRecords 中解析出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parser</span><span class="params">(record)</span>:</span></span><br><span class="line">    keys_to_features = &#123;</span><br><span class="line">        <span class="string">'image_raw'</span>: tf.FixedLenFeature((), tf.string),</span><br><span class="line">        <span class="string">'label'</span>: tf.FixedLenFeature((), tf.int64)</span><br><span class="line">    &#125;</span><br><span class="line">    parsed = tf.parse_single_example(record, keys_to_features)</span><br><span class="line">    image = tf.decode_raw(parsed[<span class="string">'image_raw'</span>], tf.uint8)</span><br><span class="line">    image = tf.cast(image, tf.float32)</span><br><span class="line">    label = tf.cast(parsed[<span class="string">'label'</span>], tf.int32)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><p>到此，关于模型的 input pipeline 就差不多结束了。下面就是模型的核心部分了：定义一个模型函数 <code>model_fn</code>。</p><h2 id="定义模型函数"><a href="#定义模型函数" class="headerlink" title="定义模型函数"></a>定义模型函数</h2><p>上面是定义了 input pipeline，那么现在该来定义模型架构了。模型大致架构就是上面的模型架构图。该函数需要返回一个定义好的 <code>tf.estimator.EstimatorSpec</code> 对象，对于不同的 <code>mode</code>，所必须提供的参数是不一样的：</p><ul><li>训练模式，即 <code>mode == tf.estimator.ModeKeys.TRAIN</code>，必须提供的是 <code>loss</code> 和 <code>train_op</code>。</li><li>验证模式，即 <code>mode == tf.estimator.ModeKeys.EVAL</code>，必须提供的是 <code>loss</code>。</li><li>预测模式，即 <code>mode == tf.estimator.ModeKeys.PREDICT</code>，必须提供的是 <code>predicitions</code>。</li></ul><blockquote><p>为保持文章简洁，我省略了一些重复性代码。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cifar_model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line">    <span class="string">"""Model function for cifar10 model"""</span></span><br><span class="line">    <span class="comment"># 输入层</span></span><br><span class="line">    x = tf.reshape(features, [<span class="number">-1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="comment"># 第一层卷积层</span></span><br><span class="line">    x = tf.layers.conv2d(inputs=x, filters=<span class="number">64</span>, kernel_size=[</span><br><span class="line">                             <span class="number">3</span>, <span class="number">3</span>], padding=<span class="string">'same'</span>, activation=tf.nn.relu, name=<span class="string">'CONV1'</span>)</span><br><span class="line">    x = tf.layers.batch_normalization(</span><br><span class="line">        inputs=x, training=mode == tf.estimator.ModeKeys.TRAIN, name=<span class="string">'BN1'</span>)</span><br><span class="line">    <span class="comment"># 第一层池化层</span></span><br><span class="line">    x = tf.layers.max_pooling2d(inputs=x, pool_size=[</span><br><span class="line">                                    <span class="number">3</span>, <span class="number">3</span>], strides=<span class="number">2</span>, padding=<span class="string">'same'</span>, name=<span class="string">'POOL1'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 你可以添加更多的卷积层和池化层 ……</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">8</span> * <span class="number">8</span> * <span class="number">128</span>])</span><br><span class="line">    x = tf.layers.dense(inputs=x, units=<span class="number">512</span>, activation=tf.nn.relu, name=<span class="string">'DENSE1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 你可以添加更多的全连接层 ……</span></span><br><span class="line"></span><br><span class="line">    logits = tf.layers.dense(inputs=x, units=<span class="number">10</span>, name=<span class="string">'FINAL'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    predictions = &#123;</span><br><span class="line">        <span class="string">'classes'</span>: tf.argmax(input=logits, axis=<span class="number">1</span>, name=<span class="string">'classes'</span>),</span><br><span class="line">        <span class="string">'probabilities'</span>: tf.nn.softmax(logits, name=<span class="string">'softmax_tensor'</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失（对于 TRAIN 和 EVAL 模式）</span></span><br><span class="line">    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=<span class="number">10</span>)</span><br><span class="line">    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits, scope=<span class="string">'LOSS'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评估方法</span></span><br><span class="line">    accuracy, update_op = tf.metrics.accuracy(</span><br><span class="line">        labels=labels, predictions=predictions[<span class="string">'classes'</span>], name=<span class="string">'accuracy'</span>)</span><br><span class="line">    batch_acc = tf.reduce_mean(tf.cast(</span><br><span class="line">        tf.equal(tf.cast(labels, tf.int64), predictions[<span class="string">'classes'</span>]), tf.float32))</span><br><span class="line">    tf.summary.scalar(<span class="string">'batch_acc'</span>, batch_acc)</span><br><span class="line">    tf.summary.scalar(<span class="string">'streaming_acc'</span>, update_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练配置（对于 TRAIN 模式）</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">        optimizer = tf.train.RMSPropOptimizer(learning_rate=FLAGS.learning_rate)</span><br><span class="line">        <span class="keyword">with</span> tf.control_dependencies(update_ops):</span><br><span class="line">            train_op = optimizer.minimize(</span><br><span class="line">                loss=loss, global_step=tf.train.get_global_step())</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)</span><br><span class="line"></span><br><span class="line">    eval_metric_ops = &#123;</span><br><span class="line">        <span class="string">'accuracy'</span>: (accuracy, update_op)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br></pre></td></tr></table></figure><p>至此，input pipeline 和模型都已经定义好了，下一步就是实际的 run 了。</p><h2 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h2><p>首先我们需要创建一个 <code>tf.estimator.Estimator</code> 对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cifar10_classifier = tf.estimator.Estimator(</span><br><span class="line">        model_fn=cifar_model_fn, model_dir=FLAGS.model_dir)</span><br></pre></td></tr></table></figure><p>其中 <code>model_dir</code> 是用于存放模型文件和 TensorBoard 文件的目录。</p><p>然后开始训练和验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cifar10_classifier.train(input_fn=train_input_fn)</span><br><span class="line">eval_results = cifar10_classifier.evaluate(input_fn=eval_input_fn)</span><br></pre></td></tr></table></figure><p>程序结束后你便可以在你的 <code>model_dir</code> 里看到类似如下的文件结构：</p><p><img src="https://i.imgur.com/ZIO86mY.png" alt="model_dir"><br><em><code>model_dir</code> 中的文件结构</em></p><p>然后你可以使用 <code>tensorboard --logdir=/your/model/dir</code>（Linux 中你可能需要使用 <code>python -m tensorboard.main --logdir=/your/model/dir</code>）来在 TensorBoard 中查看训练信息，默认只有 <code>SCALARS</code> 和 <code>GRAPHS</code> 面板是有效的，你也可以自己使用 <code>tf.summary</code> 来手动添加 summary 信息。</p><p><img src="https://i.imgur.com/mjtQ8Da.png" alt="scalars"><br><em>SCALARS 面板</em></p><p><img src="https://i.imgur.com/p9aHnKU.png" alt="graphs"><br><em>GRAPHS 面板</em></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总的来说，使用 Datasets 和 Estimators 来训练模型大致就是这么几个步骤：</p><ol><li>定义输入函数，在函数中对你的数据集做一些必要的预处理，返回 features 和 labels。</li><li>定义模型函数，返回 <code>tf.estimator.EstimatorSpec</code> 对象。</li><li>使用模型函数创建 <code>tf.estimator.Estimator</code> 对象。</li><li>使用创建好的对象 train and evaluate。</li></ol><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="关于-num-epochs"><a href="#关于-num-epochs" class="headerlink" title="关于 num_epochs"></a>关于 <code>num_epochs</code></h3><p>如果你设置 <code>num_epochs</code> 为比如说 30，然而你在训练的时候看到类似如下的控制台输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INFO:tensorflow:global_step/sec: 0.476364</span><br><span class="line">INFO:tensorflow:loss = 0.137512, step = 14901 (209.924 sec)</span><br><span class="line">INFO:tensorflow:global_step/sec: 0.477139</span><br><span class="line">INFO:tensorflow:loss = 0.0203241, step = 15001 (209.583 sec)</span><br><span class="line">INFO:tensorflow:global_step/sec: 0.477511</span><br><span class="line">INFO:tensorflow:loss = 0.132834, step = 15101 (209.419 sec)</span><br></pre></td></tr></table></figure><p>你可以看到 <code>step</code> 已经上万了，这是因为这里的 <code>step</code> 指的是一个 batch 的训练迭代，而 <code>num_epochs</code> 设为 30 意味着你要把整个训练集遍历 30 次（也是我们通常的做法）。也就是说，假如你有 50000 个样本，batch 大小为 50，那么你的数据集将被切分为 1000 个 batch，也就是遍历一遍数据集需要 1000 step，所以说 <code>num_epochs</code> 为 30 时，你的程序需要到 <code>step=30000</code> 才会训练结束。所以切记 <code>num_epochs</code> 表示的是整个训练集的迭代次数。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" target="_blank" rel="noopener">Introduction to TensorFlow Datasets and Estimators</a></li><li><a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank" rel="noopener">Importing Data | TensorFlow</a></li><li><a href="https://www.tensorflow.org/extend/estimators" target="_blank" rel="noopener">Creating Estimators in tf.estimator | TensorFlow</a></li><li><a href="https://gist.github.com/peterroelants/9956ec93a07ca4e9ba5bc415b014bcca" target="_blank" rel="noopener">Example using TensorFlow Estimator, Experiment &amp; Dataset on MNIST data</a></li><li><a href="https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0" target="_blank" rel="noopener">Higher-Level APIs in TensorFlow – Onfido Tech – Medium</a></li></ol><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Google 在 2017 年 9 月 12 号的博文 &lt;a href=&quot;https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html&quot; target=&quot;_blank&quot; rel=
      
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://secsilm.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Windows 10 资源管理器黑色风格</title>
    <link href="https://secsilm.github.io/2017/10/26/windows10-dark-theme/"/>
    <id>https://secsilm.github.io/2017/10/26/windows10-dark-theme/</id>
    <published>2017-10-26T14:47:00.000Z</published>
    <updated>2017-12-24T12:53:19.643Z</updated>
    
    <content type="html"><![CDATA[<p>今天来水一篇，说说我前几天某天上午初步实现了我一直想弄的东西：Windows 10 资源管理器黑色风格，用了几天，整体上感觉还不错，当然也有点小瑕疵，我会在后面说。</p><blockquote><p>所有图可右键在新标签页打开查看大图。</p></blockquote><hr><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>这里我会列出对本文的更新。</p><ul><li>2017 年 11 月 2 日：增加问题 #5。</li></ul><hr><h2 id="先睹为快"><a href="#先睹为快" class="headerlink" title="先睹为快"></a>先睹为快</h2><p>实现后的界面是这样的：</p><p><img src="https://i.imgur.com/PmKOXrF.png" alt="主界面"><br><em>主界面</em></p><p><img src="https://i.imgur.com/9X8085S.png" alt=""></p><font color="gray">*文件列表*</font><p><img src="https://i.imgur.com/tPhk6R6.png" alt=""></p><font color="gray">*选中状态*</font><p><img src="https://i.imgur.com/PGOZFUM.png" alt=""></p><font color="gray">*任务管理器*</font><p><img src="https://i.imgur.com/caTobn6.png" alt=""></p><font color="gray">*复制*</font><p><img src="https://i.imgur.com/AmonnUg.png" alt=""></p><font color="gray">*记事本*</font><p><img src="https://i.imgur.com/Zr0EKDg.png" alt=""></p><font color="gray">*Word*</font><p><strong>WARNING! 操作风险很大，存在不稳定因素，请三思后行，出了什么幺蛾子不要赖我没有事先提醒哦！</strong></p><p>好了，看到这里你还想弄的话，那就接着往下看吧。</p><hr><h2 id="安装开始前"><a href="#安装开始前" class="headerlink" title="安装开始前"></a>安装开始前</h2><p>在安装开始前，有几件需要你确定的事情：</p><ul><li><p>你的 Windows 版本。目前支持的 Windows 版本为 1607 (Build 14393) - Anniversary 周年更新和 1703 (Build 15063) - Creators 创意者更新。你可以在 <code>设置</code> —&gt; <code>系统</code> —&gt; <code>关于</code> 中查看你的系统版本。<br><img src="https://i.imgur.com/sFihaa1.png" alt=""></p><font color="gray">*我的版本号*</font></li><li><p>设置系统还原点</p></li><li>如果你安装过其他补丁（UxStyle，UltraUXThemePatcher），那么请卸载掉</li></ul><hr><h2 id="三个工具"><a href="#三个工具" class="headerlink" title="三个工具"></a>三个工具</h2><p>首先我们需要下载将会用到的工具：</p><ul><li><a href="http://www.msfn.org/board/topic/170375-oldnewexplorer-118/" target="_blank" rel="noopener">OldNewExplorer.rar</a></li><li><a href="https://scope10.deviantart.com/art/Penumbra-10-Windows-10-visual-style-568740374" target="_blank" rel="noopener">penumbra_10___windows_10_visual_style_by_scope10-d9em2vq.zip</a></li><li><a href="https://www.syssel.net/hoefs/software_uxtheme.php?lang=en" target="_blank" rel="noopener">UltraUXThemePatcher_3.3.1.exe</a></li></ul><p><img src="https://i.imgur.com/3NkWQ8q.png" alt=""></p><font color="gray">*三个工具*</font><p>下载完成解压后目录结构是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">|   UltraUXThemePatcher_3.3.1.exe</span><br><span class="line">|</span><br><span class="line">+---OldNewExplorer</span><br><span class="line">|       OldNewExplorer32.dll</span><br><span class="line">|       OldNewExplorer64.dll</span><br><span class="line">|       OldNewExplorerCfg.exe</span><br><span class="line">|       readme.txt</span><br><span class="line">|</span><br><span class="line">\---penumbra_10___windows_10_visual_style_by_scope10-d9em2vq</span><br><span class="line">    |   Preview Image.png</span><br><span class="line">    |</span><br><span class="line">    +---Automated Theme Reset Script</span><br><span class="line">    |       Instructions.txt</span><br><span class="line">    |       Reapply visual style (Penumbra 10).xml</span><br><span class="line">    |       Reapply visual style script.ba_</span><br><span class="line">    |       ThemeSwitcher.ex_</span><br><span class="line">    |</span><br><span class="line">    +---Font</span><br><span class="line">    |       LICENSE.txt</span><br><span class="line">    |       OpenSans-Bold.ttf</span><br><span class="line">    |       OpenSans-BoldItalic.ttf</span><br><span class="line">    |       OpenSans-ExtraBold.ttf</span><br><span class="line">    |       OpenSans-ExtraBoldItalic.ttf</span><br><span class="line">    |       OpenSans-Italic.ttf</span><br><span class="line">    |       OpenSans-Light.ttf</span><br><span class="line">    |       OpenSans-LightItalic.ttf</span><br><span class="line">    |       OpenSans-Regular.ttf</span><br><span class="line">    |       OpenSans-Semibold.ttf</span><br><span class="line">    |       OpenSans-SemiboldItalic.ttf</span><br><span class="line">    |</span><br><span class="line">    \---Visual style</span><br><span class="line">        |   Installation instructions + info.txt</span><br><span class="line">        |</span><br><span class="line">        +---Build 14393 - Version 1607 (Anniversary Update)</span><br><span class="line">        |   |   Penumbra 10.theme</span><br><span class="line">        |   |   Penumbra 10s.theme</span><br><span class="line">        |   |   Penumbra 10w.theme</span><br><span class="line">        |   |   Penumbra 10ws.theme</span><br><span class="line">        |   |</span><br><span class="line">        |   \---Penumbra 10</span><br><span class="line">        |       |   Penumbra 10.msstyles</span><br><span class="line">        |       |   Penumbra 10s.msstyles</span><br><span class="line">        |       |   Penumbra 10w.msstyles</span><br><span class="line">        |       |   Penumbra 10ws.msstyles</span><br><span class="line">        |       |</span><br><span class="line">        |       +---Shell</span><br><span class="line">        |       |   \---NormalColor</span><br><span class="line">        |       |       |   1.txt</span><br><span class="line">        |       |       |   shellstyle.dll</span><br><span class="line">        |       |       |   shellstyle_original.dll</span><br><span class="line">        |       |       |</span><br><span class="line">        |       |       \---en-US</span><br><span class="line">        |       |               shellstyle.dll.mui</span><br><span class="line">        |       |</span><br><span class="line">        |       \---Wallpaper</span><br><span class="line">        |               1.png</span><br><span class="line">        |</span><br><span class="line">        \---Build 15063 - Version 1703 (Creators Update)</span><br><span class="line">            |   Penumbra 10.theme</span><br><span class="line">            |   Penumbra 10s.theme</span><br><span class="line">            |   Penumbra 10w.theme</span><br><span class="line">            |   Penumbra 10ws.theme</span><br><span class="line">            |</span><br><span class="line">            \---Penumbra 10</span><br><span class="line">                |   Penumbra 10.msstyles</span><br><span class="line">                |   Penumbra 10s.msstyles</span><br><span class="line">                |   Penumbra 10w.msstyles</span><br><span class="line">                |   Penumbra 10ws.msstyles</span><br><span class="line">                |</span><br><span class="line">                +---Shell</span><br><span class="line">                |   \---NormalColor</span><br><span class="line">                |       |   shellstyle.dll</span><br><span class="line">                |       |</span><br><span class="line">                |       \---en-US</span><br><span class="line">                |               shellstyle.dll.mui</span><br><span class="line">                |</span><br><span class="line">                \---Wallpaper</span><br><span class="line">                        1.png</span><br></pre></td></tr></table></figure><hr><h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><p>最好把这三个工具解压后放到同一个文件夹下，放在 C 盘，然后我们就开始正式的安装吧 :-)</p><h3 id="1-UltraUXThemePatcher"><a href="#1-UltraUXThemePatcher" class="headerlink" title="#1 UltraUXThemePatcher"></a>#1 UltraUXThemePatcher</h3><p>首先是运行 <code>UltraUXThemePatcher_3.3.1.exe</code>，按照提示一步一步的安装就行，在进行到某一步的时候会显示你的系统信息，看一看是否正确，正确的话就下一步往下走，完成后需要重启电脑。</p><h3 id="2-可选-OldNewExplorer"><a href="#2-可选-OldNewExplorer" class="headerlink" title="#2[可选] OldNewExplorer"></a>#2[可选] OldNewExplorer</h3><blockquote><p>你可以自己尝试到底勾选哪些，以找到你最喜欢的风格</p></blockquote><p>这一步其实是禁掉功能区。运行 <code>OldNewExplorer</code> 中的 <code>OldNewExplorerCfg.exe</code>，按照下面的图示选择：然后点击 <code>Install</code> 。</p><p><img src="https://i.imgur.com/oWRdFJF.png" alt=""></p><font color="gray">*OldNewExplorer*</font><p><strong>但是你如果取消这些更改的话</strong>，那就直接点击 <code>Uninstall</code>，然后<strong>不勾选</strong>所有项，点击 <code>Install</code> 。</p><h3 id="3-复制文件"><a href="#3-复制文件" class="headerlink" title="#3 复制文件"></a>#3 复制文件</h3><ul><li>如果你的 Windows 版本是 <strong>1607</strong>，那么复制 <code>penumbra_10___windows_10_visual_style_by_scope10-d9em2vq\Visual style\Build 14393 - Version 1607 (Anniversary Update)</code> 到 <code>C:\Windows\Resources\Themes</code></li><li>如果你的 Windows 版本是 <strong>1703</strong>，那么复制 <code>penumbra_10___windows_10_visual_style_by_scope10-d9em2vq\Visual style\Build 15063 - Version 1703 (Creators Update)</code> 到 <code>C:\Windows\Resources\Themes</code></li></ul><h3 id="4-设置主题"><a href="#4-设置主题" class="headerlink" title="#4 设置主题"></a>#4 设置主题</h3><p>好了，你现在可以设置新的主题了，就像你往常改主题一样，在 <code>个性化</code> 或者 <code>设置</code> 中改都行。</p><p><img src="https://i.imgur.com/t4aOUHw.png" alt=""></p><font color="gray">*选择主题*</font><h3 id="5-别急，还有一点"><a href="#5-别急，还有一点" class="headerlink" title="#5 别急，还有一点"></a>#5 别急，还有一点</h3><p>其实到上面已经完了，但是当你锁屏或者重启电脑后，你发现主题又变回来了，这就需要我们设置一个计划任务自动执行。</p><ul><li>找到 <code>penumbra_10___windows_10_visual_style_by_scope10-d9em2vq\Automated Theme Reset Script</code> ，将 <code>Reapply visual style script.ba_</code> 和 <code>ThemeSwitcher.ex_</code> 分别改为 <code>Reapply visual style script.bat</code> 和 <code>ThemeSwitcher.exe</code> 。</li><li>将这两个文件复制到 <code>C:\Windows\Resources\Themes</code> 。</li><li><code>win+R</code> 运行 <code>Taskschd.msc</code> 调出任务计划程序，点击 <code>操作</code> —&gt; <code>导入任务</code> ，选择 <code>penumbra_10___windows_10_visual_style_by_scope10-d9em2vq\Automated Theme Reset Script\Reapply visual style (Penumbra 10).xml</code> ，然后确定。</li></ul><p>好了，这就全部完成了。</p><hr><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>其实还是有点不完美的，但是在我的可接受范围之内。暂且就这么多，想到了或者以后遇到了我再加。</p><h3 id="1-开始-图标"><a href="#1-开始-图标" class="headerlink" title="#1  开始 图标"></a>#1  <code>开始</code> 图标</h3><p><code>开始</code> 图标变成了类似 Cortana 的圆形，你可以在上面的 <code>主界面</code> 图的左下角看到「一双眼睛」，其实第一个是开始菜单，第二个是 Cortana 。</p><h3 id="2-Chrome-地址栏"><a href="#2-Chrome-地址栏" class="headerlink" title="#2 Chrome 地址栏"></a>#2 Chrome 地址栏</h3><p>Chrome 的地址栏变成了黑色背景。</p><p><img src="https://i.imgur.com/4klhOks.png" alt=""></p><h3 id="3-右键字体"><a href="#3-右键字体" class="headerlink" title="#3 右键字体"></a>#3 右键字体</h3><p>右键菜单中文变成了宋体字体，我找了找方法貌似可以改，不过我没试，有谁试了成功了的话可以在评论区说下或者私信我，谢谢 :-)</p><h3 id="4-Word-背景"><a href="#4-Word-背景" class="headerlink" title="#4 Word 背景"></a>#4 Word 背景</h3><p>Word 中背景会显示为黑色，其他办公软件如 Excel、PPT都会有不同程度影响。</p><h3 id="5-PotPlayer-右键菜单"><a href="#5-PotPlayer-右键菜单" class="headerlink" title="#5 PotPlayer 右键菜单"></a>#5 PotPlayer 右键菜单</h3><p>PotPlayer 右键菜单变为全灰，看不清字了。</p><p><img src="https://i.imgur.com/wjPoCg9.png" alt="potplayer"></p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://scope10.deviantart.com/art/Penumbra-10-Windows-10-visual-style-568740374" target="_blank" rel="noopener">Penumbra 10 - Windows 10 visual style</a></li><li><a href="https://www.youtube.com/watch?v=7GgIjEDYe6g" target="_blank" rel="noopener">How to Install a Dark Theme for File Explorer!</a></li></ul><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来水一篇，说说我前几天某天上午初步实现了我一直想弄的东西：Windows 10 资源管理器黑色风格，用了几天，整体上感觉还不错，当然也有点小瑕疵，我会在后面说。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有图可右键在新标签页打开查看大图。&lt;/p&gt;
&lt;/blockquot
      
    
    </summary>
    
    
      <category term="Windows" scheme="https://secsilm.github.io/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降优化算法概述</title>
    <link href="https://secsilm.github.io/2017/10/08/gradient-descent-methods/"/>
    <id>https://secsilm.github.io/2017/10/08/gradient-descent-methods/</id>
    <published>2017-10-08T13:41:00.000Z</published>
    <updated>2017-12-23T04:54:22.559Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原文作者简介：<a href="http://ruder.io/" target="_blank" rel="noopener">Sebastian Ruder</a> 是我非常喜欢的一个博客作者，是 NLP 方向的博士生，目前供职于一家做 NLP 相关服务的爱尔兰公司 <a href="http://aylien.com/" target="_blank" rel="noopener">AYLIEN</a>，博客主要是写机器学习、NLP 和深度学习相关的文章。</p><ul><li>本文原文是 <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a>，同时作者也在 arXiv 上发了一篇同样内容的 <a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">论文</a>。</li><li>本文结合了两者来翻译，但是阅读原文我个人建议读博客中的，感觉体验更好点。</li><li>文中括号中或者引用块中的 <em>斜体字</em> 为对应的英文原文或者我自己注释的话（会标明 <code>译者注</code>），否则为原文中本来就有的话。</li><li>为方便阅读，我在引用序号后加了所引用论文的题目，用 <em>斜体字</em> 表示，例如 Learning rate schedules [11，<em>A stochastic approximation method</em>] 。</li><li>水平有限，如有错误欢迎指出。翻译尽量遵循原文意思，但不意味着逐字逐句。</li><li>本文也放在 <a href="https://github.com/secsilm/awesome-posts" target="_blank" rel="noopener">我 GitHub 上的 awesome-posts 项目</a> 上，选取数据科学和机器学习领域内比较好的英文文章进行翻译，欢迎各位 star 。</li></ul></blockquote><a id="more"></a><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>梯度下降算法虽然最近越来越流行，但是始终是作为一个「黑箱」在使用，因为对他们的优点和缺点的实际解释（practical explainations）很难实现。这篇文章致力于给读者提供这些算法工作原理的一个直观理解。在这篇概述中，我们将研究梯度下降的不同变体，总结挑战，介绍最常见的优化算法，介绍并行和分布式设置的架构，并且也研究了其他梯度下降优化策略。</p><hr><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>梯度下降是最流行的优化算法之一，也是目前优化神经网络最常用的算法。同时，每一个最先进的深度学习库都包含了梯度下降算法的各种变体的实现（例如 <a href="http://lasagne.readthedocs.io/en/latest/modules/updates.html" target="_blank" rel="noopener">lasagne</a>，<a href="http://caffe.berkeleyvision.org/tutorial/solver.html" target="_blank" rel="noopener">caffe</a>，<a href="https://keras.io/optimizers/" target="_blank" rel="noopener">keras</a>）。然而始终是作为一个「黑箱」在使用，因为对他们的优点和缺点的实际解释很难实现。这篇文章致力于给读者提供这些算法工作原理的一个直观理解。我们首先介绍梯度下降的不同变体，然后简单总结下在训练中的挑战。接着，我们通过展示他们解决这些挑战的动机以及如何推导更新规则来介绍最常用的优化算法。我们也会简要介绍下在并行和分布式架构中的梯度下降。最后，我们会研究有助于梯度下降的其他策略。</p><p>梯度下降是一种最小化目标函数 $J(\theta)$ 的方法，其中 $\theta \in \mathbb{R^d}$ 是模型参数，而最小化目标函数是通过在其关于 $\theta$ 的 <a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6" target="_blank" rel="noopener">梯度</a> $\nabla_\theta J(\theta)$ 的相反方向来更新 $\theta$ 来实现的。而学习率（learning rate）则决定了在到达（局部）最小值的过程中每一步走多长。换句话说，我们沿着目标函数的下坡方向来达到一个山谷。如果你对梯度下降不熟悉，你可以在 <a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">这里</a> 找到一个很好的关于优化神经网络的介绍。</p><hr><h2 id="Gradient-descent-variants"><a href="#Gradient-descent-variants" class="headerlink" title="Gradient descent variants"></a>Gradient descent variants</h2><p>依据计算目标函数梯度使用的数据量的不同，有三种梯度下降的变体。根据数据量的大小，我们在参数更新的准确性和执行更新所需时间之间做了一个权衡。</p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><p>标准的梯度下降，即批量梯度下降（batch gradient descent）（ <em>译者注：以下简称 BGD</em> ），在整个训练集上计算损失函数关于参数 $\theta$ 的梯度。</p><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta)</script><p>由于为了一次参数更新我们需要在整个训练集上计算梯度，导致 BGD 可能会非常慢，而且在训练集太大而不能全部载入内存的时候会很棘手。BGD 也不允许我们在线更新模型参数，即实时增加新的训练样本。</p><p>下面是 BGD 的代码片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>其中 <code>nb_epochs</code> 是我们预先定义好的迭代次数（epochs），我们首先在整个训练集上计算损失函数关于模型参数 <code>params</code> 的梯度向量 <code>params_grad</code>。其实目前最新的深度学习库都已经提供了关于一些参数的高效自动求导。如果你要自己求导求梯度，那你最好使用梯度检查（gradient checking），在 <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">这里</a> 查看关于如何进行合适的梯度检查的提示。</p><p>然后我们在梯度的反方向更新模型参数，而学习率决定了每次更新的步长大小。BGD 对于凸误差曲面（convex error surface）保证收敛到全局最优点，而对于非凸曲面（non-convex surface）则是局部最优点。</p><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><p>随机梯度下降（ <em>译者注：以下简称 SGD</em> ）则是每次使用一个训练样本 $x^{(i)}$ 和标签 $y^{(i)}$ 进行一次参数更新。</p><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J(\theta;x^{x(i)};y^{(i)})</script><p>BGD 对于大数据集来说执行了很多冗余的计算，因为在每一次参数更新前都要计算很多相似样本的梯度。SGD 通过一次执行一次更新解决了这种冗余。因此通常 SGD 的速度会非常快而且可以被用于在线学习。SGD 以高方差的特点进行连续参数更新，导致目标函数严重震荡，如图 1 所示。</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png" alt=""><br><em>图 1：SGD 震荡，来自 <a href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png" target="_blank" rel="noopener">Wikipedia</a></em></p><p>BGD 能够收敛到（局部）最优点，然而 SGD 的震荡特点导致其可以跳到新的潜在的可能更好的局部最优点。已经有研究显示当我们慢慢的降低学习率时，SGD 拥有和 BGD 一样的收敛性能，对于非凸和凸曲面几乎同样能够达到局部或者全局最优点。</p><p>代码片段如下，只是加了个循环和在每一个训练样本上计算梯度。注意依据 <a href="http://ruder.io/optimizing-gradient-descent/index.html#shufflingandcurriculumlearning" target="_blank" rel="noopener">这里</a> 的解释，我们在每次迭代的时候都打乱训练集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">    np.random.shuffle(data)</span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">        params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">        params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>Mini-batch gradient descent（ <em>译者注：以下简称 MBGD</em> ）则是在上面两种方法中采取了一个折中的办法：每次从训练集中取出 $n$ 个样本作为一个 mini-batch，以此来进行一次参数更新。</p><script type="math/tex; mode=display">\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})</script><p>这样做有两个好处：</p><ul><li>减小参数更新的方差，这样可以有更稳定的收敛。</li><li>利用现在最先进的深度学习库对矩阵运算进行了高度优化的特点，这样可以使得计算 mini-batch 的梯度更高效。</li></ul><p>通常来说 mini-batch 的大小为 50 到 256 之间，但是也会因为任务的差异而不同。MBGD 是训练神经网络时的常用方法，而且通常即使实际上使用的是 MBGD，也会使用 SGD 这个词来代替。注意：在本文接下来修改 SGD 时，为了简单起见我们会省略参数 $x^{(i:i+n)}; y^{(i:i+n)}$。</p><p>代码片段如下，我们每次使用 mini-batch 为 50 的样本集来进行迭代：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">    np.random.shuffle(data)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">        params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">        params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><hr><h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>标准的 MBGD 并不保证好的收敛，也提出了一下需要被解决的挑战：</p><ul><li><strong>选择一个好的学习率是非常困难的</strong>。太小的学习率导致收敛非常缓慢，而太大的学习率则会阻碍收敛，导致损失函数在最优点附近震荡甚至发散。</li><li>Learning rate schedules [11，<em>A stochastic approximation method</em>] 试图在训练期间调整学习率即退火（annealing），根据先前定义好的一个规则来减小学习率，或者两次迭代之间目标函数的改变低于一个阈值的时候。然而这些规则和阈值也是需要在训练前定义好的，所以<strong>也不能做到自适应数据的特点</strong> [10，<em>Learning rate schedules for faster stochastic gradient search</em>]。</li><li>另外，<strong>相同的学习率被应用到所有参数更新中</strong>。如果我们的数据比较稀疏，特征有非常多不同的频率，那么此时我们可能并不想要以相同的程度更新他们，反而是对更少出现的特征给予更大的更新。</li><li>对于神经网络来说，另一个最小化高度非凸误差函数的关键挑战是<strong>避免陷入他们大量的次局部最优点（suboptimal）</strong>。Dauphin 等人 [19，<em>Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</em>] 指出事实上困难来自于鞍点而不是局部最优点，即损失函数在该点的一个维度上是上坡（slopes up）（ <em>译者注：斜率为正</em> ），而在另一个维度上是下坡（slopes down）（ <em>译者注：斜率为负</em> ）。这些鞍点通常被一个具有相同误差的平面所包围，这使得对于 SGD 来说非常难于逃脱，因为在各个维度上梯度都趋近于 0 。</li></ul><hr><h2 id="Gradient-descent-optimization-algorithms"><a href="#Gradient-descent-optimization-algorithms" class="headerlink" title="Gradient descent optimization algorithms"></a>Gradient descent optimization algorithms</h2><p>接下来，我们将会概述一些在深度学习社区常用的算法，这些算法解决了我们前面提到的挑战。我们不会讨论实际上在高维数据集上不可行的算法，例如二阶方法中的 <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank" rel="noopener">牛顿法</a>。</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>SGD 在遇到沟壑（ravines）会比较困难，即在一个维度上比另一个维度更陡峭的曲面 [1，<em>Two problems with backpropagation and other steepest-descent learning procedures for networks</em>] ，这些曲面通常包围着局部最优点。在这些场景中，SGD 震荡且缓慢的沿着沟壑的下坡方向朝着局部最优点前进，如图 2 所示。</p><p><img src="http://ruder.io/content/images/2015/12/without_momentum.gif" alt="SGD without momentum"><br><em>图 2：不带动量的 SGD</em></p><p>动量（Momentum）[2，<em>On the momentum term in gradient descent learning algorithms</em>] 是一种在相关方向加速 SGD 的方法，并且能够减少震荡，如图 3 所示。</p><p><img src="http://ruder.io/content/images/2015/12/with_momentum.gif" alt="SGD without momentum"><br><em>图 3：带动量的 SGD</em></p><p>它在当前的更新向量中加入了先前一步的状态：</p><script type="math/tex; mode=display">\begin{aligned}v\_t &= \gamma v\_{t-1} + \eta \nabla\_\theta J( \theta) \\\\\theta &= \theta - v\_t\end{aligned}</script><p>注意：一些实现可能改变了公式中的符号。动量项 $\gamma$ 通常设置为 0.9 或者相似的值。</p><p>本质上来说，当我们使用动量时，类似于我们把球推下山的过程。在球下山的过程中，球累积动量使其速度越来越快（直到达到其最终速度，如果有空气阻力的话，即 $\gamma \lt 1$）。相同的事情也发生在我们的参数更新中：对于梯度指向方向相同的维度动量项增大，对于梯度改变方向的维度动量项减小。最终，我们获得了更快的收敛并减少了震荡。</p><h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p>然而，一个球盲目的沿着斜坡下山，这不是我们希望看到的。我们希望有一个聪明的球，他知道将要去哪并可以在斜坡变成上坡前减速。</p><p>Nesterov accelerated gradient（ <em>译者注：以下简称 NAG</em> ）[7，<em>A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)</em>] 就是这样一种给予我们的动量项预知能力的方法。我们知道我们使用动量项 $\gamma v_{t-1}$ 来更新 $\theta$。因此计算 $\theta-\gamma v_{t-1}$ 给了我们一个关于的参数下一个位置的估计（这里省略了梯度项），这是一个简单粗暴的想法。我们现在可以通过计算 $\theta$ 下一个位置而不是当前位置的梯度来实现「向前看」。</p><script type="math/tex; mode=display">\\begin{aligned}v\_t &= \gamma v\_{t-1} + \eta \nabla\_\theta J( \theta - \gamma v\_{t-1} ) \\\\\\theta &= \theta - v\_t\\end{aligned}</script><p>我们仍然设置 $\gamma$ 为 0.9。动量法首先计算当前梯度（图 4 中的小蓝色向量）,然后在更新累积梯度（updated accumulated gradient）方向上大幅度的跳跃（图 4 中的大蓝色向量）。与此不同的是，NAG 首先在先前的累积梯度（previous accumulated gradient）方向上进行大幅度的跳跃（图 4 中的棕色向量），评估这个梯度并做一下修正（图 4 中的红色向量），这就构成一次完整的 NAG 更新（图 4 中的绿色向量）。这种预期更新防止我们进行的太快，也带来了更高的相应速度，这在一些任务中非常有效的提升了 RNN 的性能 [8，<em>Advances in Optimizing Recurrent Networks</em>] 。</p><p><img src="http://ruder.io/content/images/2016/09/nesterov_update_vector.png" alt="Nesterov update"><br><em>图 4：Nesterov 更新，来自 <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">G. Hinton’s lecture 6c</a></em></p><p>可以在 <a href="">这里</a> 查看对 NAG 的另一种直观解释，此外 Ilya Sutskever 在他的博士论文中也给出了详细解释 [9，<em>Training Recurrent neural Networks</em>] 。</p><p>现在我们已经能够依据误差函数的斜率来调整更新，并加快 SGD 的速度，此外我们也想根据每个参数的重要性来决定进行更大还是更小的更新。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad [3，<em>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</em>] 就是这样一种解决这个问题的基于梯度的优化算法：根据参数来调整学习率，对于不常见的参数给予更大的更新，而对于常见的给予更小的更新。因此，Adagrad 非常适用于稀疏数据。Dean 等人 [4，<em>Large Scale Distributed Deep Networks</em>] 发现 Adagrad 能够大幅提高 SGD 的鲁棒性，并在 Google 用其训练大规模神经网络，这其中就包括 <a href="http://www.wired.com/2012/06/google-x-neural-network/" target="_blank" rel="noopener">在 YouTube 中学习识别猫</a>。除此之外，Pennington 等人 [5，<em>Glove: Global Vectors for Word Representation</em>] 使用 Adagrad 来训练 GloVe 词嵌入，因为罕见的词汇需要比常见词更大的更新。</p><p>前面我们在对所有参数 $\theta$ 更新时每个参数 $\theta_i$ 使用相同的学习率 $\eta$。Adagrad 在每个时间点 $t$ 对每个参数 $ \theta_i $ 使用的学习率都不同，我们首先展现每个参数的更新，然后再向量化。简单起见，我们使用 $g_{t,i}$ 来表示目标函数关于参数 $\theta_i$ 在时间点 $t$ 时的梯度：</p><script type="math/tex; mode=display">g\_{t, i} = \nabla\_\theta J( \theta\_i )</script><p>SGD 在每个时间点 $t$ 对每个参数 $\theta_i$ 的更新变为：</p><script type="math/tex; mode=display">\theta\_{t+1,i} = \theta\_{t,i} - \eta \cdot g_{t,i}</script><p>在这个更新规则里，Adagrad 在每个时间点 $t$ 对每个参数 $\theta_i$ 都会基于过去的梯度修改学习率 $\eta$。</p><script type="math/tex; mode=display">\theta\_{t+1, i} = \theta\_{t, i} - \dfrac{\eta}{\sqrt{G\_{t, ii} + \epsilon}} \cdot g_{t, i}</script><p>其中 $G_{t} \in \mathbb{R}^{d \times d}$ 是一个对角矩阵，对角元素 $G_t[i, i]$ 是参数 $\theta_i$ 从开始到时间点 $t$ 为止的梯度平方和 [25]，$\epsilon$ 是一个平滑项，用于防止分母为 0 ，通常为 $10^{-8}$ 左右。有趣的是，如果去掉开方操作，算法性能会大幅下降。</p><p>由于 $G_t$ 的对角元素是关于所有参数的过去的梯度的平方和，我们可以将上面的实现向量化，即使用点乘 $\odot$ ：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{\sqrt{G\_{t} + \epsilon}} \odot g_{t}</script><p>Adagrad 最大的一个优点是我们可以不用手动的调整学习率。大多数实现使用一个默认值 0.01 。</p><p>Adagrad 主要的缺点是分母中累积的平方和梯度：由于每一个新添加的项都是正的，导致累积和在训练期间不断增大。这反过来导致学习率不断减小，最终变成无限小，这时算法已经不能再继续学习新东西了。下面的这个算法就解决了这个问题。</p><h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>Adadelta [6，<em>ADADELTA: An Adaptive Learning Rate Method</em>] 是 Adagrad 的扩展，旨在帮助缓解后者学习率单调下降的问题。与 Adagrad 累积过去所有梯度的平方和不同，Adadelta 限制在过去某个窗口大小为 $w$ 的大小内的梯度。</p><p>存储先前 $w$ 个梯度的平方效率不高，Adadelta 的梯度平方和被递归的定义为过去所有梯度平方的衰减平均值（decaying average）。在 $t$ 时刻的平均值 $E[g^2]_t$ 仅仅取决于先前的平均值和当前的梯度：</p><script type="math/tex; mode=display">E[g^2]\_t = \gamma E[g^2]\_{t-1} + (1-\gamma)g_t^2</script><p>其中 $\gamma$ 类似于动量项，我们同样设置为 0.9 左右。为清楚起见，我们根据参数更新向量 $\Delta \theta_t$ 来重写一般的 SGD 更新公式：</p><script type="math/tex; mode=display">\begin{aligned}\Delta \theta\_t &= - \eta \cdot g\_{t, i} \\\\\theta\_{t+1} &= \theta\_t + \Delta \theta_t \\\\\end{aligned}</script><p>先前我们推导过的 Adagrad 的参数更新向量是：</p><script type="math/tex; mode=display">\Delta \theta\_t = - \dfrac{\eta}{\sqrt{G\_{t} + \epsilon}} \odot g_{t}</script><p>我们现在用过去梯度平方和的衰减平均 $E[g^2]_t$ 来代替对角矩阵 $G_t$：</p><script type="math/tex; mode=display">\Delta \theta\_t = - \dfrac{\eta}{\sqrt{E[g^2]\_t + \epsilon}} g_{t}</script><p>由于分母只是一个梯度的均方误差（Root Mean Squared，RMS），我们可以用缩写来代替：</p><script type="math/tex; mode=display">\Delta \theta\_t = - \dfrac{\eta}{RMS[g]\_{t}} g_t</script><p>作者注意到这个更新中的单位（units）不匹配（SGD，动量和 Adagrad 也是），即更新向量应该具有和参数一样的单位。为解决这个问题，他们首先定义了另一个指数衰减平均,但是这次不是关于梯度的平方，而是关于参数更新的平方：</p><script type="math/tex; mode=display">E[\Delta \theta^2]\_t = \gamma E[\Delta \theta^2]\_{t-1} + (1 - \gamma) \Delta \theta^2_t</script><p>那么参数更新的均方误差是：</p><script type="math/tex; mode=display">RMS[\Delta \theta]\_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}</script><p>由于 $RMS[\Delta \theta]_{t}$ 未知，我们可以使用上一步的来估计。用带有 $RMS[\Delta \theta]_{t-1}$ 的参数更新规则代替学习率 $\eta$ 就得到了 Adadelta 最终的更新规则：</p><script type="math/tex; mode=display">\begin{aligned}\Delta \theta\_t &= - \dfrac{RMS[\Delta \theta]\_{t-1}}{RMS[g]\_{t}} g\_{t} \\\\\theta\_{t+1} &= \theta\_t + \Delta \theta\_t \\\\\end{aligned}</script><p>使用 Adadelta 时我们甚至不需要指定一个默认的学习率，因为它已经不在更新规则中了。</p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop 是一种未发布的自适应学习率的方法，由 Geoff Hinton 在 <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Lecture 6e of his Coursera Class</a> 中提出。</p><p>RMSprop 和 Adadelta 在同一时间被独立地发明出来，都是为了解决 Adagrad 的学习率递减问题。事实上 RMSprop 与我们上面讨论过的 Adadelta 的第一个更新向量一模一样：</p><script type="math/tex; mode=display">\begin{aligned}E[g^2]\_t &= 0.9 E[g^2]\_{t-1} + 0.1 g^2\_t \\\\\theta\_{t+1} &= \theta\_t - \dfrac{\eta}{\sqrt{E[g^2]\_t + \epsilon}} g\_{t} \\\\\end{aligned}</script><p>RMSprop 也是将学习率除以平方梯度的指数衰减平均值。Hinton 建议将 $\gamma$ 设为 0.9 ，默认学习率 $\eta$ 设为 0.001 。</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adaptive Moment Estimation (Adam) [15，<em>Adam: a Method for Stochastic Optimization</em>] 是另一种为每个参数计算自适应学习率的方法。除了像 Adadelta 和 RMSprop 一样存储历史平方梯度 $v_t$ 的指数衰减平均值外，Adam 也存储历史梯度 $m_t$ 的指数衰减平均值，类似于动量：</p><script type="math/tex; mode=display">\begin{aligned}m\_t &= \beta\_1 m\_{t-1} + (1 - \beta\_1) g\_t \\\\v\_t &= \beta\_2 v\_{t-1} + (1 - \beta\_2) g\_t^2 \\\\\end{aligned}</script><p>其中 $m_t$ 和 $v_t$ 分别是梯度在第一时刻（平均值，the mean）和第二时刻（未中心化的方差，the uncentered variance）的估计值，也就是这个方法的名称。由于 $m_t$ 和 $v_t$ 用零向量初始化，Adam 的作者发现他们趋向于 0 ，特别是最开始的时候（<em>the initial time steps</em>）和衰减率很小的时候（即 $\beta_1$ 和 $\beta_2$ 接近于 1）。</p><p>他们通过计算偏差纠正的（bias-corrected）的第一和第二时刻的估计值来抵消这个问题：</p><script type="math/tex; mode=display">\begin{aligned}\hat{m}\_t &= \dfrac{m\_t}{1 - \beta^t\_1} \\\\\hat{v}\_t &= \dfrac{v\_t}{1 - \beta^t\_2}\end{aligned}</script><p>然后他们使用这些公式来更新参数，就像 Adadelta 和 RMSprop 一样：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} \hat{m}\_t</script><p>作者建议 $\beta_1$ 的默认值为 0.9 ， $\beta_2$ 的默认值为 0.999 ，$\epsilon$ 的默认值为 $10^{-8}$ 。他们证明 Adam 在实践中非常有效，而且对比其他自适应学习率算法也有优势。</p><h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p>Adam 的更新规则中的 $v_t$ 成比例的缩放了梯度，正比于历史梯度的 $\ell_2$ 范数（通过 $v_{t-1}$ 项）和当前梯度 $|g_t|^2$（<em>译者注：此段话我非常不确定是这么翻译的，贴上原文：The $v_t$ factor in the Adam update rule scales the gradient inversely proportionally to the $\ell_2$ norm of the past gradients (via the vt−1vt−1 term) and current gradient $|g_t|^2$</em>）：</p><script type="math/tex; mode=display">v\_t = \beta\_2 v\_{t-1} + (1 - \beta\_2) |g\_t|^2</script><p>我们可以将此推广到 $\ell_p$ 范数。注意 Kingma 和 Ba 也将 $\beta_2$ 参数化为 $\beta_2^p$：</p><script type="math/tex; mode=display">v\_t = \beta\_2^p v\_{t-1} + (1 - \beta\_2^p) |g\_t|^p</script><p>当 $p$ 非常大的时候通常会导致数值上的不稳定（<em>numerically unstable</em>），这也是实际中通常使用 $\ell_1$ 和 $\ell_2$ 的原因。然而，$\ell_\infty$ 通常也会比较稳定。因此，作者提出了 AdaMax（Kingma and Ba, 2015），显示了结合了 $\ell_\infty$ 的 $v_t$ 也能够收敛到下面的更稳定的值。为了避免与 Adam 混淆，我们使用 $u_t$ 来表示无限范数约束的 $v_t$（infinity norm-constrained）：</p><script type="math/tex; mode=display">\begin{aligned}u\_t &= \beta\_2^\infty v\_{t-1} + (1 - \beta\_2^\infty) |g\_t|^\infty\\\\              & = \max(\beta\_2 \cdot v\_{t-1}, |g\_t|)\end{aligned}</script><p>我们现在可以将此加进 Adam 的更新规则里，用 $u_t$ 代替 $\sqrt{\hat{v}_t} + \epsilon$，得到 AdaMax 的更新规则：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{u\_t} \hat{m}\_t</script><p>注意到 $u_t$ 依赖于 <strong>max</strong> 操作，这不像 Adam 中的 $m_t$ 和 $v_t$ 那样容易趋向于 0（<em>bias towards zero</em>），这也是我们不需要为 $u_t$ 计算偏差纠正的原因。建议的默认值是 $\eta = 0.002$，$\beta_1 = 0.9$ 和 $\beta_2 = 0.999$ 。</p><h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><p> 正如前面说的那样，Adam 可以看成 RMSprop 和动量的组合：RMSprop 贡献了历史平方梯度的指数衰减的平均值 $v_t$ ，而动量则负责历史梯度的指数衰减平均值 $m_t$ 。我们也看到 NAG 要优于普通的动量。</p><p>Nadam（Nesterov-accelerated Adaptive Moment Estimation）[24，<em>Incorporating Nesterov Momentum into Adam</em>] 结合了 Adam 和 NAG 。为了将 NAG 融进 Adam 中，我们需要更改其中的动量项 $m_t$ 。</p><p>首先，让我们使用当前的符号回顾一下动量更新规则：</p><script type="math/tex; mode=display">\begin{aligned}g\_t &= \nabla\_{\theta\_t}J(\theta\_t)\\\\m\_t &= \gamma m\_{t-1} + \eta g\_t\\\\\theta_{t+1} &= \theta\_t - m\_t\end{aligned}</script><p>其中 $J$ 是我们的目标函数，$\gamma$ 是动量衰减项，$\eta$ 是我们的步长。将 $m_t$ 代入上面的第三个式子展开得到：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_t - ( \gamma m\_{t-1} + \eta g_t)</script><p>这再次证明了动量更新涉及两个方向：先前动量向量的方向和当前梯度的方向。</p><p>在计算梯度前通过动量更新参数，这使得 NAG 允许我们在梯度方向上执行一个更准确的步骤。因此我们只需更改 $g_t$ 来将 NAG 融进去：</p><script type="math/tex; mode=display">\begin{aligned}g\_t &= \nabla\_{\theta\_t}J(\theta\_t - \gamma m\_{t-1})\\\\m\_t &= \gamma m_{t-1} + \eta g\_t\\\\\theta\_{t+1} &= \theta\_t - m\_t\end{aligned}</script><p>Dozat 提出按以下方式来修改 NAG ：与应用动量步骤两次不同的是 —— 一次用来更新梯度 $g<em>t$ 和 一次用来更新参数 $\theta\</em>{t+1}$ ，我们现在直接对当前参数应用一个「向前看的」（<em>look-ahead</em>）动量向量：</p><script type="math/tex; mode=display">\begin{aligned}g\_t &= \nabla\_{\theta\_t}J(\theta\_t)\\\\m\_t &= \gamma m\_{t-1} + \eta g\_t\\\\\theta\_{t+1} &= \theta\_t - (\gamma m\_t + \eta g\_t)\end{aligned}</script><p>注意我们现在不再使用如上面展开的动量更新规则中的先前动量向量 $m_{t-1}$ ，而是使用当前动量向量 $m_t$ 来「向前看」。为了将 Nesterov 动量加进 Adam ，相似地我们可以用当前动量向量代替先前动量向量。首选，我们先回顾一下 Adam 的更新规则（注意我们不用修改 $\hat v_t$）：</p><script type="math/tex; mode=display">\begin{aligned} m\_t &= \beta\_1 m\_{t-1} + (1 - \beta\_1) g\_t\\\\\hat{m}\_t & = \frac{m\_t}{1 - \beta^t\_1}\\\\\theta\_{t+1} &= \theta\_{t} - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} \hat{m}\_t\end{aligned}</script><p>将 $\hat m_t$ 和 $m_t$ 定义代入展开得到：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\dfrac{\beta\_1 m\_{t-1}}{1 - \beta^t\_1} + \dfrac{(1 - \beta\_1) g\_t}{1 - \beta^t\_1})</script><p>注意到 $\dfrac{m_{t-1}}{1-\beta_1^t}$ 是先前动量向量 $m_{t-1}$ 的偏差修正，因此我们可以用 $m_{t-1}$ 来代替之：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\beta\_1 \hat{m}\_{t-1} + \dfrac{(1 - \beta\_1) g\_t}{1 - \beta^t_1})</script><p>这个式子与我们上面展开得到的动量更新规则非常相似。我们可以加进 Nesterov 动量，就像我们之前一样简单地用当前动量向量的偏差修正估计 $\hat m<em>t$ 来代替先前动量向量的偏差修正估计 $\hat m</em>{t-1}$，得到 Nadam 的更新规则：</p><script type="math/tex; mode=display">\theta\_{t+1} = \theta\_{t} - \dfrac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\beta\_1 \hat{m}\_t + \dfrac{(1 - \beta\_1) g\_t}{1 - \beta^t_1})</script><h3 id="Visualization-of-algorithms"><a href="#Visualization-of-algorithms" class="headerlink" title="Visualization of algorithms"></a>Visualization of algorithms</h3><p>下面的两个动画（来自 <a href="https://twitter.com/alecrad" target="_blank" rel="noopener">Alec Radford</a>）提供了对当前优化算法行为的直观解释。也可以在 <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">这里</a> 查看 Karpathy 对这些动画的描述和对我们讨论过的算法的解释。</p><p><img src="http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif" alt="SGD optimization on loss surface contours"><br><em>图 5：在损失曲面等值线上的 SGD 优化</em></p><p>在图 5 中我们可以看到他们在损失曲面的等值线上（<a href="https://www.sfu.ca/~ssurjano/beale.html" target="_blank" rel="noopener">the Beale function</a>）随时间的变化趋势。注意到 Adadelta 、Adagrad 和 RMSprop 几乎立即开始在正确的方向下降并收敛速度几乎一样快，然而动量和 NAG 则「脱轨」了。不过由于 NAG 的「向前看」能力使其很快的纠正方向并朝着最小点前进。</p><blockquote><p><em>译者注：方便起见我把 Beale 函数的图像和解析式放在这里。</em></p><p><img src="https://www.sfu.ca/~ssurjano/beale.png" alt="Beale Function"></p><script type="math/tex; mode=display">f(\mathbf x) = (1.5-x_1+x_1x_2)^2 + (2.25-x_1+x_1x_2^2)^2 + (2.625-x_1+x_1+x_2^3)^2</script><p>通常来说该函数的定义域为 $x_i \in [-4.5, 4.5]$ ，其中 $i = 1, 2$ 。全局最小点为 $f(\mathbf{x^*}) = 0$ ，当 $\mathbf{x^*} = (3, 0.5)$ 时 。</p></blockquote><p><img src="http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif" alt="SGD optimization on saddle point"><br><em>图 6：在鞍点处的 SGD 优化</em></p><p>图 6 显示了在鞍点处的算法行为，即该点在一个方向斜率为正，其他方向斜率为负，正如我们之前提到的这对于 SGD 是一个难点。注意到 SGD 、动量和 NAG 很难打破对称性，尽管后两者最终逃离的鞍点，但是 Adagrad 、RMSprop 和 Adadelta 很快朝着斜率为负的方向前进了。</p><p>可以看到自适应学习率的方法，例如 Adagrad 、Adadelta 、RMSprop 和 Adam 在这些场景中是最合适的并且提供了最好的收敛。</p><h3 id="Which-optimizer-to-use"><a href="#Which-optimizer-to-use" class="headerlink" title="Which optimizer to use?"></a>Which optimizer to use?</h3><p>那么，你应该使用哪种优化算法呢？如果你的数据比较稀疏，那么使用自适应学习率的算法很可能会让你得到最好的结果。另外一个好处是你不用去调节学习率，使用默认的设置可能就会让你达到最好的效果。</p><p>总的来说，RMSprop 是 Adagrad 的一种扩展，用来解决后者学习率逐渐递减的问题。它和 Adadelta 非常像，除了 Adadelta 在更新规则的分子上使用参数更新的 RMS （<em>译者注：均方误差</em>）。Adam 最终在 RMSprop 的基础上加了偏差修正和动量。在这方面，RMSprop 、Adadelta 和 Adam 非常相似，在相似的环境下也表现地一样好。Kingma 等人 [15，<em>Adam: a Method for Stochastic Optimization</em>] 表示随着梯度越来越稀疏，Adam 的偏差修正使其略微优于 RMSprop 。在这个方面总体上来说 Adam 可能是最好的选择。</p><p>有趣的是，许多最近的论文仅仅使用普通的不带动量 SGD 和一个简单的学习率退火机制（<em>annealing schedule</em>）。正如我们前面所讨论的，SGD 通常会找到最优点，但是相比其他一些优化算法可能花费的时间比较长，更依赖于一个好的初始化和退火机制，而且也可能陷入鞍点而不是局部最优点。所以，<strong>如果你比较关心收敛速度并且在训练一个深度或者复杂的神经网络，你应该选择一个自适应学习率算法。</strong></p><hr><h2 id="Parallelizing-and-distributing-SGD"><a href="#Parallelizing-and-distributing-SGD" class="headerlink" title="Parallelizing and distributing SGD"></a>Parallelizing and distributing SGD</h2><p>鉴于大数据解决方案的普及以及低价集群的可用性，使用分布式 SGD 来进一步加速训练是一个很明显的选择。SGD 本质上是按顺序执行的：我们一步一步朝着最优点前进。SGD 提供了较好的收敛但是在大数据集上可能会速度较慢。相反，异步执行 SGD 比较快，但是 worker 之间不理想的通信可能会造成比较差的收敛。另外，我们也可以不需要大型计算集群，在一台机器上并行执行 SGD 。下面是目前提出的用于优化并行和分布式 SGD 的算法和架构。</p><h3 id="Hogwild"><a href="#Hogwild" class="headerlink" title="Hogwild!"></a>Hogwild!</h3><p>Niu 等人 [23，<em>Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</em>] 引入了一个更新机制称为 Hogwild!，可以在 CPU 上并行执行 SGD 。处理器可以在不锁参数的情况下访问共享内存。由于没次更新只修改一部分参数，所以这种方法只适合于输入数据比较稀疏的情况。他们表明在这种情况下该方法可以达到几乎最优的收敛速度，因为处理器是不太可能覆盖有用信息的。</p><h3 id="Downpour-SGD"><a href="#Downpour-SGD" class="headerlink" title="Downpour SGD"></a>Downpour SGD</h3><p>Downpour SGD 是一种 SGD 的异步变体，由 Dean 等人 [4，<em>Large Scale Distributed Deep Networks</em>] 在谷歌的 DistBelief 框架（TensorFlow 的前身）中使用。它在训练数据的子集上并行的运行一个模型的多个副本。这些模型将他们的更新发送到一个参数服务器，他们分布在多个机器上。每个机器只负责存储和更新全部模型参数的一部分。然而由于这些机器并不需要相互通信，例如共享权重或者更新，导致他们的参数一直有发散的风险，阻碍收敛。</p><h3 id="Delay-tolerant-Algorithms-for-SGD"><a href="#Delay-tolerant-Algorithms-for-SGD" class="headerlink" title="Delay-tolerant Algorithms for SGD"></a>Delay-tolerant Algorithms for SGD</h3><p>McMahan 和 Streeter [12，<em>Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning</em>] 通过开发一个延迟容忍（delay-tolerant）算法来扩展 Adagrad 使其并行化，该算法不仅自适应历史梯度，而且也更新延迟（delays）。在实际中也被证明该方法很有效。</p><h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><p>TensorFlow [13，<em>TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed System</em>] 是 Google 最近开源的用于实现和部署大规模机器学习模型的框架。TensorFlow 基于他们使用 DistBelief 的经验，并且已经在内部使用，用于在大范围的移动设备和大规模分布式系统上执行计算。分布式执行中，一个计算图针对每一个设备被拆分成多个子图，使用发送/接收节点对进行通信。</p><h3 id="Elastic-Averaging-SGD"><a href="#Elastic-Averaging-SGD" class="headerlink" title="Elastic Averaging SGD"></a>Elastic Averaging SGD</h3><p>Zhang 等人 [14，<em>Deep learning with Elastic Averaging SGD</em>] 提出了 Elastic Averaging SGD（EASGD），将异步 SGD 中每个 worker 参数用一个「弹力」（<em>elastic force</em>）连接起来，即一个由参数服务器保存的中心变量。这允许本地变量在中心变量附近进一步震荡，这理论上可以在更大的参数空间中进行探索。他们以经验证明这种增加的探索能力可以寻找新的局部最优点从而提升性能。</p><hr><h2 id="Additional-strategies-for-optimizing-SGD"><a href="#Additional-strategies-for-optimizing-SGD" class="headerlink" title="Additional strategies for optimizing SGD"></a>Additional strategies for optimizing SGD</h2><p>最后，我们介绍一些可以和前面讨论的算法一起使用的策略，用以进一步提升 SGD 的性能。对于一些其他常用的技巧，可以参见 [22，<em>Efficient BackProp. Neural Networks: Tricks of the Trade</em>] 。</p><h3 id="Shuffling-and-Curriculum-Learning"><a href="#Shuffling-and-Curriculum-Learning" class="headerlink" title="Shuffling and Curriculum Learning"></a>Shuffling and Curriculum Learning</h3><p>通常，我们想要避免给模型提供的训练数据是有特定顺序的，因为这会使模型带有偏见。因此，每次迭代完之后打乱训练数据是一个很好地想法。</p><p>但是另一方面，有些情况下我们想要逐步解决更难的问题，我们将训练数据以一种有意义的顺序提供给模型，这可能会提升性能和得到更好的收敛。构建这种有意义的顺序的方法称为课程学习（<em>Curriculum Learning</em>）[16，<em>Curriculum learning</em>] 。</p><p>Zaremba 和 Sutskever [17，<em>Learning to Execute</em>] 只能训练 LSTMs 来评估使用课程学习的简单程序，而且表明组合或者混合的方法要比单一方法更有效，通过增加难度来排序样本。</p><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>为方便学习，我们一般会正规化（<em>normalize</em>）参数的初始值，以 0 均值和单位方差来初始化。随着训练的进行和我们将参数更新到不同的程度，我们损失了这种正规化，导致训练速度变慢并且随着网络越来越深，这种影响被渐渐放大。</p><p>Batch normalization [18，<em>Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>] 重新为每一个 mini-batch 建立了这种正规化并且变化也会随着这个操作反向传播。通过在模型中加入正规化，我们可以使用更高的学习率而且不用太关心参数的初始化。Batch normalization 此外也扮演者正则化的角色，可以减少（甚至有些时候消除）Dropout 的需要.</p><h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>根据 Geoff Hinton: “<em>Early stopping (is) beautiful free lunch</em>“（<a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf" target="_blank" rel="noopener">NIPS 2015 Tutorial slides</a>，slide 63），你应该在训练时时刻监视着验证集误差，并且在你的验证集误差不再足够地降低时停止训练。</p><h3 id="Gradient-noise"><a href="#Gradient-noise" class="headerlink" title="Gradient noise"></a>Gradient noise</h3><p>Neelakantan 等人 [21，<em>Adding Gradient Noise Improves Learning for Very Deep Network</em>] 在每次梯度更新时加上一个服从高斯分布 $N(0, \sigma_t^2)$ 的噪声：</p><script type="math/tex; mode=display">g\_{t, i} = g\_{t, i} + N(0, \sigma^2_t)</script><p>他们依照下面的公式更新（<em>anneal</em>）方差：</p><script type="math/tex; mode=display">\sigma^2_t = \dfrac{\eta}{(1 + t)^\gamma}</script><p>他们表明添加这个噪声使得网络对较差的初始化更具有鲁棒性并且尤其对训练深度和复杂的网络很有帮助。他们怀疑添加的噪声使得模型有更多机会逃离和找到新的局部最优点，这在深度模型中很常见。</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文中，我们首先看了梯度下降的 3 中变体，其中 mini-batch 梯度下降最流行。我们然后研究了几种最常使用的用于优化 SGD 的算法：动量，Nesterov accelerated gradient，Adagrad，Adadelta，RMSprop，Adam 以及为优化异步 SGD 的不同算法。最后，我们考虑了用于提升 SGD 性能的其他策略，例如 shuffling 与 curriculum learning，batch normalization 以及 early stopping。我希望这篇文章可以给你提供一个关于不同优化算法的行为和动机的直观理解。</p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society.</li><li>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151. <a href="http://doi.org/10.1016/S0893-6080(98)00116-6" target="_blank" rel="noopener">http://doi.org/10.1016/S0893-6080(98)00116-6</a></li><li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="noopener">http://jmlr.org/papers/v12/duchi11a.html</a></li><li>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. <a href="http://doi.org/10.1109/ICDAR.2011.95" target="_blank" rel="noopener">http://doi.org/10.1109/ICDAR.2011.95</a></li><li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162" target="_blank" rel="noopener">http://doi.org/10.3115/v1/D14-1162</a></li><li>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="http://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">http://arxiv.org/abs/1212.5701</a></li><li>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543– 547.</li><li>Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="http://arxiv.org/abs/1212.0901" target="_blank" rel="noopener">http://arxiv.org/abs/1212.0901</a></li><li>Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis.</li><li>Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1–11. <a href="http://doi.org/10.1109/NNSP.1992.253713" target="_blank" rel="noopener">http://doi.org/10.1109/NNSP.1992.253713</a></li><li>H. Robinds and S. Monro, “A stochastic approximation method,” Annals of Mathematical Statistics, vol. 22, pp. 400–407, 1951.</li><li>Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1–9. Retrieved from <a href="http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf</a></li><li>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems.</li><li>Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1–24. Retrieved from <a href="http://arxiv.org/abs/1412.6651" target="_blank" rel="noopener">http://arxiv.org/abs/1412.6651</a></li><li>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13.</li><li>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41–48. <a href="http://doi.org/10.1145/1553374.1553380" target="_blank" rel="noopener">http://doi.org/10.1145/1553374.1553380</a></li><li>Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1–25. Retrieved from <a href="http://arxiv.org/abs/1410.4615" target="_blank" rel="noopener">http://arxiv.org/abs/1410.4615</a></li><li>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3.</li><li>Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1–14. Retrieved from <a href="http://arxiv.org/abs/1406.2572" target="_blank" rel="noopener">http://arxiv.org/abs/1406.2572</a></li><li>Sutskever, I., &amp; Martens, J. (2013). On the importance of initialization and momentum in deep learning. <a href="http://doi.org/10.1109/ICASSP.2013.6639346" target="_blank" rel="noopener">http://doi.org/10.1109/ICASSP.2013.6639346</a></li><li>Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.06807" target="_blank" rel="noopener">http://arxiv.org/abs/1511.06807</a></li><li>LeCun, Y., Bottou, L., Orr, G. B., &amp; Müller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9–50. <a href="http://doi.org/10.1007/3-540-49430-8_2" target="_blank" rel="noopener">http://doi.org/10.1007/3-540-49430-8_2</a></li><li>Niu, F., Recht, B., Christopher, R., &amp; Wright, S. J. (2011). Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1–22.</li><li>Dozat, T. (2016). Incorporating Nesterov Momentum into Adam. ICLR Workshop, (1), 2013–2016.</li><li>Duchi et al. [3] give this matrix as an alternative to the full matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters dd.</li></ol><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原文作者简介：&lt;a href=&quot;http://ruder.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sebastian Ruder&lt;/a&gt; 是我非常喜欢的一个博客作者，是 NLP 方向的博士生，目前供职于一家做 NLP 相关服务的爱尔兰公司 &lt;a href=&quot;http://aylien.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AYLIEN&lt;/a&gt;，博客主要是写机器学习、NLP 和深度学习相关的文章。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本文原文是 &lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An overview of gradient descent optimization algorithms&lt;/a&gt;，同时作者也在 arXiv 上发了一篇同样内容的 &lt;a href=&quot;https://arxiv.org/abs/1609.04747&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;本文结合了两者来翻译，但是阅读原文我个人建议读博客中的，感觉体验更好点。&lt;/li&gt;
&lt;li&gt;文中括号中或者引用块中的 &lt;em&gt;斜体字&lt;/em&gt; 为对应的英文原文或者我自己注释的话（会标明 &lt;code&gt;译者注&lt;/code&gt;），否则为原文中本来就有的话。&lt;/li&gt;
&lt;li&gt;为方便阅读，我在引用序号后加了所引用论文的题目，用 &lt;em&gt;斜体字&lt;/em&gt; 表示，例如 Learning rate schedules [11，&lt;em&gt;A stochastic approximation method&lt;/em&gt;] 。&lt;/li&gt;
&lt;li&gt;水平有限，如有错误欢迎指出。翻译尽量遵循原文意思，但不意味着逐字逐句。&lt;/li&gt;
&lt;li&gt;本文也放在 &lt;a href=&quot;https://github.com/secsilm/awesome-posts&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;我 GitHub 上的 awesome-posts 项目&lt;/a&gt; 上，选取数据科学和机器学习领域内比较好的英文文章进行翻译，欢迎各位 star 。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
      <category term="Translation" scheme="https://secsilm.github.io/tags/Translation/"/>
    
  </entry>
  
  <entry>
    <title>使用集成学习提升机器学习算法性能</title>
    <link href="https://secsilm.github.io/2017/08/30/ensemble-methods/"/>
    <id>https://secsilm.github.io/2017/08/30/ensemble-methods/</id>
    <published>2017-08-30T10:47:00.000Z</published>
    <updated>2017-12-25T13:02:14.934Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>译者注：这篇文章是对 PythonWeekly 推荐的一篇讲集成模型的文章的翻译，原文为 <a href="https://blog.statsbot.co/ensemble-learning-d1dcd548e936" target="_blank" rel="noopener">Ensemble Learning to Improve Machine Learning Results</a>，由 Vadim Smolyakov 于 2017 年 8 月 22 日发表在 Medium 上，Vadim Smolyakov 是一名 MIT 的研究生。水平有限，如有错误，欢迎评论区或者邮件或者私信指出。建议阅读英文原文。</p></blockquote><a id="more"></a><p><em>集成学习（Ensemble Learning）通过联合几个模型来帮助提高机器学习结果。与单一模型相比，这种方法可以很好地提升模型的预测性能。这也是为什么集成模型在很多著名机器学习比赛中被优先使用的原因，例如 Netflix 比赛，KDD 2009 和 Kaggle。</em></p><p>集成方法是一种将几种机器学习技术组合成一个预测模型的元算法（meta-algorithm），以减小方差（bagging），偏差（boosting），或者改进预测（stacking）。</p><p>集成方法可以分为两类：</p><ul><li>序列集成方法（sequential ensemble methods），基学习器（base learner）顺序生成。序列方法的基本动机是<strong>利用基学习器之间的依赖关系</strong>。算法可以通过提高被分错样本的权重来提高性能。</li><li>并行集成方法（parallel ensemble methods），基学习器并行生成。并行方法的基本动机是<strong>利用基学习器之间的独立性</strong>，因为可以通过平均来显著降低误差。</li></ul><p>大多数集成方法使用一个基学习算法来产生多个同质基学习器（homogeneous base learners），即相同类型的学习器，这就是同质集成（homogeneous ensembles）。</p><p>也有一些方法使用的是异质学习器（heterogeneous learner），即不同类型的学习器，这就是异质集成（heterogeneous ensembles）。为了使集成方法能够比任何构成它的单独的方法更准确，基学习器必须尽可能的准确和多样。</p><hr><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging 表示的是 Bootstrap AGGregatING。降低一个估计的方差的一个方法就是平均多个估计。例如，我们可以在一个数据集的不同子集上（有放回的随机选取）训练 $M$ 个不同的树然后计算结果的平均值：</p><script type="math/tex; mode=display">f(x)=\frac{1}{M}\Sigma_{m=1}^M f_m(x)</script><p>bagging 使用<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics" target="_blank" rel="noopener">自助抽样法</a>)（bootstrapping）来为每个基学习器获得一个数据集的子集。对于如何聚合多个基学习器的结果，bagging 在分类任务中使用投票，而在回归任务重使用平均。</p><p>我们可以通过在 Iris 数据集上执行分类任务来学习 bagging。我们选择两种基学习器：决策树（decision tree）和 kNN 分类器。下图显示了基学习器在 Iris 上学习到的决策边界和他们 bagging 集成之后学习到的决策边界。</p><ul><li>决策树准确率：0.63（+/- 0.02）</li><li>kNN 准确率：0.70（+/- 0.02）</li><li>bagging 树准确率：0.64（+/- 0.01）</li><li>bagging kNN准确率：0.59（+/- 0.07）</li></ul><p><img src="https://i.imgur.com/XA3aooQ.png" width="100%" alt="bagging"></p><center><font color="gray">*Bagging*</font></center><p>决策树的边界与轴平行，而 $k=1$ 时的 kNN 算法与数据点紧密贴合。该集成方法使用了 10 个基学习器，训练子集由原训练数据和特征的 80% 构成。</p><p>决策树集成相对于 kNN 集成达到了较高的准确率。kNN 对训练样本的扰动不敏感，因此也被称为稳定学习器（stable learner）。</p><blockquote><p>稳定学习器的集成不太有利，因为这样的集成并不会提升泛化性能。</p></blockquote><p>图一也显示了集成大小是如何提高测试准确率的。基于交叉验证的结果，我们可以看到在大约 10 个基学习器前准确率一直在增加，随后趋于平缓，在 0.7 左右上下波动。因此，再增加超过 10 个基学习器不仅没有得到准确率的提升，反而增加了计算复杂度。</p><p>我们也可以看到 bagging 树集成的学习曲线。注意到训练数据的平均误差为 0.3 和 测试数据的 U 型误差曲线。训练和测试误差差距最小时发生在 Training set size in percent 为 80% 左右。</p><blockquote><p>一种常用的集成算法是随机森林。</p></blockquote><p>在随机森林算法中，每个树都是基于从原训练数据集中有放回抽样（即自助抽样法）得到的子集训练的。另外，也对特征进行自助抽样，而不是使用全部特征。</p><p>最终随机森林的偏差可能会轻微增大，但是由于平均了几个不相关的树的结果，降低了方差，导致最终模型的整体性能更好。（<em>译者注：个人觉得类似于时间序列分析中的移动平均</em>）</p><p><img src="https://i.imgur.com/iXTf6Ck.png" alt=""></p><p>在<a href="https://en.wikipedia.org/wiki/Random_forest#ExtraTrees" target="_blank" rel="noopener">极限随机树（extremely randomized tree）</a>算法中，随机性更近了一步：分裂阈值是随机选取的。与寻找最具有判别性的阈值不同，极限随机树为每个候选特征选取一个阈值，并将这些阈值的最佳值作为最终的分割阈值。这通常会降低方差，偏差会稍微增大。</p><hr><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting 指的是能够将弱学习器转化为强学习器的一系列算法。Boosting 的主要原理是给一系列的弱学习器赋予权重，这些弱学习器各自的性能仅仅比随机猜测高一些，例如小的决策树。先前被分错的样本会被给予更多权重。</p><p>在进行分类任务时，使用一个加权投票来决定最终的预测，而在回归任务时使用加权和。Boosting 和其他方法例如 bagging 的主要区别是基学习器使用加权版本的数据集来顺序生成。</p><p>下面的算法是 AdaBoost（ADAptive BOOSTing），这是目前使用最广泛的 boosting 算法：</p><blockquote><ol><li>初始化样本权重 ${w_n}$ 为 $1/N$</li><li>for $m=1$ to $M$</li><li>通过最小化加权误差函数 $J_m$ 训练一个分类器 $y_m(x)$，此处 $J_m = \Sigma_{n=1}^{N} w_n^{(m)} 1[y_m(x_n) \neq t_n]$</li><li>计算 $\epsilon_m = \dfrac{\Sigma_{n=1}^{N} w_n^{(m)}1[y_m(x_n) \neq t_n]}{\Sigma_{n=1}^{N} w_n^{(m)}}$</li><li>计算 $\alpha_m = \text{log}(\dfrac{1-\epsilon_m}{\epsilon_m})$</li><li>更新样本权重：$w_n^{m+1} = w_n^m e^{\alpha_m1[y_m(x_n) \neq t_n]}$</li><li>end for</li><li>使用最终的模型进行预测：$Y_M(x)=\text{sign}\left(\Sigma_{m=1}{M}\alpha_my_m(x)\right)$</li></ol></blockquote><p>其中 $N$ 为样本数，$M$ 为集成大小，即基学习器的数量。我们可以看到第一个基分类器 $y_1(x)$ 训练时所使用的权重均为 $1/N$。在接下来的 boosting 循环中，那些被分错的样本的权重会被增大，而那些被分对的样本的权重会减小。</p><p>$\epsilon$ 表示基分类器的加权错误率。因此，权重系数 $\alpha$ 会给予准确率更高的分类器更大的权重。</p><p><img src="https://i.imgur.com/oVZftZh.png" width="100%"></p><center><font color="gray">*AdaBoost*</font></center><p>如上图所示，每一个基学习器由一个深度为 1 的决策树组成，因此当 n_est=1 时模型只是基于一个特征阈值将样本空间分为两部分。上图也显示了集成大小是如何提高测试准确率的以及训练样本和测试样本的学习曲线。</p><p>梯度树提升（<strong>Gradient Tree Boosting</strong>）是 boosting 在任意可微分损失函数的一种推广，既可用于回归也可用于分类，同样是顺序生成基学习器。</p><script type="math/tex; mode=display">F_m(x)=F_{m-1}(x)+\gamma_mh_m(x)</script><p>每一步基于当前模型 $F_{m-1}(x)$ 来计算损失 $L$，并最小化 $L$ 来训练决策树 $h_m(x)$：</p><script type="math/tex; mode=display">F_m(x)=F_{m-1}(x)+\text{argmin}_h\Sigma_{i=1}^n L(y_i, F_{m-1}(x_i)+h(x_i))</script><p>使用该算法进行回归和分类任务时不同在于损失函数。</p><hr><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>Stacking 是一种通过元分类器（meta-classifier）或者元回归器（meta-regressor）来综合几个分类模型和回归模型的集成学习技术。基模型（base level model）基于整个数据集进行训练，然后元模型（meta-model）将基模型的输出作为特征来进行训练。( <em>译者注：个人觉得和 stacking-autoencoder 一个思想</em> )</p><p>基模型通常由不同的学习算法组成，因此 stacking 集成通常是异构的。下面的算法总结了 stacking ：</p><blockquote><ol><li>输入：训练样本 $D=\{x_i, y_i\}_{i=1}^{m}$</li><li>输出：集成分类器 $H$</li><li><em>Step 1：学习基分类器</em></li><li>for $t=1$ to $T$</li><li>基于 $D$ 训练 $h_t$</li><li>end for</li><li><em>Step 2：构建新数据</em></li><li>for $i=1$ to $m$</li><li>$D_h=\{x_i^{‘}, y_i\}$，其中 $x_i^{‘}=\{h_1(x_i), \ldots, h_T(x_i)\}$</li><li>end for</li><li><em>Step 3：学习元分类器</em></li><li>基于 $D_h$ 训练 $H$</li><li>返回 $H$</li></ol></blockquote><ul><li>KNN 准确率：0.91（+/- 0.01）</li><li>随机森林 准确率：0.91（+/- 0.06）</li><li>朴素贝叶斯 准确率：0.92（+/- 0.03）</li><li>Stacking 准确率：0.95（+/- 0.03）</li></ul><p><img src="https://i.imgur.com/vm1MTWE.png" width="100%"></p><center><font color="gray">*Stacking*</font></center><p>如上图所示，该 stacking 集成由 KNN、随机森林和朴素贝叶斯这几个基分类器构成，其预测输出再由逻辑斯底回归综合构成一个元分类器。我们可以看到 stacking 实现了决策边界的混合。上图还显示了 stacking 的准确率要高于单独的分类器，且基于学习曲线，模型没有过拟合的迹象。</p><p>Stacking 在 Kaggle 比赛中经常被用到。例如，在 Otto Group Product Classification 比赛中第一名就使用了超过 30 个模型，这些模型的输出又被作为特征来训练得到 3 个元分类器：XGBoost、神经网络和 AdaBoost。可以在 <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" target="_blank" rel="noopener">这里</a> 查看细节。</p><p><em>译者注：为了直观，我把获胜者所使用的模型结构图放在下面。</em></p><p><img src="https://i.imgur.com/J5ghO9c.png" width="100%"></p><center><font color="gray">*模型结构*</font></center><hr><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>生成本文所用图片的代码在这个 <a href="https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb" target="_blank" rel="noopener">jupyter notebook</a> 上。</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>除了本文研究的方法外，集成学习也通常用于深度学习来训练多样化和高准确率的分类器。可以通过不同的网络结构、超参数设置和训练技巧来达到多样性。</p><p>集成方法在比赛数据集中获得创纪录的性能方面非常成功，也是 Kaggle 数据科学比赛中优胜者常用的算法。</p><hr><h2 id="Recommended-reading"><a href="#Recommended-reading" class="headerlink" title="Recommended reading"></a>Recommended reading</h2><ul><li><a href="http://www2.islab.ntua.gr/attachments/article/86/Ensemble%20methods%20-%20Zhou.pdf" target="_blank" rel="noopener">Zhi-Hua Zhou, “Ensemble Methods: Foundations and Algorithms”, CRC Press, 2012</a>（ <em>译者注：原文这里没有链接，我自己找了找资源加上了资源地址，如若下载不下来请在 <a href="http://download.csdn.net/download/u010099080/9957633" target="_blank" rel="noopener">这里</a> 下载</em> ）</li><li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwj77_jo1f7VAhXCTLwKHbP4D2AQFggtMAA&amp;url=https%3A%2F%2Fpdfs.semanticscholar.org%2F453c%2F2b407c57d7512fdbe19fa1cefa08dd22614a.pdf&amp;usg=AFQjCNGaB5BchT1b-l0KWkdrMgxkeg15yA" target="_blank" rel="noopener">L. Kuncheva, “Combining Pattern Classifiers: Methods and Algorithms”, Wiley, 2004</a>（ <em>译者注：原文这里没有链接，我自己找了找资源加上了资源地址，如若下载不下来请在 <a href="http://download.csdn.net/download/u010099080/9957642" target="_blank" rel="noopener">这里</a> 下载</em> ）</li><li><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">Kaggle Ensembling Guide</a></li><li><a href="http://scikit-learn.org/stable/modules/ensemble.html" target="_blank" rel="noopener">Scikit Learn Ensemble Guide</a></li><li><a href="http://rasbt.github.io/mlxtend/" target="_blank" rel="noopener">S. Rachka, MLxtend library</a></li><li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" target="_blank" rel="noopener">Kaggle Winning Ensemble</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;译者注：这篇文章是对 PythonWeekly 推荐的一篇讲集成模型的文章的翻译，原文为 &lt;a href=&quot;https://blog.statsbot.co/ensemble-learning-d1dcd548e936&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ensemble Learning to Improve Machine Learning Results&lt;/a&gt;，由 Vadim Smolyakov 于 2017 年 8 月 22 日发表在 Medium 上，Vadim Smolyakov 是一名 MIT 的研究生。水平有限，如有错误，欢迎评论区或者邮件或者私信指出。建议阅读英文原文。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
      <category term="Translation" scheme="https://secsilm.github.io/tags/Translation/"/>
    
  </entry>
  
  <entry>
    <title>理解 TensorBoard</title>
    <link href="https://secsilm.github.io/2017/08/20/understanding-tensorboard/"/>
    <id>https://secsilm.github.io/2017/08/20/understanding-tensorboard/</id>
    <published>2017-08-20T02:01:00.000Z</published>
    <updated>2017-12-26T15:22:34.117Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="noopener">TensorBoard</a> 是用于可视化 TensorFlow 模型的训练过程的工具（the flow of tensors），在你安装 TensorFlow 的时候就已经安装了 TensorBoard。我在前面的 <a href="http://blog.csdn.net/u010099080/article/details/62882006" target="_blank" rel="noopener">【TensorFlow】TensorFlow 的卷积神经网络 CNN - TensorBoard版</a> 和 <a href="http://blog.csdn.net/u010099080/article/details/53560426" target="_blank" rel="noopener">【Python | TensorBoard】用 PCA 可视化 MNIST 手写数字识别数据集</a> 分别非常简单的介绍了一下这个工具，没有详细说明，这次来（尽可能详细的）整体说一下，而且这次也是对 <a href="http://blog.csdn.net/u010099080/article/details/62882006" target="_blank" rel="noopener">前者</a> 代码的一个升级，很大程度的改变了代码结构，将输入和训练分离开来，结构更清晰。小弟不才，如有错误，欢迎评论区指出。</p><a id="more"></a><p>全部代码和 TensorBoard 文件均在 <a href="https://github.com/secsilm/understanding-tensorboard" target="_blank" rel="noopener">我的 GitHub</a> 上，你也可以从 <a href="https://pan.baidu.com/s/1hrUGI0k" target="_blank" rel="noopener">百度网盘</a> 下载，密码 <code>t27f</code> ，只不过如果以后内容更新，网盘中的内容不会更新。</p><p><strong>Tensorboard 使用的版本为 0.1.4，对应于 TensorFlow 1.3.0，但训练代码未在 TensorFlow 1.3.0 上测试，不过应该是可以运行的。Windows 下 TensorFlow 的安装可以看 <a href="http://blog.csdn.net/u010099080/article/details/53418159" target="_blank" rel="noopener">【TensorFlow】Windows10 64位下安装TensorFlow - 官方原生支持</a></strong> 。</p><hr><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>这里我会列出对本文的更新。</p><ul><li>2017 年 8 月 22 日：TensorBoard 0.1.4 发布，更新界面截图。</li></ul><hr><h2 id="TensorBoard-是如何工作的？"><a href="#TensorBoard-是如何工作的？" class="headerlink" title="TensorBoard 是如何工作的？"></a>TensorBoard 是如何工作的？</h2><p>简单来说，TensorBoard 是通过一些操作（summary operations）将数据记录到文件（event files）中，然后再读取文件来完成作图的。想要在浏览器上看到 TensorBoard 页面，大概需要这几步：</p><ol><li><strong>summary</strong>。在定义计算图的时候，在适当的位置加上一些 <a href="https://www.tensorflow.org/api_guides/python/summary" target="_blank" rel="noopener">summary 操作</a> 。</li><li><strong>merge</strong>。你很可能加了很多 summary 操作，我们需要使用 <code>tf.summary.merge_all</code> 来将这些 summary 操作聚合成一个操作，由它来产生所有 summary 数据。</li><li><strong>run</strong>。在没有运行的时候这些操作是不会执行任何东西的，仅仅是定义了一下而已。在运行（开始训练）的时候，我们需要通过 <code>tf.summary.FileWriter()</code> 指定一个目录来告诉程序把产生的文件放到哪。然后在运行的时候使用 <code>add_summary()</code> 来将某一步的 summary 数据记录到文件中。</li></ol><p>当训练完成后，在命令行使用 <code>tensorboard --logdir=path/to/log-directory</code> 来启动 TensorBoard，按照提示在浏览器打开页面，注意把 <code>path/to/log-directory</code> 替换成你上面指定的目录。</p><hr><h2 id="OVERVIEW"><a href="#OVERVIEW" class="headerlink" title="OVERVIEW"></a>OVERVIEW</h2><p>总体上，目前 TensorBoard 主要包括下面几个面板：</p><div><img src="https://i.imgur.com/TK5BIH4.png" width="100%"></div><p>其中 <code>TEXT</code> 是 最新版（应该是 1.3）才加进去的，实验性功能，官方都没怎么介绍。除了 <code>AUDIO</code>（没用过）、<code>EMBEDDINGS</code>（还不是很熟） 和 <code>TEXT</code>（没用过） 这几个，这篇博客主要说剩下的几个，其他的等回头熟练了再来说，尽量避免误人子弟。</p><p>TensorBoard 的工作原理是读取模型训练时产生的 TensorFlow events 文件，这个文件包括了一些 summary 数据（就是作图时用的数据）。</p><hr><h2 id="SCALARS"><a href="#SCALARS" class="headerlink" title="SCALARS"></a>SCALARS</h2><div><img src="http://i.imgur.com/s9drW9Z.png" width="100%"></div><center><font color="gray">*TensorBoard 的默认打开样式*</font></center>TensorBoard 打开时默认直接进入 `SCALARS`，并且默认使用 `.*` 正则表达式显示所有图（其他面板同理，下面就不再赘述），你用到的面板会在顶部导航栏直接显示，而其他用不到的（你代码中没有相关代码）则会收起到 `INACTIVE` 中。<div align="center"><img src="http://i.imgur.com/W8ewfgz.png"></div><p><code>SCALARS</code> 面板主要用于记录诸如准确率、损失和学习率等单个值的变化趋势。在代码中用 <a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="noopener"><code>tf.summary.scalar()</code></a> 来将其记录到文件中。对应于我的代码中，我是使用其记录了训练准确率和损失。</p><p>训练准确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br></pre></td></tr></table></figure><p><div><img src="http://i.imgur.com/fpWU4rj.png" width="100%"></div></p><center><font color="gray">*全部 run 的 acuracy*</font></center><blockquote><p>可以看到这些曲线并不是那么平滑，这是因为我记录的步数比较少，也就是点比较少，如果每一步都记录或者间隔比较短，那么最后的文件会很大。下同</p></blockquote><p>损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) + \</span><br><span class="line">        BETA * tf.add_n([tf.nn.l2_loss(v)</span><br><span class="line">                        <span class="keyword">for</span> v <span class="keyword">in</span> trainable_vars <span class="keyword">if</span> <span class="keyword">not</span> <span class="string">'b'</span> <span class="keyword">in</span> v.name])</span><br><span class="line"></span><br><span class="line">    tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br></pre></td></tr></table></figure><p><div><img src="http://i.imgur.com/zLPpuJt.png" width="100%" alt="loss"></div></p><center><font color="gray">*全部 run 的 loss*</font></center><p>每个图的右下角都有 3 个小图标，第一个是查看大图，第二个是是否对 y 轴对数化，第三个是如果你拖动或者缩放了坐标轴，再重新回到原始位置。</p><p><img src="http://img.blog.csdn.net/20170822104513317?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="100%" alt="fit-domain-to-data"></p><center><font color="gray">*Fit domain to data*</font></center><p><code>tf.summary.scalar(name, tensor)</code> 有两个参数：</p><ul><li><code>name</code>：可以理解为图的标题。在 <code>GRAPHS</code> 中则是该节点的名字</li><li><code>tensor</code>：包含单个值的 tensor，说白了就是作图的时候要用的数据</li></ul><p>在上面的图中，可以看到除了 <code>accuracy</code> 和 <code>loss</code> 外，还有一个 <code>eval_accuracy</code>，这个是我用来记录验证准确率的，代码中相关的部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eval_writer = tf.summary.FileWriter(LOGDIR + <span class="string">'/eval'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some other codes</span></span><br><span class="line"></span><br><span class="line">eval_writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=<span class="string">'eval_accuracy'</span>, simple_value=np.mean(test_acc))]), i)</span><br></pre></td></tr></table></figure><p><div><img src="http://i.imgur.com/XxQ1oRW.png" width="100%" alt="eval-accuracy"></div></p><center><font color="gray">*全部 run 的 eval_accuracy*</font></center><p>这里我是手动添加了一个验证准确率到 <code>SCALARS</code> 中，其实想要记录验证准确率完全不必这么做，和训练准确率不同的只是 feed 的数据不一样而已。然而由于我的显存不够一次装下整个验证集，所以我就分了两部分计算然后求平均值来得到整个验证集的准确率。<em>如果谁有更好的方法，请给我发邮件或者在评论区评论，非常感谢 :-)</em></p><p>当我们有很多的 tag （图）的时候，我们可以在左上角写正则表达式来选择其中一些 tag，比如我想选择包括 <code>accuracy</code> 的 tag，那么我直接写上 <code>accuracy</code> 就可以了，右侧就会多出一个 <code>accuracy</code> 的 tag，显示匹配出的结果。</p><p>页面左上是 <code>Show data download links</code> 和 <code>Ignore outliers in chart scaling</code>，这两个比较好理解，第一个就是显示数据下载链接，可以把 TensorBoard 作图用的数据下载下来，点击后可以在图的右下角可以看到下载链接以及选择下载哪一个 run 的，下载格式支持 CSV 和 JSON。第二个是排除异常点，默认选中。</p><p>当我们用鼠标在图上滑过的时候可以显示出每个 run 对应的点的值，这个显示顺序是由 <code>Tooltip sorting method</code> 来控制的，有 <code>default</code>、<code>descending</code>（降序）、<code>asceding</code> （升序）和 <code>nearest</code> 四个选项，大家可以试试点几下。</p><p>而下面的 <code>Smoothing</code> 指的是作图时曲线的平滑程度，使用的是<strong>类似</strong>指数平滑的处理方法。如果不平滑处理的话，有些曲线波动很大，难以看出趋势。0 就是不平滑处理，1 就是最平滑，默认是 0.6。</p><p><code>Horizontal Axis</code> 顾名思义指的是横轴的设置：</p><ul><li><code>STEP</code>：默认选项，指的是横轴显示的是训练迭代次数</li><li><code>RELATIVE</code>：这个相对指的是相对时间，相对于训练开始的时间，也就是说是<em>训练用时</em> ，单位是小时</li><li><code>WALL</code>：指训练的绝对时间</li></ul><p>最下面的 <code>Runs</code> 列出了各个 run，你可以选择只显示某一个或某几个。</p><hr><h2 id="IMAGES"><a href="#IMAGES" class="headerlink" title="IMAGES"></a>IMAGES</h2><p>如果你的模型输入是图像（的像素值），然后你想看看模型每次的输入图像是什么样的，以保证每次输入的图像没有问题（因为你可能在模型中对图像做了某种变换，而这种变换是很容易出问题的），<code>IMAGES</code> 面板就是干这个的，它可以显示出相应的输入图像，默认显示最新的输入图像，如下图：</p><p><div><img src="http://i.imgur.com/nqVkbBo.png" width="100%" alt="images"></div></p><p><center><font color="gray"><em>第 45000 次迭代时输入的 3 个图像</em></font><center></center></center></p><p>图的右下角的两个图标，第一个是查看大图，第二个是查看原图（真实大小，默认显示的是放大后的）。左侧和 <code>SCALARS</code> 差不多，我就不赘述了。</p><p>而在代码中，需要在合适的位置使用 <a href="https://www.tensorflow.org/api_docs/python/tf/summary/image" target="_blank" rel="noopener"><code>tf.summary.image()</code></a> 来把图像记录到文件中，其参数和 <code>tf.summary.scalar()</code> 大致相同，多了一个 <code>max_outputs</code> ，指的是最多显示多少张图片，默认为 3。对应于我的代码，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, N_FEATURES], name=<span class="string">'x'</span>)</span><br><span class="line">x_image = tf.transpose(tf.reshape(x, [<span class="number">-1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]), perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">tf.summary.image(<span class="string">'input'</span>, x_image, max_outputs=<span class="number">3</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, N_CLASSES], name=<span class="string">'labels'</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="GRAPHS"><a href="#GRAPHS" class="headerlink" title="GRAPHS"></a>GRAPHS</h2><p>这个应该是最常用的面板了。很多时候我们的模型很复杂，包含很多层，我们想要总体上看下构建的网络到底是什么样的，这时候就用到 <code>GRAPHS</code> 面板了，在这里可以展示出你所构建的网络整体结构，显示数据流的方向和大小，也可以显示训练时每个节点的用时、耗费的内存大小以及参数多少。默认显示的图分为两部分：主图（Main Graph）和辅助节点（Auxiliary Nodes）。其中主图显示的就是网络结构，辅助节点则显示的是初始化、训练、保存等节点。我们可以双击某个节点或者点击节点右上角的 <code>+</code> 来展开查看里面的情况，也可以对齐进行缩放，每个节点的命名都是我们在代码中使用 <code>tf.name_scope()</code> 定义好的。下面介绍下该面板左侧的功能。</p><p><div><img src="http://i.imgur.com/r8mXplM.png" width="100%" alt="graph"></div></p><p><center><font color="gray"><em>计算图</em></font><center></center></center></p><p>左上是 <code>Fit to screen</code>，顾名思义就是将图缩放到适合屏幕。下面的 <code>Download PNG</code> 则是将图保存到本地。<code>Run</code> 和 <code>Session Run</code> 分别是不同的训练和迭代步数。比如我这里以不同的超参训练了 6 次，那么 就有 6 个 run，而你所记录的迭代次数（并不是每一步都会记录当前状态的，那样的话太多了，一般都是每隔多少次记录一次）则显示在 <code>Session Run</code> 里。再下面大家应该都能看懂，我就不详细说每个功能的意思了。</p><p><div align="center"><img src="https://i.imgur.com/sJFNrmY.png" alt="run"></div></p><p><center><font color="gray"><em>选择迭代步数</em></font><center></center></center></p><p>TensorBoard 默认是不会记录每个节点的用时、耗费的内存大小等这些信息的，那么如何才能在图上显示这些信息呢？关键就是如下这些代码，主要就是在 <code>sess.run()</code> 中加入 <code>options</code> 和 <code>run_metadata</code> 参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line">s, lss, acc , _ = sess.run([merged_summary, loss, accuracy, train_step], </span><br><span class="line">                           feed_dict=&#123;x: batch_x, y: batch_y, phase: <span class="number">1</span>&#125;,</span><br><span class="line">                           options=run_options,</span><br><span class="line">                           run_metadata=run_metadata)</span><br><span class="line">summary_writer.add_run_metadata(run_metadata, <span class="string">'step&#123;&#125;'</span>.format(i))</span><br><span class="line">summary_writer.add_summary(s, i)</span><br></pre></td></tr></table></figure><p>然后我们就可以选择 <code>Compute time</code> 或者 <code>Memory</code> 来查看相应信息，颜色深浅代表耗时多少或者内存耗用多少。</p><p><div><img src="https://i.imgur.com/TIMp4tq.png" width="100%" alt="compute-time-memory"></div></p><p><center><font color="gray"><em>计算耗时</em></font><center></center></center></p><p>我们也可以将某个节点从主图移除，将其放到辅助节点中，以便于我们更清晰的观察整个网络。具体操作是 <em>右键该节点，选择 <code>Remove from main graph</code></em> 。</p><hr><h2 id="DISTRIBUTIONS"><a href="#DISTRIBUTIONS" class="headerlink" title="DISTRIBUTIONS"></a>DISTRIBUTIONS</h2><p><code>DISTRIBUTIONS</code> 主要用来展示网络中各参数随训练步数的增加的变化情况，可以说是 <em>多分位数折线图</em> 的堆叠。下面我就下面这张图来解释下。</p><p><div><img src="http://i.imgur.com/ztasEZc.png" width="100%" alt="conv1-weights"></div></p><p><center><font color="gray"><em>权重分布</em></font><center></center></center></p><p>这张图表示的是第二个卷积层的权重变化。横轴表示训练步数，纵轴表示权重值。而从上到下的折现分别表示权重分布的不同分位数：<code>[maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, minimum]</code>。对应于我的代码，部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(name):</span><br><span class="line">    W = tf.Variable(tf.truncated_normal(</span><br><span class="line">        [k, k, channels_in, channels_out], stddev=<span class="number">0.1</span>), name=<span class="string">'W'</span>)</span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[channels_out]), name=<span class="string">'b'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(inpt, W, strides=[<span class="number">1</span>, s, s, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    act = tf.nn.relu(conv)</span><br><span class="line">    tf.summary.histogram(<span class="string">'weights'</span>, W)</span><br><span class="line">    tf.summary.histogram(<span class="string">'biases'</span>, b)</span><br><span class="line">    tf.summary.histogram(<span class="string">'activations'</span>, act)</span><br></pre></td></tr></table></figure><hr><h2 id="HISTOGRAMS"><a href="#HISTOGRAMS" class="headerlink" title="HISTOGRAMS"></a>HISTOGRAMS</h2><p><strong><code>HISTOGRAMS</code> 和 <code>DISTRIBUTIONS</code> 是对同一数据不同方式的展现</strong>。与 <code>DISTRIBUTIONS</code> 不同的是，<code>HISTOGRAMS</code> 可以说是 <em>频数分布直方图</em> 的堆叠。</p><p><div><img src="http://i.imgur.com/efGIjU9.png" width="100%" alt="conv1-weights-hist"></div></p><p><center><font color="gray"><em>权重分布</em></font><center></center></center></p><p>横轴表示权重值，纵轴表示训练步数。颜色越深表示时间越早，越浅表示时间越晚（越接近训练结束）。除此之外，<code>HISTOGRAMS</code> 还有个 <code>Histogram mode</code>，有两个选项：<code>OVERLAY</code> 和 <code>OFFSET</code>。选择 <code>OVERLAY</code> 时横轴为权重值，纵轴为频数，每一条折线为训练步数。颜色深浅与上面同理。默认为 <code>OFFSET</code> 模式。</p><hr><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>这篇博文写了好久，从准备数据到开始动笔写，中间一直被各种事干扰。由于我水平有限，我只能尽最大程度的给出尽可能正确的解释，然而还有很多我目前还兼顾不到，很多话也不是很通顺。如有错误，欢迎在评论区或者给我私信或者给我邮件指出。</p><hr><h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><ul><li><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="noopener">TensorBoard: Visualizing Learning</a></li><li><a href="https://stats.stackexchange.com/questions/220491/how-does-one-interpret-histograms-given-by-tensorflow-in-tensorboard" target="_blank" rel="noopener">How does one interpret histograms given by TensorFlow in TensorBoard?</a></li><li><a href="https://github.com/tensorflow/tensorboard/blob/master/README.md" target="_blank" rel="noopener">TensorBoard</a></li><li><a href="https://www.tensorflow.org/get_started/tensorboard_histograms" target="_blank" rel="noopener">TensorBoard Histogram Dashboard</a></li><li><a href="https://stackoverflow.com/questions/42315202/understanding-tensorboard-weight-histograms" target="_blank" rel="noopener">Understanding TensorBoard (weight) histograms</a></li><li><a href="https://www.youtube.com/watch?v=eBbEDRsCmv4&amp;t=1105s" target="_blank" rel="noopener">Hands-on TensorBoard (TensorFlow Dev Summit 2017)</a></li></ul><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/get_started/summaries_and_tensorboard&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorBoard&lt;/a&gt; 是用于可视化 TensorFlow 模型的训练过程的工具（the flow of tensors），在你安装 TensorFlow 的时候就已经安装了 TensorBoard。我在前面的 &lt;a href=&quot;http://blog.csdn.net/u010099080/article/details/62882006&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;【TensorFlow】TensorFlow 的卷积神经网络 CNN - TensorBoard版&lt;/a&gt; 和 &lt;a href=&quot;http://blog.csdn.net/u010099080/article/details/53560426&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;【Python | TensorBoard】用 PCA 可视化 MNIST 手写数字识别数据集&lt;/a&gt; 分别非常简单的介绍了一下这个工具，没有详细说明，这次来（尽可能详细的）整体说一下，而且这次也是对 &lt;a href=&quot;http://blog.csdn.net/u010099080/article/details/62882006&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;前者&lt;/a&gt; 代码的一个升级，很大程度的改变了代码结构，将输入和训练分离开来，结构更清晰。小弟不才，如有错误，欢迎评论区指出。&lt;/p&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://secsilm.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Numpy 中的 shuffle VS permutation</title>
    <link href="https://secsilm.github.io/2017/06/17/numpy-shuffle-permutation/"/>
    <id>https://secsilm.github.io/2017/06/17/numpy-shuffle-permutation/</id>
    <published>2017-06-17T15:28:00.000Z</published>
    <updated>2017-12-23T00:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>有时候我们会有随机打乱一个数组的需求，例如训练时随机打乱样本，我们可以使用 <code>numpy.random.shuffle()</code> 或者 <code>numpy.random.permutation()</code> 来完成。这两者非常相似，实现的功能是一样的，那么他们到底有什么区别？</p><a id="more"></a><p>本文代码及图片可以在 <a href="https://github.com/secsilm/shuffle-vs-permutation-numpy" target="_blank" rel="noopener">我的GitHub</a> 找到。</p><hr><h2 id="参数区别"><a href="#参数区别" class="headerlink" title="参数区别"></a>参数区别</h2><p>以下 <code>numpy.random.shuffle()</code> 简称 <code>shuffle</code>，<code>numpy.random.permutation()</code> 简称 <code>permutation</code>。</p><ul><li><code>shuffle</code> 的参数只能是 <code>array_like</code>，而 <code>permutation</code> 除了 <code>array_like</code> 还可以是 <code>int</code> 类型，如果是 <code>int</code> 类型，那就随机打乱 <code>numpy.arange(int)</code>。</li><li><strong><code>shuffle</code> 返回 <code>None</code></strong>，这点尤其要注意，也就是说没有返回值，而 <code>permutation</code> 则返回打乱后的 array。</li></ul><hr><h2 id="实现区别"><a href="#实现区别" class="headerlink" title="实现区别"></a>实现区别</h2><p><code>permutation</code> 其实在内部实现也是调用的 <code>shuffle</code>，这点从 <a href="https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/mtrand.pyx#L4847" target="_blank" rel="noopener">Numpy 的源码</a> 可以看出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">permutation</span><span class="params">(self, object x)</span>:</span></span><br><span class="line">    <span class="string">'''这里都是帮助文档，我就省略了'''</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(x, (int, long, np.integer)):</span><br><span class="line">        arr = np.arange(x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        arr = np.array(x)</span><br><span class="line">    self.shuffle(arr)</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><hr><h2 id="速度区别"><a href="#速度区别" class="headerlink" title="速度区别"></a>速度区别</h2><p>为了测试两者的速度区别，我分别使用了 <code>shuffle</code> 和 <code>permutation</code> 对不同长度的 array 进行随机打乱并计时。</p><p>关键代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">10</span> ** np.arange(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">shuffle_elapsed = []</span><br><span class="line">permutation_elapsed = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> n:</span><br><span class="line">    print(i)</span><br><span class="line">    start = time.time()</span><br><span class="line">    a = np.arange(i)</span><br><span class="line">    np.random.shuffle(a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    shuffle_elapsed.append((i, end - start))</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    b = np.random.permutation(i)</span><br><span class="line">    end = time.time()</span><br><span class="line">    permutation_elapsed.append((i, end - start))</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="http://img.blog.csdn.net/20170618130741322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><em>右键在新标签页打开查看大图</em></p><p>可以看出在达到 $10^9$ 级别以前，两者速度几乎没有差别，但是在 达到 $10^9$ 以后两者速度差距明显拉大，<code>shuffle</code> 的用时明显短于 <code>permutation</code>。</p><p>所以在 array 很大的时候还是使用 <code>shuffle</code> 速度更快些，但要注意其不返回打乱后的 array，是 inplace 修改。</p><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有时候我们会有随机打乱一个数组的需求，例如训练时随机打乱样本，我们可以使用 &lt;code&gt;numpy.random.shuffle()&lt;/code&gt; 或者 &lt;code&gt;numpy.random.permutation()&lt;/code&gt; 来完成。这两者非常相似，实现的功能是一样的，那么他们到底有什么区别？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://secsilm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow DNNRegressor 的简单使用</title>
    <link href="https://secsilm.github.io/2017/06/02/tensorflow-DNNRegressor/"/>
    <id>https://secsilm.github.io/2017/06/02/tensorflow-DNNRegressor/</id>
    <published>2017-06-02T14:03:00.000Z</published>
    <updated>2017-12-26T02:07:47.750Z</updated>
    
    <content type="html"><![CDATA[<p><code>tf.contrib.learn.DNNRegressor</code> 是 TensoFlow 中实现的一个神经网络回归器。一般神经网络用于分类问题的比较多，但是同样可以用于回归问题和无监督学习问题。<br><a id="more"></a><br>此文的代码和所生成的 TensorBoard 文件可以从 <a href="https://github.com/secsilm/TensorFlow-DNNRegressor" target="_blank" rel="noopener">secsilm/TensorFlow-DNNRegressor: 文章 TensorFlow DNNRegressor 的简单使用 的代码及 TensorBoard 文件。</a> 下载。</p><hr><h2 id="tf-contrib-learn"><a href="#tf-contrib-learn" class="headerlink" title="tf.contrib.learn"></a><code>tf.contrib.learn</code></h2><p><code>tf.contrib.learn</code> 是 TensorFlow 提供的一个机器学习高级 API 模块，让用户可以更方便的配置、训练和评估各种各样的机器学习模型，里面内置了很多模型可以直接调用，使用类似 <a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit-learn</a> 的 API 。大致有以下几种模型，具体可参考 <a href="https://www.tensorflow.org/api_guides/python/contrib.learn" target="_blank" rel="noopener">Learn (contrib)</a>：</p><ul><li>K-means 聚类 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/KMeansClustering" target="_blank" rel="noopener"><code>tf.contrib.learn.KMeansClustering</code></a></li><li>神经网络分类器 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="noopener"><code>tf.contrib.learn.DNNClassifier</code></a></li><li>神经网络回归器 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor" target="_blank" rel="noopener"><code>tf.contrib.learn.DNNRegressor</code></a></li><li>广度深度回归 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNLinearCombinedRegressor" target="_blank" rel="noopener"><code>tf.contrib.learn.DNNLinearCombinedRegressor</code></a></li><li>广度深度分类 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNLinearCombinedClassifier" target="_blank" rel="noopener"><code>tf.contrib.learn.DNNLinearCombinedClassifier</code></a></li><li>线性分类<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier" target="_blank" rel="noopener"><code>tf.contrib.learn.LinearClassifier</code></a></li><li>线性回归 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearRegressor" target="_blank" rel="noopener"><code>tf.contrib.learn.LinearRegressor</code></a></li><li>逻辑斯谛回归 <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LogisticRegressor" target="_blank" rel="noopener"><code>tf.contrib.learn.LogisticRegressor</code></a></li></ul><hr><h2 id="DNNRegressor"><a href="#DNNRegressor" class="headerlink" title="DNNRegressor"></a>DNNRegressor</h2><p>下面以波士顿房价预测的例子来说明一下 DNNRegressor 的使用。</p><p>波士顿房价数据集大小为 506*14，也就是说有 506 个样本，每个样本有 13 个特征，另外一个是要预测的房价。数据集我们直接使用 scikit-learn 的 <code>load_boston()</code> 函数直接载入，这里引用下UCI 的解释：</p><blockquote><ol><li>CRIM      per capita crime rate by town</li><li>ZN        proportion of residential land zoned for lots over 25,000 sq.ft.</li><li>INDUS     proportion of non-retail business acres per town</li><li>CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li><li>NOX       nitric oxides concentration (parts per 10 million)</li><li>RM        average number of rooms per dwelling</li><li>AGE       proportion of owner-occupied units built prior to 1940</li><li>DIS       weighted distances to five Boston employment centres</li><li>RAD       index of accessibility to radial highways</li><li>TAX      full-value property-tax rate per $10,000</li><li>PTRATIO  pupil-teacher ratio by town</li><li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li><li>LSTAT    % lower status of the population</li><li>MEDV     Median value of owner-occupied homes in $1000’s</li></ol></blockquote><p>开始前先说明一下，DNNRegressor 的使用逻辑和其他库的不太一样，如果你比较熟悉 TensorFlow 的话就比较好理解：我们是先定义一些计算图，这时候并不真正的传入数据，然后在训练的时候去执行这个计算图，也就是说这时候才开始将真正的数据穿进去。相当于是组装一条产品的生产线，刚开始我们只是把这台生产线组装好，并不开始真正的生产产品（保有数据），到最后阶段（训练阶段）才会真正的把原材料「喂给」生产线上来生产产品。</p><p>下面的程序大致上是这么几个步骤：</p><ol><li>载入数据</li><li>定义 <code>FeatureColumn</code></li><li>定义 regressor</li><li>训练</li><li>评估</li><li>预测</li></ol><h3 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h3><p>如上所说，这里我们使用 scikit-learn 的 <code>load_boston()</code> 函数来载入数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">boston = load_boston()</span><br><span class="line">boston_df = pd.DataFrame(np.c_[boston.data, boston.target], columns=np.append(boston.feature_names, <span class="string">'MEDV'</span>))</span><br><span class="line">LABEL_COLUMN = [<span class="string">'MEDV'</span>]</span><br><span class="line">FEATURE_COLUMNS = [f <span class="keyword">for</span> f <span class="keyword">in</span> boston_df <span class="keyword">if</span> <span class="keyword">not</span> f <span class="keyword">in</span> LABEL_COLUMN]</span><br></pre></td></tr></table></figure><p><code>boston</code> 是一个 numpy 多维数组，为了后面方便转换成 pandas 的 DataFrame 类型，<code>MEDV</code> 就是我们要预测的变量。</p><p>然后我们使用 <code>train_test_split()</code> 来按照 7:3 的比例来分割数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(boston_df[FEATURE_COLUMNS], boston_df[LABEL_COLUMN], test_size=<span class="number">0.3</span>)</span><br><span class="line">print(<span class="string">'训练集：&#123;&#125;\n测试集：&#123;&#125;'</span>.format(X_train.shape, X_test.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集：(354, 13)</span><br><span class="line">测试集：(152, 13)</span><br></pre></td></tr></table></figure><p>在进行下一步之前，我们先来简单的看下这个数据集的分布情况。</p><p>各个变量的 violin 图，从这些图我们可以看出每个变量的分布情况：</p><p><img src="http://img.blog.csdn.net/20170602195143390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="violinplot"></p><p>14 个变量之间的相关情况：</p><p><img src="http://img.blog.csdn.net/20170602200737942?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="corrplot"><br><em>右键在新标签页打开可以查看大图</em></p><p>从这个图我们可以看出 RM （住宅平均房间数）与最终的售价是最正相关的，而 LSTAT （人口中地位低下者的比例）是最负相关的。</p><h3 id="定义-FeatureColumn"><a href="#定义-FeatureColumn" class="headerlink" title="定义 FeatureColumn"></a>定义 <code>FeatureColumn</code></h3><p>TensorFlow 使用 <a href="https://www.tensorflow.org/tutorials/linear#feature_columns_and_transformations" target="_blank" rel="noopener"><code>FeatureColumn</code></a> 来表示数据集中的一个的特征，我们需要根据特征类型（连续或者分类）把原来的特征都转换成 <code>FeatureColumn</code>。此处数据已经都是连续数值了，所以直接使用 <code>tf.contrib.layers.real_valued_column()</code> 来转换成 <code>FeatureColumn</code>，如果是分类变量，则需要使用 <code>tf.contrib.layers.sparse_column_with_keys()</code> 或者 <code>tf.contrib.layers.sparse_column_with_hash_bucket()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feature_cols = [tf.contrib.layers.real_valued_column(k) <span class="keyword">for</span> k <span class="keyword">in</span> FEATURE_COLUMNS]</span><br></pre></td></tr></table></figure><h3 id="定义-regressor"><a href="#定义-regressor" class="headerlink" title="定义 regressor"></a>定义 regressor</h3><p>这里就到定义模型的时候了，也就是 DNNRegressor，将我们前面定义的 <code>FeatureColumn</code> 传进去，再设置下隐藏层的层数和神经元数量，指定模型保存目录（训练完成后可以使用 tensorboard 可视化网络结构和训练过程）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, </span><br><span class="line">                                          hidden_units=[<span class="number">64</span>, <span class="number">128</span>], </span><br><span class="line">                                          model_dir=<span class="string">'./models/dnnregressor'</span>)</span><br></pre></td></tr></table></figure><h3 id="定义-input-fn"><a href="#定义-input-fn" class="headerlink" title="定义 input_fn"></a>定义 <code>input_fn</code></h3><p>定义完 regressor 之后就是模型的训练了，但是在训练开始前，我们需要定义一下传给模型训练的数据的格式，这就是 <a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="noopener"><code>input_fn</code></a> 的作用。<code>input_fn</code> 必须返回 <code>feature_cols</code> 和 <code>labels</code>。</p><ul><li><code>feature_cols</code> 是一个字典， key 是特征的名字，value 是对应的数据（Tensor 格式）</li><li><code>labels</code> 是一个 Tensor，就是你要预测的变量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(df, label)</span>:</span></span><br><span class="line">    feature_cols = &#123;k: tf.constant(df[k].values) <span class="keyword">for</span> k <span class="keyword">in</span> FEATURE_COLUMNS&#125;</span><br><span class="line">    label = tf.constant(label.values)</span><br><span class="line">    <span class="keyword">return</span> feature_cols, label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''训练阶段使用的 input_fn'''</span></span><br><span class="line">    <span class="keyword">return</span> input_fn(X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''测试阶段使用的 input_fn'''</span></span><br><span class="line">    <span class="keyword">return</span> input_fn(X_test, y_test)</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>定义完 <code>input_fn</code> 我们就可以开始真正的训练过程了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regressor.fit(input_fn=train_input_fn, steps=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><p>训练阶段和测试阶段所用的数据是不一样的，所以在上面我又分别为训练和测试写了一个 <code>input_fn</code>。当然除了这种方式还有其他方式，你可以只写一个 <code>input_fn</code>，然后在 <code>fit</code> 的时候使用类似 <code>input_fn=lambda: input_fn(training_set)</code> 来制定传入的数据集。注意你不能直接使用 <code>input_fn=input_fn(training_set)</code> ，因为 <code>input_fn</code> 参数的值是一个<strong>函数</strong>。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ev = regressor.evaluate(input_fn=test_input_fn, steps=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'ev: &#123;&#125;'</span>.format(ev))</span><br></pre></td></tr></table></figure><p>测试结果是一个字典，包括最终损失和迭代步数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ev: &#123;&apos;loss&apos;: 32.229404, &apos;global_step&apos;: 5000&#125;</span><br></pre></td></tr></table></figure><p>我们从 TensorBoard 可以看到最终的训练损失和测试损失：</p><p><img src="http://img.blog.csdn.net/20170602215657871?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="loss"><br><em>右键在新标签页打开可以查看大图</em></p><p>整个计算图是这样的：</p><p><img src="http://img.blog.csdn.net/20170602215843609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="graph"><br><em>右键在新标签页打开可以查看大图</em></p><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>使用训练好的模型对测试数据进行预测，只输出前 10 项预测结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict = regressor.predict(input_fn=test_input_fn, as_iterable=<span class="keyword">False</span>)</span><br><span class="line">print(predict[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ <span class="number">25.04570198</span>  <span class="number">31.65488815</span>  <span class="number">11.31779861</span>  <span class="number">14.20497131</span>  <span class="number">31.12402344</span></span><br><span class="line">  <span class="number">37.50260925</span>  <span class="number">25.20910835</span>  <span class="number">24.17683983</span>  <span class="number">20.83440208</span>  <span class="number">35.22043991</span>]</span><br></pre></td></tr></table></figure><hr><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><h3 id="InternalError-see-above-for-traceback-Blas-SGEMM-launch-failed"><a href="#InternalError-see-above-for-traceback-Blas-SGEMM-launch-failed" class="headerlink" title="InternalError (see above for traceback): Blas SGEMM launch failed"></a><code>InternalError (see above for traceback): Blas SGEMM launch failed</code></h3><p>如果你的程序报类似这样的错，说明你在使用 GPU 计算（默认行为）且你的 GPU 可用显存不足，TensorFlow 总是试图为自己分配全部显存，例如你的显存是 2GB，那么他就会试图为自己分配 2GB，但是一般情况下你的显存不会一点都不被其他程序占用的，导致 TensorFlow 分配显存失败。</p><p><strong>解决办法</strong>是在定义 regressor 的时候使用 <code>config</code> 参数中的 <code>gpu_memory_fraction</code> 来指定分配给 TensorFlow 的显存大小（比例）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># log_device_placement 用于记录并输出使用的设备，可以不用写</span></span><br><span class="line">config = tf.contrib.learn.RunConfig(gpu_memory_fraction=<span class="number">0.3</span>, log_device_placement=<span class="keyword">True</span>)</span><br><span class="line">regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols, </span><br><span class="line">                                          hidden_units=[<span class="number">64</span>, <span class="number">128</span>], </span><br><span class="line">                                          model_dir=<span class="string">'./models/dnnregressor'</span>, </span><br><span class="line">                                          config=config)</span><br></pre></td></tr></table></figure><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;tf.contrib.learn.DNNRegressor&lt;/code&gt; 是 TensoFlow 中实现的一个神经网络回归器。一般神经网络用于分类问题的比较多，但是同样可以用于回归问题和无监督学习问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://secsilm.github.io/tags/TensorFlow/"/>
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost 在 Windows 10 和 Ubuntu 上的安装</title>
    <link href="https://secsilm.github.io/2017/05/18/installing-xgboost/"/>
    <id>https://secsilm.github.io/2017/05/18/installing-xgboost/</id>
    <published>2017-05-18T02:58:00.000Z</published>
    <updated>2017-12-23T00:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>关于什么是 <code>XGBoost</code>，我在这里不再解释，如果有时间的话再写一篇文章来解释，在数据科学里非常有用。大家可以参考 Tianqi Chen 的论文 <a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a> ，本篇文章只是在 Windows 10 和 Ubuntu 上安装 <code>XGBoost</code> 的 Python 包的方法，Ubuntu 版本为 16.04 LTS，Python 版本均为 3.5，Anaconda。</p><a id="more"></a><hr><h2 id="Windows-10"><a href="#Windows-10" class="headerlink" title="Windows 10"></a>Windows 10</h2><h3 id="官网"><a href="#官网" class="headerlink" title="官网"></a>官网</h3><p><a href="https://xgboost.readthedocs.io/en/latest/build.html" target="_blank" rel="noopener">官网</a> 介绍的方法是从 <a href="https://github.com/dmlc/xgboost/" target="_blank" rel="noopener">GitHub</a> 编译安装，但是在我的机器上没有成功，<code>pip</code> 同样不成功。有兴趣的话可以参考官网和 2016 年的一篇文章 <a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=zh" target="_blank" rel="noopener"><br>Installing XGBoost For Anaconda on Windows</a> 来试试。</p><h3 id="Unofficial-Windows-Binaries-for-Python-Extension-Packages"><a href="#Unofficial-Windows-Binaries-for-Python-Extension-Packages" class="headerlink" title="Unofficial Windows Binaries for Python Extension Packages"></a>Unofficial Windows Binaries for Python Extension Packages</h3><p><a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">这个</a> 是加州大学提供的一个非官方包列表，提供了很多 Python 包的 <code>whl</code> 文件下载，<code>XGBoost</code> 的在 <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost" target="_blank" rel="noopener">这里</a>，我自己下好了一份放在里 <a href="http://download.csdn.net/download/u010099080/9845455" target="_blank" rel="noopener">这里</a>。根据你的环境下载对应的包，比如我的是 Python 3.5，Windows 10 64 位，那就选下面红框这个：</p><p><img src="http://img.blog.csdn.net/20170518105305496?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"> </p><p>然后再命令行中使用下面的语句安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xgboost-0.6-cp35-cp35m-win_amd64.whl</span><br></pre></td></tr></table></figure><p>安装完成后使用 <code>import xgboost</code> 导入即可。导入完成后可能会有警告提示：</p><p><img src="http://img.blog.csdn.net/20170518105542912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>不过暂时不影响，这个问题已经在 GitHub 版本修复了，但是这个 <code>whl</code> 文件并不是最新的。</p><hr><h2 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h2><h3 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h3><p>我在 Ubuntu 上使用 pip 方式安装成功，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ pip install xgboost</span><br><span class="line">Collecting xgboost</span><br><span class="line">  Downloading xgboost-0.6a2.tar.gz (1.2MB)</span><br><span class="line">    100% |████████████████████████████████| 1.2MB 1.0MB/s </span><br><span class="line">Requirement already satisfied: numpy <span class="keyword">in</span> /home/alan/.<span class="built_in">local</span>/lib/python3.5/site-packages (from xgboost)</span><br><span class="line">Requirement already satisfied: scipy <span class="keyword">in</span> /home/alan/anaconda3/lib/python3.5/site-packages (from xgboost)</span><br><span class="line">Requirement already satisfied: scikit-learn <span class="keyword">in</span> /home/alan/anaconda3/lib/python3.5/site-packages (from xgboost)</span><br><span class="line">Building wheels <span class="keyword">for</span> collected packages: xgboost</span><br><span class="line">Running setup.py bdist_wheel <span class="keyword">for</span> xgboost ... <span class="keyword">done</span></span><br><span class="line">Stored <span class="keyword">in</span> directory: /home/alan/.cache/pip/wheels/5e/c1/d6/522af54e5cc001fad4dd855117f8bf61b11d56443e06672e26</span><br><span class="line">Successfully built xgboost</span><br><span class="line">Installing collected packages: xgboost</span><br><span class="line">Successfully installed xgboost-0.6a2</span><br></pre></td></tr></table></figure><p>安装后的版本为 <code>0.6a2</code>。</p><h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><p>同样参照 <a href="http://xgboost.readthedocs.io/en/latest/build.html#building-on-ubuntu-debian" target="_blank" rel="noopener">官网</a> 的安装方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recursive https://github.com/dmlc/xgboost</span><br><span class="line"><span class="built_in">cd</span> xgboost</span><br><span class="line">make -j4</span><br></pre></td></tr></table></figure><p>然后再进入 <code>python-package</code> 目录，使用 <code>sudo python setup.py install</code> 安装 Python 包。</p><hr><h2 id="Issues"><a href="#Issues" class="headerlink" title="Issues"></a>Issues</h2><h3 id="1-OSError-version-39-GLIBCXX-3-4-20-39-not-found"><a href="#1-OSError-version-39-GLIBCXX-3-4-20-39-not-found" class="headerlink" title="#1 OSError: version &#39;GLIBCXX_3.4.20&#39; not found"></a>#1 <code>OSError: version &#39;GLIBCXX_3.4.20&#39; not found</code></h3><p>Ubuntu 下，在安装完成后用 <code>import xgboost</code> 导入的时候你可能遇到这样的问题：</p><p><img src="http://img.blog.csdn.net/20170522102222313?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><em>右键在新标签页打开查看大图</em></p><p>这时你可以使用 <code>conda</code> 安装 <code>libgcc</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install libgcc</span><br></pre></td></tr></table></figure><p>然后再导入问题就应该解决了。</p><p>其他问题可以参考 <a href="http://www.exegetic.biz/blog/2015/12/installing-xgboost-ubuntu/" target="_blank" rel="noopener">Installing XGBoost on Ubuntu</a> 。</p><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于什么是 &lt;code&gt;XGBoost&lt;/code&gt;，我在这里不再解释，如果有时间的话再写一篇文章来解释，在数据科学里非常有用。大家可以参考 Tianqi Chen 的论文 &lt;a href=&quot;https://arxiv.org/abs/1603.02754&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;XGBoost: A Scalable Tree Boosting System&lt;/a&gt; ，本篇文章只是在 Windows 10 和 Ubuntu 上安装 &lt;code&gt;XGBoost&lt;/code&gt; 的 Python 包的方法，Ubuntu 版本为 16.04 LTS，Python 版本均为 3.5，Anaconda。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
      <category term="Data Science" scheme="https://secsilm.github.io/tags/Data-Science/"/>
    
  </entry>
  
  <entry>
    <title>Python 自动生成命令行工具 - fire 简介</title>
    <link href="https://secsilm.github.io/2017/04/22/python-fire/"/>
    <id>https://secsilm.github.io/2017/04/22/python-fire/</id>
    <published>2017-04-22T02:16:00.000Z</published>
    <updated>2017-12-23T00:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>Python 中用于生成命令行接口（Command Line Interfaces, CLIs）的工具已经有一些了，例如已经成为 Python 标准库的 <a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener">argparse</a> 和第三方的 <a href="http://click.pocoo.org/5/" target="_blank" rel="noopener">click</a> ，这些都是很不错的工具。但是这些工具为 Python 程序生成 CLIs 的时候稍显麻烦，需要增加的地方比较多，过程稍显繁琐。今天介绍的这个工具，几乎可以不改变原始代码就可以生成 CLIs，是 Google 于 今年（2017 年）3 月 2 日宣布在 <a href="https://github.com/google/python-fire" target="_blank" rel="noopener">GitHub</a> 上开源的。</p><a id="more"></a><p><img src="http://img.blog.csdn.net/20170421194937214?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="fire 开源"></p><p>文中的三个程序文件可以在 <a href="http://download.csdn.net/download/u010099080/9821999" target="_blank" rel="noopener">这里</a> 打包下载。</p><hr><h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>这里我会列出对本文的更新。</p><ul><li>2017 年 10 月 18 日：优化排版，与其他博文保持风格统一。</li></ul><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>大致的简介我在上面简单说了下，下面我引用下官方简介：</p><blockquote><p>Python Fire is a library for creating command line interfaces (CLIs) from absolutely any Python object.</p><ul><li>Python Fire is a simple way to create a CLI in Python.</li><li>Python Fire is a helpful tool for developing and debugging Python code.</li><li>Python Fire helps with exploring existing code or turning other people’s code into a CLI.</li><li>Python Fire makes transitioning between Bash and Python easier.</li><li>Python Fire makes using a Python REPL easier by setting up the REPL with the modules and variables you’ll need already imported and created.</li></ul></blockquote><p>可以看出来，Fire 不仅仅是一个生成 CLIs 的工具，而且还可以调试 Python 程序，交互式的使用 Fire 。</p><hr><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>在命令行中运行 <code>pip install fire</code> 即可安装。</p><hr><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><p>使用 <code>fire</code> 生成 CLIs 的基本用法就是使用 <code>fire.Fire()</code>，可以传入任意参数，用官方的说法就是 <em>any Python object</em> 。下面我举三个例子简单的说明一下用法，更多的用法大家可以参考 <a href="https://github.com/google/python-fire/blob/master/doc/guide.md" target="_blank" rel="noopener">The Python Fire Guide</a> 和 <a href="https://github.com/google/python-fire/blob/master/doc/using-cli.md" target="_blank" rel="noopener">Using a Fire CLI</a> 。</p><p>下面的例子均是以计算时间差（两个日期之间相差的天数）的程序为例。</p><h3 id="单个函数"><a href="#单个函数" class="headerlink" title="单个函数"></a>单个函数</h3><p>文件 <code>test-fire.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_days</span><span class="params">(date_str1, date_str2)</span>:</span></span><br><span class="line">    <span class="string">'''计算两个日期之间的天数'''</span></span><br><span class="line"></span><br><span class="line">    date_str1 = str(date_str1)</span><br><span class="line">    date_str2 = str(date_str2)</span><br><span class="line">    d1 = datetime.datetime.strptime(date_str1, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">    d2 = datetime.datetime.strptime(date_str2, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">    delta = d1 - d2</span><br><span class="line">    <span class="keyword">return</span> delta.days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire(cal_days)</span><br></pre></td></tr></table></figure><p>这个程序中只有一个函数 <code>cal_days</code>，最后使用 <code>fire.Fire(cal_days)</code> 来生成 CLIs。</p><p>我们可以先用下面的语句查看帮助信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ python <span class="built_in">test</span>-fire.py</span><br><span class="line">Fire trace:</span><br><span class="line">1. Initial component</span><br><span class="line">2. (<span class="string">'The function received no value for the required argument:'</span>, <span class="string">'date_str1'</span>)</span><br><span class="line"></span><br><span class="line">Type:        <span class="keyword">function</span></span><br><span class="line">String form: &lt;<span class="keyword">function</span> cal_days at 0x000002F0099B7F28&gt;</span><br><span class="line">File:        d:\masterfiles\notebook\<span class="built_in">test</span>-fire.py</span><br><span class="line">Line:        4</span><br><span class="line">Docstring:   计算两个日期之间的天数</span><br><span class="line"></span><br><span class="line">Usage:       <span class="built_in">test</span>-fire.py DATE_STR1 DATE_STR2</span><br><span class="line">             <span class="built_in">test</span>-fire.py --date-str1 DATE_STR1 --date-str2 DATE_STR2</span><br></pre></td></tr></table></figure><p>我们可以看到传给 <code>fire.Fire()</code> 的参数类型（<code>function</code>）、文件路径、文档字符串、参数用法等信息。</p><p>现在我们试着计算一下 2017 年 4 月 21 号和 2017 年 4 月 1 号差了多少天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python <span class="built_in">test</span>-fire.py 20170421 20170401</span><br><span class="line">20</span><br></pre></td></tr></table></figure><p>当然，你也可以使用 <code>python test-fire.py --date-str1 20170421 --date-str2 20170401</code>。</p><p>此外，对于这种单个函数的情况，程序的最后一行 <code>fire.Fire(cal_days)</code> 可以改为 <code>fire.Fire()</code>，结果完全一样，<code>fire.Fire()</code> 会默认使用 <code>cal_days()</code> 函数。</p><hr><h3 id="多个函数"><a href="#多个函数" class="headerlink" title="多个函数"></a>多个函数</h3><p>文件 <code>test-fire-2func.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_days</span><span class="params">(date_str1, date_str2)</span>:</span></span><br><span class="line">    <span class="string">'''计算两个日期之间的天数'''</span></span><br><span class="line"></span><br><span class="line">    date_str1 = str(date_str1)</span><br><span class="line">    date_str2 = str(date_str2)</span><br><span class="line">    d1 = datetime.datetime.strptime(date_str1, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">    d2 = datetime.datetime.strptime(date_str2, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">    delta = d1 - d2</span><br><span class="line">    <span class="keyword">return</span> delta.days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">days2today</span><span class="params">(date_str)</span>:</span></span><br><span class="line">    <span class="string">'''计算某天距离今天的天数'''</span></span><br><span class="line"></span><br><span class="line">    date_str = str(date_str)</span><br><span class="line">    d = datetime.datetime.strptime(date_str, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">    delta = datetime.datetime.now() - d</span><br><span class="line">    <span class="keyword">return</span> delta.days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><p>这个程序比 <code>test-fire.py</code> 多了一个函数，但是结尾处仍然使用 <code>fire.Fire()</code> 来生成 CLIs。我们先不输任何参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ python <span class="built_in">test</span>-fire-2func.py 20170401</span><br><span class="line">Fire trace:</span><br><span class="line">1. Initial component</span><br><span class="line">2. (<span class="string">'Cannot find target in dict'</span>, <span class="string">'20170401'</span>, &#123;<span class="string">'__name__'</span>: <span class="string">'__main__'</span>, <span class="string">'datetime'</span>: &lt;module <span class="string">'datetime'</span> from <span class="string">'C:\\Users\\secsi\\Anaconda3\\lib\\datetime.py'</span>&gt;, <span class="string">'cal_days'</span>: &lt;<span class="keyword">function</span> cal_days at 0x0000027A855E7F28&gt;, <span class="string">'days2today'</span>: &lt;<span class="keyword">function</span> days2today at 0x0000027A86EC6B70&gt;, <span class="string">'__loader__'</span>: &lt;_frozen_importlib_external.SourceFileLoader object at 0x0000027A85655A90&gt;, <span class="string">'__file__'</span>: <span class="string">'test-fire-2func.py'</span>, <span class="string">'__cached__'</span>: None, <span class="string">'fire'</span>: &lt;module <span class="string">'fire'</span> from <span class="string">'C:\\Users\\secsi\\Anaconda3\\lib\\site-packages\\fire\\__init__.py'</span>&gt;, <span class="string">'__package__'</span>: None, <span class="string">'__builtins__'</span>: &lt;module <span class="string">'builtins'</span> (built-in)&gt;, <span class="string">'__doc__'</span>: None, <span class="string">'__spec__'</span>: None&#125;)</span><br><span class="line"></span><br><span class="line">Type:        dict</span><br><span class="line">String form: &#123;<span class="string">'__name__'</span>: <span class="string">'__main__'</span>, <span class="string">'datetime'</span>: &lt;module <span class="string">'datetime'</span> from <span class="string">'C:\\Users\\secsi\\Anaconda3\\lib\\d &lt;...&gt; kage__'</span>: None, <span class="string">'__builtins__'</span>: &lt;module <span class="string">'builtins'</span> (built-in)&gt;, <span class="string">'__doc__'</span>: None, <span class="string">'__spec__'</span>: None&#125;</span><br><span class="line">Length:      12</span><br><span class="line"></span><br><span class="line">Usage:       <span class="built_in">test</span>-fire-2func.py</span><br><span class="line">             <span class="built_in">test</span>-fire-2func.py datetime</span><br><span class="line">             <span class="built_in">test</span>-fire-2func.py cal-days</span><br><span class="line">             <span class="built_in">test</span>-fire-2func.py days2today</span><br><span class="line">             <span class="built_in">test</span>-fire-2func.py fire</span><br></pre></td></tr></table></figure><p>程序中有个两个函数，而我们运行 <code>python test-fire-2func.py 20170401</code> 的时候却没有指定使用哪个函数，所以会报错。</p><p>正确的写法应该是这样的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python <span class="built_in">test</span>-fire-2func.py days2today 20170401</span><br><span class="line">21</span><br><span class="line">$ python <span class="built_in">test</span>-fire-2func.py cal_days 20170422 20170401</span><br><span class="line">21</span><br></pre></td></tr></table></figure><p>也就是说在后面跟上要使用的函数名。</p><hr><h3 id="对象（类）"><a href="#对象（类）" class="headerlink" title="对象（类）"></a>对象（类）</h3><p><code>Fire</code> 既然能够 <em>fire</em> 任何 Python 对象，那么给 <code>fire.Fire()</code> 传一个类也是完全可以的。下面是一个例子：</p><p>文件 <code>test-fire-class.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DateStr</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_days</span><span class="params">(self, date_str1, date_str2)</span>:</span></span><br><span class="line">        <span class="string">'''计算两个日期之间的天数'''</span></span><br><span class="line">        date_str1 = str(date_str1)</span><br><span class="line">        date_str2 = str(date_str2)</span><br><span class="line">        d1 = datetime.datetime.strptime(date_str1, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">        d2 = datetime.datetime.strptime(date_str2, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">        delta = d1 - d2</span><br><span class="line">        <span class="keyword">return</span> delta.days</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">days2today</span><span class="params">(self, date_str)</span>:</span></span><br><span class="line">        <span class="string">'''计算某天距离今天的天数'''</span></span><br><span class="line">        date_str = str(date_str)</span><br><span class="line">        d = datetime.datetime.strptime(date_str, <span class="string">'%Y%m%d'</span>)</span><br><span class="line">        delta = datetime.datetime.now() - d</span><br><span class="line">        <span class="keyword">return</span> delta.days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire(DateStr)</span><br></pre></td></tr></table></figure><p>这里我定义了一个 <code>DateStr</code> 类，有两个类方法 <code>cal_days</code> 和 <code>days2today</code> ，那么我只需 <code>fire.Fire(DateStr)</code> 就可以调用这两个类方法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python <span class="built_in">test</span>-fire-class.py days2today 20170401</span><br><span class="line">21</span><br><span class="line">$ python <span class="built_in">test</span>-fire-class.py cal-days 20170422 20170401</span><br><span class="line">21</span><br></pre></td></tr></table></figure><hr><h2 id="还有几点"><a href="#还有几点" class="headerlink" title="还有几点"></a>还有几点</h2><ul><li><code>fire</code> 默认使用 <code>-</code> 作为参数分隔符，所以如果你要在命令行传入类似 <code>2017-04-22</code> 的参数时，那么程序接收到的参数就肯定不是 <code>2017-04-22</code> 了，你需要使用 <code>--separator</code> 来改变分隔符，参考 <a href="https://github.com/google/python-fire/blob/master/doc/using-cli.md#separator-flag" target="_blank" rel="noopener">Changing the Separator</a></li><li><code>fire</code> 会自动区分你在命令行传入的参数的类型，例如 <code>20170422</code> 会自动识别成 <code>int</code>，<code>hello</code> 会自动识别成 <code>str</code>，<code>&#39;(1,2)&#39;</code> 会自动识别成 <code>tuple</code>，<code>&#39;{&quot;name&quot;: &quot;Alan Lee&quot;}&#39;</code> 会自动识别成 <code>dict</code>。但是你如果想要传入一个字符串类型的 <code>20170422</code> 怎么办？那就需要这样写：<code>&#39;&quot;20170422&quot;&#39;</code> 或者 <code>&quot;&#39;20170422&#39;&quot;</code> 或者 <code>\&quot;20170422\&quot;</code>，总之呢，就是加一个转义，因为命令行默认会吃掉你的引号。参考 <a href="https://github.com/google/python-fire/blob/master/doc/guide.md#user-content-argument-parsing" target="_blank" rel="noopener">Argument Parsing</a></li></ul><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python 中用于生成命令行接口（Command Line Interfaces, CLIs）的工具已经有一些了，例如已经成为 Python 标准库的 &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;argparse&lt;/a&gt; 和第三方的 &lt;a href=&quot;http://click.pocoo.org/5/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;click&lt;/a&gt; ，这些都是很不错的工具。但是这些工具为 Python 程序生成 CLIs 的时候稍显麻烦，需要增加的地方比较多，过程稍显繁琐。今天介绍的这个工具，几乎可以不改变原始代码就可以生成 CLIs，是 Google 于 今年（2017 年）3 月 2 日宣布在 &lt;a href=&quot;https://github.com/google/python-fire&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt; 上开源的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://secsilm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>奇异值分解 SVD 的数学解释</title>
    <link href="https://secsilm.github.io/2017/03/29/understanding-svd/"/>
    <id>https://secsilm.github.io/2017/03/29/understanding-svd/</id>
    <published>2017-03-29T04:33:00.000Z</published>
    <updated>2017-12-25T01:22:10.817Z</updated>
    
    <content type="html"><![CDATA[<p>奇异值分解（Singular Value Decomposition，SVD）是一种矩阵分解（Matrix Decomposition）的方法。除此之外，矩阵分解还有很多方法，例如特征分解（Eigendecomposition）、LU分解（LU decomposition）、QR分解（QR decomposition）和极分解（Polar decomposition）等。这篇文章主要说下奇异值分解，这个方法在机器学习的一些算法里占有重要地位。</p><a id="more"></a><hr><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p><em>参考自维基百科。</em></p><ul><li><strong>正交矩阵</strong>：若一个方阵其行与列皆为正交的单位向量，则该矩阵为正交矩阵，且该矩阵的转置和其逆相等。两个向量正交的意思是两个向量的内积为 0</li><li><strong>正定矩阵</strong>：如果对于所有的非零实系数向量 $z$，都有 $z^TAz&gt;0$，则称矩阵 $A$ 是正定的。正定矩阵的行列式必然大于 0， 所有特征值也必然 &gt; 0。相对应的，半正定矩阵的行列式必然 ≥ 0。</li></ul><hr><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>下面引用 <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank" rel="noopener">SVD 在维基百科中的定义</a>。</p><blockquote><p>In linear algebra, the <strong>singular value decomposition</strong> (<strong>SVD</strong>) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any $ m\times n $ matrix via an extension of polar decomposition. </p></blockquote><p>也就是说 SVD 是线代中对于实数矩阵和复数矩阵的分解，将特征分解从 <em>半正定矩阵</em> 推广到任意 $ m\times n $ 矩阵。</p><p><strong>注意：本篇文章内如未作说明矩阵均指实数矩阵。</strong></p><p>假设有 $ m\times n $ 的矩阵 $A$ ，那么 SVD 就是要找到如下式的这么一个分解，将 $A$ 分解为 3 个矩阵的乘积：</p><script type="math/tex; mode=display">A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}</script><p>其中，$U$ 和 $V$ 都是<strong>正交矩阵</strong> （Orthogonal Matrix），在复数域内的话就是酉矩阵（Unitary Matrix），即 </p><script type="math/tex; mode=display">U^TU = E_{m \times m}</script><script type="math/tex; mode=display">V^TV=E_{n \times n}</script><p> 换句话说，就是说 $U$ 的转置等于 $U$ 的逆，$V$ 的转置等于 $V$ 的逆：</p><script type="math/tex; mode=display">U^T = U^{-1}</script><script type="math/tex; mode=display">V^T = V^{-1}</script><p>而 $ \Sigma $ 就是一个非负实对角矩阵。</p><p>那么 $U$ 和 $V$ 以及 $ \Sigma $ 是如何构成的呢？</p><hr><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>$U$ 和 $V$ 的列分别叫做 $A$ 的 <strong>左奇异向量</strong>（left-singular vectors）和 <strong>右奇异向量</strong>（right-singular vectors），$ \Sigma $ 的对角线上的值叫做 $A$ 的奇异值（singular values）。</p><p>其实整个求解 SVD 的过程就是求解这 3 个矩阵的过程，而求解这 3 个矩阵的过程就是求解特征值和特征向量的过程，问题就在于 <strong>求谁的特征值和特征向量</strong>。</p><ul><li>$U$ 的列由 $AA^T$ 的单位化过的特征向量构成</li><li>$V$ 的列由 $A^TA$ 的单位化过的特征向量构成</li><li>$ \Sigma $ 的对角元素来源于 $AA^T$ 或 $A^TA$ 的特征值的平方根，并且是按从大到小的顺序排列的</li></ul><p>知道了这些，那么求解 SVD 的步骤就显而易见了：</p><ol><li>求 $AA^T$ 的特征值和特征向量，用单位化的特征向量构成 $U$</li><li>求 $A^TA$ 的特征值和特征向量，用单位化的特征向量构成 $V$</li><li>将 $AA^T$ 或者 $A^TA$ 的特征值求平方根，然后构成 $ \Sigma $</li></ol><hr><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>假设 </p><script type="math/tex; mode=display">A = \begin{pmatrix} 2 & 4 \\\\ 1 & 3 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \end{pmatrix}</script><p>那么可以计算得到 </p><script type="math/tex; mode=display">AA^T = \begin{pmatrix} 20 & 14 & 0 & 0 \\\\ 14 & 10 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ \end{pmatrix}</script><p>接下来就是求这个矩阵的特征值和特征向量了 </p><script type="math/tex; mode=display">AA^T x = \lambda x</script><script type="math/tex; mode=display">(AA^T - \lambda E)x = 0</script><p>要想该方程组有非零解（即非零特征值），那么系数矩阵 $ AA^T - \lambda E $ 的行列式必须为 0 </p><script type="math/tex; mode=display">\begin{vmatrix} 20-\lambda & 14 & 0 & 0 \\\\ 14 & 10-\lambda & 0 & 0 \\\\ 0 & 0 & -\lambda & 0 \\\\ 0 & 0 & 0 & -\lambda \\\\ \end{vmatrix} = 0</script><p>求解这个行列式我就不再赘述了，这个直接使用行列式展开定理就可以了，可以得到 $\lambda_1 \approx 29.86606875，\lambda_2 \approx  0.13393125，\lambda_3 = \lambda_4 = 0$，有 4 个特征值，因为特征多项式 $ \vert AA^T - \lambda E \vert $ 是一个 4 次多项式。对应的单位化过的特征向量为</p><script type="math/tex; mode=display">\begin{pmatrix} 0.81741556 & -0.57604844 & 0 & 0 \\\\ 0.57604844 & 0.81741556 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \end{pmatrix}</script><p>这就是矩阵 $U$ 了。</p><p>同样的过程求解 $A^TA$ 的特征值和特征向量，求得 $\lambda_1 \approx 0.13393125，\lambda_2 \approx  29.86606875 $，将<strong>特征值降序排列后</strong>对应的单位化过的特征向量为</p><script type="math/tex; mode=display">\begin{pmatrix} 0.40455358 & -0.9145143 \\\\ 0.9145143 & 0.40455358 \\\\ \end{pmatrix}</script><p>这就是矩阵 $V$ 了。</p><p>而矩阵 $\Sigma$ 根据上面说的为特征值的平方根构成的对角矩阵</p><script type="math/tex; mode=display">\begin{pmatrix} 5.4649857 & 0 \\\\ 0 & 0.36596619 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \end{pmatrix}</script><p>到此，SVD 分解就结束了，原来的矩阵 $A$ 就被分解成了 3 个矩阵的乘积。</p><script type="math/tex; mode=display">A\_{4 \times 2} = U\_{4 \times 4}\Sigma\_{4 \times 2} V^T_{2 \times 2}</script><script type="math/tex; mode=display">\begin{pmatrix} 2 & 4 \\\\ 1 & 3 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \end{pmatrix} = \begin{pmatrix} 0.81741556 & -0.57604844 & 0 & 0 \\\\ 0.57604844 & 0.81741556 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \end{pmatrix} \begin{pmatrix} 5.4649857 & 0 \\\\ 0 & 0.36596619 \\\\ 0 & 0 \\\\ 0 & 0 \\\\ \end{pmatrix} \begin{pmatrix} 0.40455358 & -0.9145143 \\\\ 0.9145143 & 0.40455358 \\\\ \end{pmatrix} ^ T</script><hr><h2 id="Numpy-实现"><a href="#Numpy-实现" class="headerlink" title="Numpy 实现"></a>Numpy 实现</h2><p>Python 中可以使用 numpy 包的 <code>linalg.svd()</code> 来求解 SVD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">2</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">print(np.linalg.svd(A))</span><br></pre></td></tr></table></figure><p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(array([[-0.81741556, -0.57604844,  0.        ,  0.        ],</span><br><span class="line">        [-0.57604844,  0.81741556,  0.        ,  0.        ],</span><br><span class="line">        [ 0.        ,  0.        ,  1.        ,  0.        ],</span><br><span class="line">        [ 0.        ,  0.        ,  0.        ,  1.        ]]),</span><br><span class="line"> array([ 5.4649857 ,  0.36596619]),</span><br><span class="line"> array([[-0.40455358, -0.9145143 ],</span><br><span class="line">        [-0.9145143 ,  0.40455358]]))</span><br></pre></td></tr></table></figure></p><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;奇异值分解（Singular Value Decomposition，SVD）是一种矩阵分解（Matrix Decomposition）的方法。除此之外，矩阵分解还有很多方法，例如特征分解（Eigendecomposition）、LU分解（LU decomposition）、QR分解（QR decomposition）和极分解（Polar decomposition）等。这篇文章主要说下奇异值分解，这个方法在机器学习的一些算法里占有重要地位。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>使用 Python 统计字符串中英文、空格、数字、标点个数</title>
    <link href="https://secsilm.github.io/2017/03/25/python-string-count/"/>
    <id>https://secsilm.github.io/2017/03/25/python-string-count/</id>
    <published>2017-03-25T14:01:00.000Z</published>
    <updated>2017-12-23T00:21:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>题外话：今天打酱油的做了网易数据挖掘工程师的在线笔试题，被打击了。</em></p><p>本文代码可在 <a href="http://download.csdn.net/detail/u010099080/9793725" target="_blank" rel="noopener">这里</a> 下载。</p><a id="more"></a><hr><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>在网上无意间看到这么一个题目：统计一个字符串中的中英文、空格、数字、标点符号个数。<br>正好再熟悉一下 Python 中字符串相关方法，所以来做一下。</p><hr><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str_count</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">'''找出字符串中的中英文、空格、数字、标点符号个数'''</span></span><br><span class="line">    </span><br><span class="line">    count_en = count_dg = count_sp = count_zh = count_pu = <span class="number">0</span></span><br><span class="line">    s_len = len(s)</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">        <span class="keyword">if</span> c <span class="keyword">in</span> string.ascii_letters:</span><br><span class="line">            count_en += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> c.isdigit():</span><br><span class="line">            count_dg += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> c.isspace():</span><br><span class="line">            count_sp += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> c.isalpha():</span><br><span class="line">            count_zh += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            count_pu += <span class="number">1</span></span><br><span class="line">    total_chars = count_zh + count_en + count_sp + count_dg + count_pu</span><br><span class="line">    <span class="keyword">if</span> total_chars == s_len:</span><br><span class="line">        <span class="keyword">return</span> namedtuple(<span class="string">'Count'</span>, [<span class="string">'total'</span>, <span class="string">'zh'</span>, <span class="string">'en'</span>, <span class="string">'space'</span>, <span class="string">'digit'</span>, <span class="string">'punc'</span>])(s_len, count_zh, count_en, count_sp, count_dg, count_pu)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'Something is wrong!'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s = <span class="string">'上面是引用了官网的介绍，意思就是说 TensorBoard 就是一个方便你理解、调试、优化 TensorFlow 程序的可视化工具，你可以可视化你的 TensorFlow graph、学习参数以及其他数据比如图像。'</span></span><br><span class="line">count = str_count(s)</span><br><span class="line">print(s, end=<span class="string">'\n\n'</span>)</span><br><span class="line">print(<span class="string">'该字符串共有 &#123;&#125; 个字符，其中有 &#123;&#125; 个汉字，&#123;&#125; 个英文，&#123;&#125; 个空格，&#123;&#125; 个数字，&#123;&#125; 个标点符号。'</span>.format(count.total, count.zh, count.en, count.space, count.digit, count.punc))</span><br></pre></td></tr></table></figure><p>将上面的程序保存到 <code>str_count.py</code>，然后执行测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> python str_count.py</span></span><br><span class="line"></span><br><span class="line">上面是引用了官网的介绍，意思就是说 TensorBoard 就是一个方便你理解、调试、优化 TensorFlow 程序的可视化工具，你可以可视化你的 TensorFlow graph、学习参数以及其他数据比如图像。</span><br><span class="line"></span><br><span class="line">该字符串共有 107 个字符，其中有 59 个汉字，36 个英文，6 个空格，0 个数字，6 个标点符号。</span><br></pre></td></tr></table></figure><p>那个用于测试的字符串 <code>s</code> 源自 <a href="http://blog.csdn.net/u010099080/article/details/62882006" target="_blank" rel="noopener">我的一篇关于 TensorBoard 的博文</a>，首先输出原始字符串，然后输出中英文、空格、数字、标点符号各自的个数。</p><p>以后有好的想法再来优化这个程序，大家有什么好的想法也欢迎可以在评论区留言。</p><hr><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;题外话：今天打酱油的做了网易数据挖掘工程师的在线笔试题，被打击了。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;本文代码可在 &lt;a href=&quot;http://download.csdn.net/detail/u010099080/9793725&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt; 下载。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://secsilm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 中的卷积神经网络 CNN - TensorBoard 版</title>
    <link href="https://secsilm.github.io/2017/03/17/tensorflow-cnn-tensorboard/"/>
    <id>https://secsilm.github.io/2017/03/17/tensorflow-cnn-tensorboard/</id>
    <published>2017-03-17T02:57:00.000Z</published>
    <updated>2017-12-23T00:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.csdn.net/u010099080/article/details/53906810" target="_blank" rel="noopener">前面</a> 写了一篇用 TensorFlow 实现 CNN 的文章，没有实现 TensorBoard，这篇来加上 TensorBoard 的实现，代码可以从 <a href="http://download.csdn.net/detail/u010099080/9781711" target="_blank" rel="noopener">这里</a> 下载。</p><a id="more"></a><hr><h1 id="什么是-TensorBoard"><a href="#什么是-TensorBoard" class="headerlink" title="什么是 TensorBoard"></a>什么是 TensorBoard</h1><blockquote><p>To make it easier to understand, debug, and optimize TensorFlow programs, we’ve included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.</p></blockquote><p>上面是引用了官网的介绍，意思就是说 TensorBoard 就是一个方便你理解、调试、优化 TensorFlow 程序的可视化工具，你可以可视化你的 TensorFlow graph、学习参数以及其他数据比如图像。</p><p>启动你的 TensorBoard 并在浏览器中打开后应该是类似下面这样的：</p><p><img src="http://img.blog.csdn.net/20170317093749890?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><h1 id="CNN-结构"><a href="#CNN-结构" class="headerlink" title="CNN 结构"></a>CNN 结构</h1><p>CNN 的结构和 <a href="http://blog.csdn.net/u010099080/article/details/53906810" target="_blank" rel="noopener">上篇</a> 一样，数据集仍为 CIFAR10 数据集。</p><p>下面我用 TensorBoard 绘制的结构图说一下：</p><p><img src="http://img.blog.csdn.net/20170317094029137?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>为了简洁我隐掉了一些不必要的节点。从图中可以看出有<strong>两个卷积层</strong>、<strong>两个池化层</strong>、<strong>两个 norm 层</strong>以及<strong>三个全连接层</strong>，图中指向 <code>train</code> 节点的线条的粗细表示需要训练的参数的多少，各层之间的线条上的数字表示了传递给下一层的参数的维度，例如 <code>conv1</code> 传递给 <code>pool1</code> 的参数维度是 <code>?×32×32×64</code> （<em>由于这个图不能放大导致重叠，在浏览器中是可以放大的</em>），<code>?</code> 表示 batch 的大小。具体的各层参数如下：</p><ul><li><code>conv1</code>：kernel 大小是 <code>[5, 5, 3, 64]</code>，步长为 1，padding 为 <code>SAME</code>。</li><li><code>pool1</code>：kernel 大小是 <code>[1, 3, 3, 1]</code>，步长为 2，padding 为 <code>SAME</code>。</li><li><code>conv1</code>：kernel 大小是 <code>[5, 5, 64, 64]</code>，步长为 1，padding 为 <code>SAME</code>。</li><li><code>pool1</code>：kernel 大小是 <code>[1, 3, 3, 1]</code>，步长为 2，padding 为 <code>SAME</code>。</li><li><code>fc1</code>：神经元个数为 384</li><li><code>fc2</code>：神经元个数为 192</li></ul><hr><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>完整代码可以在 <a href="http://download.csdn.net/detail/u010099080/9781711" target="_blank" rel="noopener">这里</a> 下载，下面我就说下关于 TensorBoard 的部分。</p><p>上面那个图中的每个节点都是用 <code>tf.namescope()</code> 指定的，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1'</span>):</span><br><span class="line">        conv1 = tf.nn.conv2d(x4d, weight_variable(<span class="string">'conv1'</span>, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">64</span>], <span class="number">5e-2</span>, <span class="string">'w_conv1'</span>), strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        conv1 = tf.nn.bias_add(conv1, bias_variable(<span class="string">'conv1'</span>, <span class="number">0.0</span>, tf.float32, [<span class="number">64</span>], <span class="string">'b_conv1'</span>))</span><br><span class="line">        conv1 = tf.nn.relu(conv1)</span><br></pre></td></tr></table></figure><p>这就指定了 <code>conv1</code> 的节点。相同节点名字会在一起。</p><p>你可以使用 <code>tf.summary.scalar</code> 记录准确率、损失等数据，使用 <code>tf.summary.histogram</code> 记录参数的分布情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</span><br><span class="line">            correct_pred = tf.equal(tf.argmax(fc3, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line">    tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br></pre></td></tr></table></figure><p>然后用 <code>tf.summary.merge_all</code> 将这些操作集中起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merged_summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><p>最后运行的时候使用 <code>tf.summary.FileWriter</code> 将这些操作得到的数据写进日志文件，以供 TensorBoard 可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">'./tensorboard/log/'</span>, graph=tf.get_default_graph())</span><br></pre></td></tr></table></figure><p>还可以可以使用<code>tf.train.Saver</code> 保存模型，TensorBoard 可以显示每一步的运行时间以及内存使用情况。（<em>下面仅是代码片段</em>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="comment"># 这里有其他代码</span></span><br><span class="line">run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)</span><br><span class="line">run_metadata = tf.RunMetadata()</span><br><span class="line"><span class="comment"># 这里有其他代码</span></span><br><span class="line">summary_writer.add_run_metadata(run_metadata, <span class="string">'step%d'</span> % (i * total_batch + batch))</span><br><span class="line">saver.save(sess, <span class="string">'./tensorboard/log/model.ckpt'</span>, i * total_batch + batch)</span><br></pre></td></tr></table></figure><p>总之有好多功能，我在这里就不一一阐述了，可以去官网看文档。</p><hr><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><p>进入到 <code>tensorboard</code> 所在目录后，执行下面的语句即可启动 TensorBoard ：</p><p>没有使用 <code>tf.train.Saver()</code> 的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=tensorboard/log/without-saver</span><br></pre></td></tr></table></figure></p><p>使用 <code>tf.train.Saver()</code> 的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=tensorboard/log/with-saver</span><br></pre></td></tr></table></figure></p><p>按照提示，在浏览器中打开地址就可以看到可视化结果了。</p><p><img src="http://img.blog.csdn.net/20170317104021861?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><em>例如我这里是 <code>http://192.168.16.1:6006/</code></em> </p><p>训练准确率曲线：</p><p><img src="http://img.blog.csdn.net/20170317104255958?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>损失曲线：</p><p><img src="http://img.blog.csdn.net/20170317104417375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>Graph：</p><p><img src="http://img.blog.csdn.net/20170317104519633?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>Step 100 的各节点计算时间（需要使用 <code>tf.train.Saver()</code>）：</p><p><img src="http://img.blog.csdn.net/20170317105057094?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>Step 100 的各节点内存消耗（需要使用 <code>tf.train.Saver()</code>）：</p><p><img src="http://img.blog.csdn.net/20170317105148222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p><code>fc1</code> 层参数的降维可视化，可以旋转缩放，这个在这里不太有用，仅作展示用，三个主成分解释的总方差才 21%。在做 NLP 的时候这个功能就非常有用了，可以方便的展示词的位置。</p><p><img src="http://img.blog.csdn.net/20170317105524256?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><hr><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1><p>OK，先到这里吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/u010099080/article/details/53906810&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;前面&lt;/a&gt; 写了一篇用 TensorFlow 实现 CNN 的文章，没有实现 TensorBoard，这篇来加上 TensorBoard 的实现，代码可以从 &lt;a href=&quot;http://download.csdn.net/detail/u010099080/9781711&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt; 下载。&lt;/p&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://secsilm.github.io/tags/TensorFlow/"/>
    
      <category term="Machine Learning" scheme="https://secsilm.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>使用 tree 命令格式化输出目录结构</title>
    <link href="https://secsilm.github.io/2017/03/15/using-tree/"/>
    <id>https://secsilm.github.io/2017/03/15/using-tree/</id>
    <published>2017-03-15T04:45:00.000Z</published>
    <updated>2017-12-23T00:18:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天在写一个 Markdown 文件的时候需要将一个目录的结构表示出来，于是找了找有没有相关命令，找到一个叫做 <code>tree</code> 的命令，Windows 和 Linux 都有这个命令。</p><a id="more"></a><hr><h1 id="Windows-10"><a href="#Windows-10" class="headerlink" title="Windows 10"></a>Windows 10</h1><p>我是在 Windows 10 上用的，不过根据 <a href="http://www.computerhope.com/treehlp.htm" target="_blank" rel="noopener">这篇文章</a> 应该在 Windows 7 和 8 上也是可以用的。用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree [Drive:[[Path] [/F] [/A]</span><br></pre></td></tr></table></figure><p>其中</p><ul><li><code>/F</code> 表示不仅输出文件夹，也输出文件名。默认是只输出文件夹的名字。</li><li><code>/A</code> 表示使用另一种方式来绘制目录树。</li></ul><p>例如：</p><p>仅列出目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree D:\MasterFiles\MachineLearning\TensorFlow\TensorFlow-Examples\MyCode\cifar10-TensorFlow-tensorboard</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20170315123020407?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="仅列出目录"></p><hr><p>另一种方式输出<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree D:\MasterFiles\MachineLearning\TensorFlow\TensorFlow-Examples\MyCode\cifar10-TensorFlow-tensorboard /A</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20170315123119881?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="另一种方式输出"></p><hr><p>列出目录及文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree D:\MasterFiles\MachineLearning\TensorFlow\TensorFlow-Examples\MyCode\cifar10-TensorFlow-tensorboard /F</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20170315123155565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="列出目录及文件"></p><hr><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><p>由于我现在用的是 Windows 10，所以就先暂时用 Windows 10 自带的 Ubuntu 子系统测试了，回头再用真正的 Ubuntu 试试。关于如何使用 Windows 10 自带的 Ubuntu 子系统我就不多说了，大家直接百度就好，教程很多。</p><p>列出目录及文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree cifar10-TensorFlow-tensorboard</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20170315123846386?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="列出目录及文件"></p><hr><p>只列出目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree cifar10-TensorFlow-tensorboard -d</span><br></pre></td></tr></table></figure></p><p><img src="http://img.blog.csdn.net/20170315123953494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA5OTA4MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="只列出目录"></p><p>更详细的参数设置可以使用 <code>man tree</code> 查看，如果输入 <code>tree</code> 提示没有安装，那么可以使用 <code>sudo apt-get install tree</code> 来安装该命令。</p><hr><h1 id="END"><a href="#END" class="headerlink" title="END"></a>END</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在写一个 Markdown 文件的时候需要将一个目录的结构表示出来，于是找了找有没有相关命令，找到一个叫做 &lt;code&gt;tree&lt;/code&gt; 的命令，Windows 和 Linux 都有这个命令。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ubuntu" scheme="https://secsilm.github.io/tags/Ubuntu/"/>
    
      <category term="Windows" scheme="https://secsilm.github.io/tags/Windows/"/>
    
  </entry>
  
</feed>
