<!DOCTYPE html>
<html>
    <!-- title -->




<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="Alan Lee">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Alan Lee">
    <meta name="keywords" content="Alan Lee Space Station | Alan Lee">
    <meta name="description" content="Machine Learning, NLP, Data Science">
    <meta name="Cache-Control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <title>使用 TensorFlow Estimators 进行文本分类 · Lee&#39;s Space Station</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s 1;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= /css/style.css?v=201807017_1502 as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= /css/mobile.css?v=20180709 media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/avatar/Bastion_cute.png" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <script>
        var _hmt = _hmt || [];
        (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?6e8b0c627bf164f809ee4346796b1952";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
    <!-- 谷歌统计  -->
    
    <script>
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'UA-111553531-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Lee&#39;s Space Station</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">使用 TensorFlow Estimators 进行文本分类</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Lee's Space Station</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style=








height:50vh;

>
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://i.imgur.com/CCFYySM.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            使用 TensorFlow Estimators 进行文本分类
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Machine Learning">Machine Learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Translation">Translation</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "TensorFlow">TensorFlow</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "NLP">NLP</a>
    
</div>
                
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2018/07/18</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <blockquote>
<p>译者注：</p>
<ul>
<li>本文翻译自 <a href="http://ruder.io/" target="_blank" rel="noopener">Sebastian Ruder</a> 于 2018 年 4 月 16 日发表的文章 <a href="http://ruder.io/text-classification-tensorflow-estimators/" target="_blank" rel="noopener">Text Classification with TensorFlow Estimators</a>，文章和 <a href="https://twitter.com/eisenjulian" target="_blank" rel="noopener">Julian Eisenschlos</a> 共同撰写，原先发表在 <a href="https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe" target="_blank" rel="noopener">TensorFlow 博客</a>。</li>
<li>文中括号或者引用块中的 <em>斜体字</em> 为对应的英文原文或者我自己注释的话（会标明「译者注」），否则为原文中本来就有的话。</li>
<li>目录保留英文原文。</li>
<li>你如果对 TensorFlow 中的 Datasets 和 Estimators 还不是很了解，那么可以参考我写的<a href="https://alanlee.fun/2017/12/22/understanding-estimators-datasets/" target="_blank" rel="noopener">理解 Estimators 和 Datasets</a>。</li>
<li>本人水平有限，如有错误欢迎指出。</li>
</ul>
</blockquote>
<p>Hello there! 通过这篇博文，我们将为你展示如何使用 TensorFlow 中的 Estimators 进行文本分类。下面是本文内容大纲：</p>
<ul>
<li>使用 Datasets 载入数据</li>
<li>使用预构建的 Estimators 构建基准</li>
<li>使用词嵌入（<em>word embeddings</em>）</li>
<li>使用卷积和 LSTM 层构建自定义的 Estimators</li>
<li>载入预训练的词向量（<em>word vectors</em>）</li>
<li>使用 TensorBoard 评估和比较模型</li>
</ul>
<hr>
<p>欢迎来到 TensorFlow Estimators 和 Datasets 入门系列博客的第四部分。你不需要事先阅读前几部分，但是如果想要再回顾一下下面的几个概念的话，还是建议去看下。<a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" target="_blank" rel="noopener">第一部分</a> 聚焦于预构建的 Estimators，<a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" target="_blank" rel="noopener">第二部分</a> 讨论了特征列（<em>feature columns</em>），而<a href="https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html" target="_blank" rel="noopener">第三部分</a> 则是介绍如何创建自定义的 Estimators。</p>
<p>这里，在第四部分中，我们将在前面几个部分的基础上去解决自然语言处理（NLP）中的一系列不同问题。具体来说，本文说明了如何使用自定义的 Estimators、嵌入和 <a href="https://www.tensorflow.org/api_docs/python/tf/layers" target="_blank" rel="noopener">tf.layers</a> 模块来解决一个文本分类问题。在这个过程中，我们将会学习使用 word2vec 和迁移学习技术来提升模型性能，尤其是在带标签的数据稀缺时。</p>
<p>我们将会给你展示相关的代码片段。<a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" target="_blank" rel="noopener">这里</a>是包含完整代码的 Jupyter Notebook 文件，你可以在本地或者 <a href="https://colab.research.google.com/drive/1oXjNYSJ3VsRvAsXN4ClmtsVEgPW_CX_c?hl=en#forceEdit=true&amp;offline=true&amp;sandboxMode=true" target="_blank" rel="noopener">Google Colaboratory</a> 运行。你也可以在<a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" target="_blank" rel="noopener">这里</a>找到相关的 <code>.py</code> 源文件。需要注意的是程序只是为了说明 Estimators 是如何工作的，并没有为了达到最大性能而优化。</p>
<h2 id="The-Task"><a href="#The-Task" class="headerlink" title="The Task"></a>The Task</h2><p>我们将要使用的数据集是 IMDB <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">Large Movie Review Dataset</a>，包含用于训练的 25000 篇带有明显情感倾向的电影评论，测试集也是 25000 篇。我们将会用此数据集训练一个二分类模型，用于判断一篇评论是积极的还是消极的。</p>
<p>为了说明，下面列出数据集中一个负面评论（2 颗星）的片段：</p>
<blockquote>
<p>Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film.</p>
</blockquote>
<p>Keras 提供了一个方便的函数来导入该数据集，同时也可以在<a href="https://s3.amazonaws.com/text-datasets/imdb.npz" target="_blank" rel="noopener">这里</a>下载一个序列化的 numpy array <code>.npy</code> 文件。对于文本分类来说，通常来说会限制词汇表的大小，以防止数据集变得太稀疏和维度太高从而过拟合。因此，每一个评论都由一系列的单词索引组成，从 4（对应于数据集中最常见的单词 <strong>the</strong>） 到 4999，对应于单词 <strong>orange</strong>。索引 1 代表句子的开始，2 代表所有的未知词（也叫作 out-of-vocabulary 或者 OOV）。这些索引是通过一个预处理文本的 pipeline 来得到的，这个 pipeline 包括首先清理、归一化（normalize）和 tokenize（<em>译者注：可以理解为分词</em>）每一个句子，然后构建一个以词（<em>tokens</em>）频为索引的字典。</p>
<p>在我们将数据载入到内存中后，我们用 0 将所有句子补齐到相同长度（这里是 200），这样对于训练集和测试集我们就分别有一个两维的 $25000 \times 200$ 的数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">5000</span></span><br><span class="line">sentence_size = <span class="number">200</span></span><br><span class="line">(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(num_words=vocab_size)</span><br><span class="line">x_train = sequence.pad_sequences(</span><br><span class="line">    x_train_variable, </span><br><span class="line">    maxlen=sentence_size, </span><br><span class="line">    padding=<span class="string">'post'</span>, </span><br><span class="line">    value=<span class="number">0</span>)</span><br><span class="line">x_test = sequence.pad_sequences(  </span><br><span class="line">    x_test_variable,</span><br><span class="line">    maxlen=sentence_size, </span><br><span class="line">    padding=<span class="string">'post'</span>, </span><br><span class="line">    value=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Input-Functions"><a href="#Input-Functions" class="headerlink" title="Input Functions"></a>Input Functions</h2><p>Estimators 框架使用<em>输入函数</em>（<em>input functions</em>）来将数据导入和模型区分开来。无论你的数据是在一个 <code>.csv</code> 文件中，还是在一个 <code>pandas.DataFrame</code> 中，无论你的数据能否载入到内存中，都有几个辅助函数可以帮你来创建输入函数。在我们的例子中，我们可以对训练集和测试集使用 <code>Dataset.from_tensor_slices</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x_len_train = np.array([min(len(x), sentence_size) <span class="keyword">for</span> x <span class="keyword">in</span> x_train_variable])</span><br><span class="line">x_len_test = np.array([min(len(x), sentence_size) <span class="keyword">for</span> x <span class="keyword">in</span> x_test_variable])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parser</span><span class="params">(x, length, y)</span>:</span></span><br><span class="line">    features = &#123;<span class="string">"x"</span>: x, <span class="string">"len"</span>: length&#125;</span><br><span class="line">    <span class="keyword">return</span> features, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))</span><br><span class="line">    dataset = dataset.shuffle(buffer_size=len(x_train_variable))</span><br><span class="line">    dataset = dataset.batch(<span class="number">100</span>)</span><br><span class="line">    dataset = dataset.map(parser)</span><br><span class="line">    dataset = dataset.repeat()</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line">    <span class="keyword">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))</span><br><span class="line">    dataset = dataset.batch(<span class="number">100</span>)</span><br><span class="line">    dataset = dataset.map(parser)</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line">    <span class="keyword">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure>
<p>我们打乱训练集数据和不预先指定我们想要训练的步数（<em>epochs</em>），而在测试阶段我们只需要遍历数据集一次就可以了。我们也添加了一个额外的 <code>&quot;len&quot;</code> 字段，该字段表示序列的原始长度，即没有补充过的，我们会在后面用到。</p>
<h2 id="Building-a-baseline"><a href="#Building-a-baseline" class="headerlink" title="Building a baseline"></a>Building a baseline</h2><p>在开始任何机器学习项目之前先构建一个基准总是好的。最开始的模型越简单越好，因为有一个简单鲁棒的基准可以帮助我们理解在增加模型复杂度之后，模型可以得到多少性能提升。当然如果这个简单的模型就能满足我们的需求，那就再好不过了。</p>
<p>因此，现在我们用一个最简单的模型来做文本分类，即一个不考虑词序的稀疏线性模型，为每一个 token（<em>译者注：可以理解为词，下同</em>）都分配一个权重，然后进行线性求和。因为这个模型不考虑词在句子中的顺序，我们也将称之为一种词袋（<em>Bag-of-Words</em>）方法。现在让我们来看下如何使用 Estimator 实现这个模型。</p>
<p>我们首先定义特征列（<em>feature column</em>），然后将此特征列输入到分类器。就像我们在<a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" target="_blank" rel="noopener">第二部分</a>中所看到的，对于这种文本预处理输入，我们应该使用 <code>categorical_column_with_identity</code>。如果我们的输入就是原本的文本，那么也有其他的 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column" target="_blank" rel="noopener"><code>feature_column</code></a> 可以进行很多预处理。现在我们可以使用预构建的 <code>LinearClassifier</code> 了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">column = tf.feature_column.categorical_column_with_identity(<span class="string">'x'</span>, vocab_size)</span><br><span class="line">classifier = tf.estimator.LinearClassifier(</span><br><span class="line">    feature_columns=[column],</span><br><span class="line">    model_dir=os.path.join(model_dir, <span class="string">'bow_sparse)</span></span><br><span class="line"><span class="string">)</span></span><br></pre></td></tr></table></figure>
<p>最后，我们创建一个用于训练分类器和创建 PR 曲线的函数。由于本文的目的并不是得到最好的模型性能，我们这里就只训练模型 25000 步。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_evaluate</span><span class="params">(classifier)</span>:</span></span><br><span class="line">    classifier.train(input_fn=train_input_fn, steps=<span class="number">25000</span>)</span><br><span class="line">    eval_results = classifier.evaluate(input_fn=eval_input_fn)</span><br><span class="line">    predictions = np.array([p[<span class="string">'logistic'</span>][<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> classifier.predict(input_fn=eval_input_fn)])</span><br><span class="line">    tf.reset_default_graph()</span><br><span class="line">    <span class="comment"># 添加一个 PR summary</span></span><br><span class="line">    pr = summary_lib.pr_curve(</span><br><span class="line">        <span class="string">'precision_recall'</span>, </span><br><span class="line">        predictions=predictions, </span><br><span class="line">        labels=y_test.astype(bool),</span><br><span class="line">        num_threshholds=<span class="number">21</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, <span class="string">'eval'</span>), sess.graph)</span><br><span class="line">        writer.add_summary(sess.run(pr), global_step=<span class="number">0</span>)</span><br><span class="line">        writer.close()</span><br><span class="line"></span><br><span class="line">train_and_evaluate(classifier)</span><br></pre></td></tr></table></figure>
<p>选择简单模型的一个好处是模型解释性更好。模型越复杂，就越难以检查和越像一个「黑箱」（<em>black box</em>）。在这个例子中，我们可以从模型的最新检查点（<em>checkpoint</em>）中载入权重，然后看下绝对值最大的权重对应于哪个词。模型结果正如我们所料：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入模型权重</span></span><br><span class="line">weights = classifier.get_variable_value(<span class="string">'linear/linear_model/x/weights'</span>).flatten()</span><br><span class="line">sorted_indexes = np.argsort(weights)</span><br><span class="line"><span class="comment"># 找到绝对值最大权重</span></span><br><span class="line">extremes = np.concatenate((sorted_indexes[<span class="number">-8</span>:], sorted_indexes[:<span class="number">8</span>]))</span><br><span class="line">extreme_weights = sorted(</span><br><span class="line">    [(weights[i], word_inverted_index[i-index_offset]) <span class="keyword">for</span> i <span class="keyword">in</span> extremes]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">y_pos = np.arange(len(extreme_weights))</span><br><span class="line">plt.bar(y_pos, [pair[<span class="number">0</span>] <span class="keyword">for</span> pair <span class="keyword">in</span> extreme_weights])</span><br><span class="line">plt.xticks(y_pos, [pair[<span class="number">1</span>] <span class="keyword">for</span> pair <span class="keyword">in</span> extreme_weights])</span><br><span class="line">plt.y_label(<span class="string">'Weight)</span></span><br><span class="line"><span class="string">plt.title('</span>Most significant tokens<span class="string">')</span></span><br><span class="line"><span class="string">plt.show()</span></span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/xhXjSdI.png" alt="token weights"></p>
<p>正如图中所看到的，正数权重最大的的几个词例如 ‘refreshing’，和明显与正面情绪有关，而有负数权重的词则是和负面情绪相关。一个简单但是强大的可以改善模型的方法是使用 <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">tf-idf</a> 权重。</p>
<h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>下一个增加模型复杂度的方法就是词嵌入。嵌入是稀疏高维数据的低维稠密表示，这可以使我们的模型对每个词学习到一个更有意义的表示，而不是仅仅一个索引。这个稠密表示一般是从一个足够大的文档集中学习得到，其中每一个单独的维度并没有什么具体的意义，但是已经被证明可以捕捉到诸如时态、复数、性别、主题关系和其他关系。我们可以通过将当前的特征列转为 <code>embedding_column</code> 来加入词嵌入。模型中的表示是每一个词对应的嵌入的平均（参见<a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener">文档</a>中的 <code>combiner</code> 参数）（<em>译者注：应该指的是句子或者文档的嵌入</em>）。我们可以将这个嵌入特征加入到预构建的 <code>DNNClassifier</code> 中。</p>
<p>注意：一个 <code>embedding_column</code> 仅仅是将全连接层应用于词的稀疏二值特征向量的一种高效方式，会乘上一个取决于 <code>combiner</code> 的常数。这样做的直接后果就是如果你直接在 <code>LinearClassifier</code> 中使用 <code>embedding_column</code>，那么这是没有任何意义的，因为连续两个没有非线性激励的线性层对模型的预测没有好处，除非那个嵌入是预训练过的。（<em>译者注：此处翻译感觉不太准确，这里贴上原文：A note for the keen observer: an <code>embedding_column</code> is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn’t make sense to use an <code>embedding_column</code> directly in a <code>LinearClassifier</code> because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.</em>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">embedding_size = <span class="number">50</span></span><br><span class="line">word_embedding_column = tf.feature_column.embedding_column(</span><br><span class="line">    column, dimension=embedding_size</span><br><span class="line">)</span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">    hidden_units=[<span class="number">100</span>],</span><br><span class="line">    feature_columns=[word_embedding_column],</span><br><span class="line">    model_dir=os.path.join(model_dir, <span class="string">'bow_embeddings'</span>)</span><br><span class="line">)</span><br><span class="line">train_and_evaluate(classifier)</span><br></pre></td></tr></table></figure>
<p>我们可以在 TensorBoard 中使用 <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank" rel="noopener">t-SNE</a> 来将 50 维的词向量在三维空间中进行可视化。我们希望语义上相似的词在词向量空间中也是相近的。这对我们检查模型权重和找到异常行为很有用。</p>
<p><img src="https://i.imgur.com/cRfwk5r.gif" alt="tensorboard word-embeddings visualization"></p>
<h2 id="Convolutions"><a href="#Convolutions" class="headerlink" title="Convolutions"></a>Convolutions</h2><p>现在继续提升模型性能的一个方法是继续往深了走，加更多的全连接层，调整每层大小和训练函数。然而这样我们只是增加了模型复杂度，忽略了重要的句子结构。词并不是孤立的，词的意义也是由词本身和其周围的上下文决定的。</p>
<p>卷积就是一种利用这种结构的方式，就像我们在<a href="https://www.tensorflow.org/tutorials/layers" target="_blank" rel="noopener">图像分类</a>中做的一样。直观上来说，无论在句子中的位置如何，词的特定序列或者 n-grams 通常都具有相同的意思。通过卷积操作引入一个结构先验（<em>structural prior</em>）可以使我们为相邻词之间的关系建模，因此也会给我们一个更好的表示。</p>
<p>下图显示了一个大小为 $d \times m$ 的卷积核 $F$ 遍历每一个 3-gram 词窗口从而构建一个新的特征图。然后接上一个池化层（<em>pooling layer</em>）来组合相近的结果。</p>
<p><img src="https://i.imgur.com/Iz34b17.png" alt="convolution"></p>
<center><font color="gray" size="2">来源：<a href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9" target="_blank" rel="noopener">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a> by Severyn et al. [2015]</font></center>

<p>让我们来看下完整的模型结构。Dropout 是一种正则化方法，可以避免模型过拟合。</p>
<p><img src="https://i.imgur.com/KJAPMrv.png" alt="model architecture"></p>
<center><font color="gray" size="2">译者注：由于<a href="https://cdn-images-1.medium.com/max/1200/1*zwj1G4Hem-PX1I54j_9gAw.png" target="_blank" rel="noopener">原图</a>分辨率较低，不太清楚，此图为我对其的重制版</font></center>

<blockquote>
<p>译者注：上图的重制 dot 代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">digraph G&#123;</span><br><span class="line">    rankdir=LR</span><br><span class="line">    node [shape=box, style=filled, fillcolor=&quot;.7 .3 1.0&quot;]</span><br><span class="line">    &quot;Embedding Layer&quot; -&gt; Droupout -&gt; Convolutuin1D -&gt; GlobalMaxPooling -&gt; &quot;Hidden Dense Layer&quot; -&gt; Dropout -&gt; &quot;Output Layer&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>正如前面的博文所说，<code>tf.estimator</code> 框架为训练模型提供了一个高层 API，定义了 <code>train()</code>、<code>evaluate()</code> 和 <code>predict()</code> 操作，能够自动处理检查点、载入、初始化、部署服务、构建计算图和会话。<code>tf.estimator</code> 已经提供了一小部分的预构建 Estimators，就像我们前面所用到的，但是大多数情况下你都需要<a href="https://www.tensorflow.org/extend/estimators" target="_blank" rel="noopener">自己去构建</a>。</p>
<p>写一个自定义的 Estimator 就意味着写一个能够返回一个 <code>EstimatorSpec</code> 的 <code>model_fn(features, labels, mode, params)</code> 函数。第一步就是将特征映射到我们的嵌入层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input_layer = tf.contrib.layers.embed_sequence(</span><br><span class="line">    features[<span class="string">'x'</span>],</span><br><span class="line">    vocab_size,</span><br><span class="line">    embedding_size,</span><br><span class="line">    initializer=params[<span class="string">'embedding_initializer'</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后使用 <code>tf.layers</code> 来构建各层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line">dropout_emb = tf.layers.dropout(</span><br><span class="line">    inputs=input_layer,</span><br><span class="line">    rate=<span class="number">0.2</span>,</span><br><span class="line">    training=training</span><br><span class="line">)</span><br><span class="line">conv = tf.layers.con1d(</span><br><span class="line">    inputs=dropout_emb,</span><br><span class="line">    filters=<span class="number">32</span>,</span><br><span class="line">    kernel_size=<span class="number">3</span>,</span><br><span class="line">    padding=<span class="string">'same'</span>,</span><br><span class="line">    activation=tf.nn.relu</span><br><span class="line">)</span><br><span class="line">pool = tf.reduce_max(input_tensor=conv, axis=<span class="number">1</span>)</span><br><span class="line">hidden = tf.layers.dense(</span><br><span class="line">    inputs=pool,</span><br><span class="line">    units=<span class="number">250</span>,</span><br><span class="line">    activation=tf.nn.relu</span><br><span class="line">)</span><br><span class="line">dropout_hidden = tf.layers.dropout(</span><br><span class="line">    inputs=hidden,</span><br><span class="line">    rate=<span class="number">0.2</span>,</span><br><span class="line">    training=training</span><br><span class="line">)</span><br><span class="line">logits = tf.layers.dense(</span><br><span class="line">    inputs=dropout_hidden,</span><br><span class="line">    units=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>最后，我们将使用一个 <code>Head</code> 来简化 <code>model_fn</code> 的编写。这个 head 已经知道如何计算预测值、损失、train_op、评估指标以及导出输出，而且可以被多个模型重用。这种用法在预构建的 Estimator 中也在使用，而且可以在我们所有的模型之间提供一个统一的评估函数。我们将会使用 <code>binary_classification_head</code> 这个用于单标签二分类的 head，使用 <code>sigmoid_cross_entropy_with_logits</code> 作为损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cnn_model_fn</span><span class="params">(features, labels, mode, params)</span>:</span></span><br><span class="line">    <span class="comment"># 一些其他代码</span></span><br><span class="line">    head = tf.contrib.estimator.binary_classification_head()</span><br><span class="line">    optimizer = tf.train.AdamOptimizer()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train_op_fn</span><span class="params">(loss)</span>:</span></span><br><span class="line">        tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line">        <span class="keyword">return</span> optimizer.minimize(</span><br><span class="line">            loss=loss,</span><br><span class="line">            global_step=tf.train.get_global_step()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> head.create_estimator_spec(</span><br><span class="line">        features=features,</span><br><span class="line">        labels=labels,</span><br><span class="line">        mode=mode,</span><br><span class="line">        logits=logits,</span><br><span class="line">        train_op_fn=_train_op_fn</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>最后就像之前一样运行这个模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">initializer = tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">params = &#123;<span class="string">'embedding_initializer'</span>: initializer&#125;</span><br><span class="line">cnn_classifier = tf.estimator.Estimator(</span><br><span class="line">    model_fn=model_fn,</span><br><span class="line">    model_dir=os.path.join(model_dir, <span class="string">'cnn'</span>),</span><br><span class="line">    params=params</span><br><span class="line">)</span><br><span class="line">train_and_evaluate(cnn_classifier)</span><br></pre></td></tr></table></figure>
<h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>使用 <code>Estimator</code> API 和相同的模型 <code>head</code>，我们还可以创建一个使用长短时记忆网络（LSTM）而不是卷积的分类器。诸如此类的循环模型（<em>recurrent models</em>）在 NLP 应用中是一种最成功的构建模块。一个 LSTM 顺序处理整篇文档，使用其内部的单元（<em>cell</em>）对序列进行递归，同时将序列的当前状态存储在内存中。</p>
<p>由于循环模型的递归性质，相比 CNN，模型会变得更深和更复杂，这也会导致训练时间变长和更差的收敛，这也是循环模型的一个缺点。LSTMs（和一般的 RNNs）可能会遇到例如梯度消失和梯度爆炸等收敛问题，但是通过有效的调整，也可以使他们在许多问题上得到最优的结果。一般来说，CNNs 擅于进行特征提取，而 RNNs 则在那些模型效果取决于整个句子意义的任务上比较有优势，比如问答和机器翻译。</p>
<p>每一个单元一次处理一个词嵌入，然后根据在时间点 $t$ 的嵌入向量 $x$ 和前一个在时间点 $t-1$ 的状态 $h$ 进行一个可微分计算（<em>differential computation</em>），然后更新其内部状态。为了能够更好地理解 LSTMs 是怎么工作的，你可以参见 Chris Olah 的<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">博文</a>。（<em>译者注：我之前将这篇博文翻译成了中文，可以参见<a href="https://alanlee.fun/2017/12/29/understanding-lstms/" target="_blank" rel="noopener">理解 LSTM 网络</a></em>）。</p>
<p><img src="https://i.imgur.com/DP4o5jc.png" alt="LSTM cell"></p>
<center><font color="gray" size="2">来源：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a> by Chris Olah</font></center>

<p>整个 LSTM 模型可以如简单的流程图：</p>
<p><img src="https://i.imgur.com/K6fIRvT.png" alt="LSTM model"></p>
<center><font color="gray" size="2">译者注：由于<a href="https://cdn-images-1.medium.com/max/1200/1*y3w54qFX7D2P0I4FKHUddQ.png" target="_blank" rel="noopener">原图</a>分辨率和大小问题，此图为我对其的重制版</font></center>

<blockquote>
<p>译者注：上图的重制 dot 代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">digraph G&#123;</span><br><span class="line">    rankdir=LR</span><br><span class="line">    node [shape=box, style=filled, fillcolor=&quot;.7 .3 1.0&quot;]</span><br><span class="line">    &quot;Embedding Layer&quot; -&gt;  &quot;LSTM Cell&quot; -&gt; &quot;Output Layer&quot;;</span><br><span class="line">    &quot;LSTM Cell&quot; -&gt; &quot;LSTM Cell&quot; [label=&quot;Recurison&quot;];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>在本文开头，我们将所有文档都裁剪成 200 个词，这对于构建一个合适的 tensor 是必要的。然而当一篇文档少于 200 个词的时候，我们不希望 LSTM 继续去补充长度到 200，因为这不仅不会增加任何有用信而且还会降低性能。因此，我们想要额外给网络提供句子的原本长度。在内部，模型然后把最新的状态复制到序列的末尾。这可以通过使用输入函数中的 <code>&quot;len&quot;</code> 特征实现。我们现在可以使用和上面相同的逻辑，简单地用 LSTM 单元替换掉卷积、池化和拉平层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lsmt_cell = tf.nn.rnn_cell.BasicLSTMCell(<span class="number">100</span>)</span><br><span class="line">_, final_states = tf.nn.dynamic_rnn(</span><br><span class="line">    lstm_cell,</span><br><span class="line">    inputs,</span><br><span class="line">    sequence_length=features[<span class="string">'len'</span>],</span><br><span class="line">    dtype=tf.float32</span><br><span class="line">)</span><br><span class="line">logits = tf.layers.dense(</span><br><span class="line">    inputs=final_states.h,</span><br><span class="line">    units=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="Pre-trained-vectors"><a href="#Pre-trained-vectors" class="headerlink" title="Pre-trained vectors"></a>Pre-trained vectors</h2><p>我们前面所展示的大多数模型都是使用词嵌入作为第一层。到目前为止，我们都是随机初始化这个嵌入层。然后，<a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">很多先前的工作</a>已经表明使用在一个很大的无标签的词库中预训练的嵌入是很有好处的，尤其是当要在一个小的有标签的数据集上训练时。最流行的预训练嵌入是 <a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">word2vec</a>。通过从预训练嵌入来利用无标签数据中的知识是<a href="http://ruder.io/transfer-learning/" target="_blank" rel="noopener">迁移学习</a>的一个例子。</p>
<p>现在我们将为你展示如何在一个 <code>Estimator</code> 中使用他们。我们将会使用来自另外一个流行模型 <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> 的预训练词嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">embeddings = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'glove.6B.50d.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    values = line.strip().split()</span><br><span class="line">    w = values[<span class="number">0</span>]</span><br><span class="line">    vectors = np.asarray(values[:<span class="number">1</span>], dtype=<span class="string">'float32'</span>)</span><br><span class="line">    embeddings[w] = vectors</span><br></pre></td></tr></table></figure>
<p>在从一个文件中载入词向量到内存后，我们使用和词汇表中相同的索引来将其存储为一个 <code>numpy.array</code>，这个数组大小为 <code>(5000, 50)</code>。每一行中都包含一个 50 维的词向量，行索引对应于该词在词汇表中的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">embedding_matrix = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=(vocab_size, embedding_size))</span><br><span class="line"><span class="keyword">for</span> w, i <span class="keyword">in</span> word_index.items():</span><br><span class="line">    v = embeddings.get(w)</span><br><span class="line">    <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> i &lt; vocab_size:</span><br><span class="line">    embedding_matrix[i] = v</span><br></pre></td></tr></table></figure>
<p>最后，我们可以使用一个自定义的初始化函数，然后将其通过 <code>params</code> 对象传给我们的 <code>cnn_model_fn</code>，不用改动其他的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_initializer</span><span class="params">(shape=None, dtype=tf.float32, partition_info=None)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> dtype <span class="keyword">is</span> tf.float32</span><br><span class="line">    <span class="keyword">return</span> embedding_matrix</span><br><span class="line"></span><br><span class="line">params = &#123;<span class="string">'embedding_initialize'</span>: my_initializer&#125;</span><br><span class="line">cnn_pretrained_classifier = tf.estimator.Estimator(</span><br><span class="line">    model_fn=cnn_model_fn,</span><br><span class="line">    model_dir=os.path.join(model_dir, <span class="string">'cnn_pretrained'</span>),</span><br><span class="line">    params=params</span><br><span class="line">)</span><br><span class="line">train_and_evaluate(cnn_pretrained_classifier)</span><br></pre></td></tr></table></figure>
<h2 id="Running-TensorBoard"><a href="#Running-TensorBoard" class="headerlink" title="Running TensorBoard"></a>Running TensorBoard</h2><p>现在我们可以启动 TensorBoard，看下我们刚训练的几个模型在训练时间和性能上有何不同。</p>
<p>在终端输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; tensorboard --logdir=&#123;model_dir&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到在训练和测试时收集的许多评估指标，包括每个模型在每个训练步骤的损失函数值和 PR 曲线。这对于我们选择性能最好的模型和分类阈值很有帮助。</p>
<p><img src="https://i.imgur.com/tw9TzgZ.png" alt="pr curves"></p>
<center><font color="gray" size="2">每个模型在测试集上的 PR 曲线</font></center>

<p><img src="https://i.imgur.com/zbApsYm.png" alt="loss"></p>
<center><font color="gray" size="2">训练损失与训练步数</font></center>

<h2 id="Getting-Predictions"><a href="#Getting-Predictions" class="headerlink" title="Getting Predictions"></a>Getting Predictions</h2><p>为了得到对新句子的预测值，我们可以使用 <code>Estimator</code> 的 <code>predict</code> 方法，这个方法会载入每个模型的最新检查点然后执行评估。但是在将数据送入给模型之前我们需要先清洗一下，分词然后将每个词映射到相应的索引：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_to_index</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="comment"># 去除标点符号，除了 '</span></span><br><span class="line">    translator = str.maketrans(<span class="string">''</span>, <span class="string">' '</span>, string.punctuation.replace(<span class="string">"'"</span>, <span class="string">''</span>))</span><br><span class="line">    tokens = sentence.translate(translator).lower().split()</span><br><span class="line">    <span class="keyword">return</span> np.array([<span class="number">1</span>] + [word_index[t] <span class="keyword">if</span> t <span class="keyword">in</span> word_index <span class="keyword">else</span> <span class="number">2</span> <span class="keyword">for</span> t <span class="keyword">in</span> tokens])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_predictions</span><span class="params">(sentences, classifier)</span>:</span></span><br><span class="line">    indexes = [text_to_index(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    x = sequence.pad_sequences(</span><br><span class="line">        indexes,</span><br><span class="line">        maxlen=sentence_size,</span><br><span class="line">        padding=<span class="string">'post'</span>,</span><br><span class="line">        value=<span class="number">0</span></span><br><span class="line">    )</span><br><span class="line">    length = np.array([min(len(x), sentence_size) <span class="keyword">for</span> x <span class="keyword">in</span> indexes])</span><br><span class="line">    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x"</span>: x, <span class="string">"len"</span>: length&#125;, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    predictions = [p[<span class="string">'logistic'</span>][<span class="number">0</span>] <span class="keyword">for</span> p <span class="keyword">in</span> classifier.predict(input_fn=predict_input_fn)]</span><br><span class="line">    print(predictions)</span><br></pre></td></tr></table></figure>
<p>需要注意的是检查点本身并不足以进行预测，用于构建 Estimator 的代码也是必需的，需要用这段代码将保存的权重和相应的 tensors 对应上。将保存的检查点和相应的代码联系起来是一种很好地做法。</p>
<p>如果你有兴趣以完全可恢复的方式将模型导出到硬盘上，你可能需要查看 <a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators" target="_blank" rel="noopener">SavedModel</a> 类，这对于使用 <a href="https://github.com/tensorflow/serving" target="_blank" rel="noopener">TensorFlow Serving</a> 通过 API 来提供模型特别有用。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>本篇博文中，我们探索了如何使用 Estimators 在 IMDB Reviews Dataset 上进行文本分类。我们训练和可视化了我们自己的嵌入和载入的预训练的词嵌入。我们从最简单的模型开始逐渐扩展到卷积神经网络和 LSTMs。</p>
<p>对于更多细节，你可以查看以下内容：</p>
<ul>
<li>一个 可以在本地或者 Colaboratory 上运行的 <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" target="_blank" rel="noopener">Jupyter Notebook</a></li>
<li>本文的完整的<a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" target="_blank" rel="noopener">源代码</a></li>
<li>TensorFlow 关于<a href="https://www.tensorflow.org/programmers_guide/embedding" target="_blank" rel="noopener">嵌入</a>的指导</li>
<li>TensorFlow 关于 <a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">Vector Representation of Words</a> 的教程</li>
<li>NLTK 中关于如何设计语言处理 pipelines 的章节 <a href="http://www.nltk.org/book/ch03.html" target="_blank" rel="noopener">Processing Raw Text</a></li>
</ul>
<p><em>Thanks for reading！</em></p>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>原文作者: <a href="https://secsilm.github.io">Alan Lee</a>
            <p>原文链接: <a href="https://secsilm.github.io/2018/07/18/text-classification-with-tensorflow-estimator/">https://secsilm.github.io/2018/07/18/text-classification-with-tensorflow-estimator/</a>
            <p>发表日期: <a href="https://secsilm.github.io/2018/07/18/text-classification-with-tensorflow-estimator/">July 18th 2018, 6:57:19 pm</a>
            <p>版权声明: 本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2018/07/14/remote-jupyter/" title= 在本地使用远程 Jupyter Lab 服务器 >
                    <div class="prevTitle">在本地使用远程 Jupyter Lab 服务器</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    <div id="disqus_thread"></div>
    <script>
        /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        
        var disqus_config = function () {
        this.page.url = "https://secsilm.github.io/2018/07/18/text-classification-with-tensorflow-estimator/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "使用 TensorFlow Estimators 进行文本分类"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        
        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://secsilm.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();

    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

    
    <!--PC和WAP自适应版-->

    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:secsilm@outlook.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/secsilm" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <a href="https://twitter.com/SilverSecondMan" class="iconfont-archer twitter" target="_blank" title=twitter></a>
            
        
    
        
            
                <a href="https://www.instagram.com/secsilm/" class="iconfont-archer instagram" target="_blank" title=instagram></a>
            
        
    
        
            
                <a href="https://www.v2ex.com/member/secsilm" class="iconfont-archer v2ex" target="_blank" title=v2ex></a>
            
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title=rss></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">本站已被浏览 <span id="busuanzi_value_site_pv"></span> 次</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Task"><span class="toc-number">1.</span> <span class="toc-text">The Task</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Input-Functions"><span class="toc-number">2.</span> <span class="toc-text">Input Functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Building-a-baseline"><span class="toc-number">3.</span> <span class="toc-text">Building a baseline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embeddings"><span class="toc-number">4.</span> <span class="toc-text">Embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Convolutions"><span class="toc-number">5.</span> <span class="toc-text">Convolutions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-Networks"><span class="toc-number">6.</span> <span class="toc-text">LSTM Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pre-trained-vectors"><span class="toc-number">7.</span> <span class="toc-text">Pre-trained vectors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Running-TensorBoard"><span class="toc-number">8.</span> <span class="toc-text">Running TensorBoard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Getting-Predictions"><span class="toc-number">9.</span> <span class="toc-text">Getting Predictions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">10.</span> <span class="toc-text">Summary</span></a></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 54
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/18</span><a class="archive-post-title" href= "/2018/07/18/text-classification-with-tensorflow-estimator/" >使用 TensorFlow Estimators 进行文本分类</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/14</span><a class="archive-post-title" href= "/2018/07/14/remote-jupyter/" >在本地使用远程 Jupyter Lab 服务器</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span><a class="archive-post-title" href= "/2018/06/04/understanding-objective-functions-in-neural-networks/" >理解神经网络中的目标函数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/18</span><a class="archive-post-title" href= "/2018/05/18/ubuntu-ssr/" >在 Ubuntu 上使用 SSR 梯子</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/28</span><a class="archive-post-title" href= "/2018/04/28/analyzing-python-survey-2017/" >2017 Python 问卷调查结果初步分析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/05</span><a class="archive-post-title" href= "/2018/04/05/unzip-chinese/" >使用 zipfile 解压含有中文文件名的 zip 文件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span><a class="archive-post-title" href= "/2018/03/15/embed-bokeh-plot/" >嵌入 bokeh 绘图到 hexo 博客中</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/14</span><a class="archive-post-title" href= "/2018/03/14/stephen-hawking/" >曾经我也有一位 Stephen Hawking</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/09</span><a class="archive-post-title" href= "/2018/03/09/using-https-namecheap-hexo/" >使用 CloudFlare 为 hexo 博客实现 HTTPS</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/03</span><a class="archive-post-title" href= "/2018/01/03/neteasemusic-year-report-2017/" >我的网易云音乐 2017 年度听歌报告</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/30</span><a class="archive-post-title" href= "/2017/12/30/google-sitemap/" >Google sitemap 不允许的网址的解决办法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/29</span><a class="archive-post-title" href= "/2017/12/29/understanding-lstms/" >理解 LSTM 网络</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/25</span><a class="archive-post-title" href= "/2017/12/25/export-csdn-blogs-to-md/" >批量导出 CSDN 博客为 Markdown 文件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span><a class="archive-post-title" href= "/2017/12/22/understanding-estimators-datasets/" >理解 Estimators 和 Datasets</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/26</span><a class="archive-post-title" href= "/2017/10/26/windows10-dark-theme/" >Windows 10 资源管理器黑色风格</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/08</span><a class="archive-post-title" href= "/2017/10/08/gradient-descent-methods/" >梯度下降优化算法概述</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span><a class="archive-post-title" href= "/2017/08/30/ensemble-methods/" >使用集成学习提升机器学习算法性能</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/20</span><a class="archive-post-title" href= "/2017/08/20/understanding-tensorboard/" >理解 TensorBoard</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/17</span><a class="archive-post-title" href= "/2017/06/17/numpy-shuffle-permutation/" >Numpy 中的 shuffle VS permutation</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2017/06/02/tensorflow-DNNRegressor/" >TensorFlow DNNRegressor 的简单使用</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/18</span><a class="archive-post-title" href= "/2017/05/18/installing-xgboost/" >XGBoost 在 Windows 10 和 Ubuntu 上的安装</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/22</span><a class="archive-post-title" href= "/2017/04/22/python-fire/" >Python 自动生成命令行工具 - fire 简介</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/29</span><a class="archive-post-title" href= "/2017/03/29/understanding-svd/" >奇异值分解 SVD 的数学解释</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/2017/03/25/python-string-count/" >使用 Python 统计字符串中英文、空格、数字、标点个数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/17</span><a class="archive-post-title" href= "/2017/03/17/tensorflow-cnn-tensorboard/" >TensorFlow 中的卷积神经网络 CNN - TensorBoard 版</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span><a class="archive-post-title" href= "/2017/03/15/using-tree/" >使用 tree 命令格式化输出目录结构</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/2017/03/11/vscode-pdf-error/" >VSCode Markdown PDF 导出成PDF报 phantomjs binary does not exist 错误的解决办法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span><a class="archive-post-title" href= "/2017/03/01/numpy-copy/" >Numpy 中的 copy 问题详解</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/26</span><a class="archive-post-title" href= "/2017/02/26/tensorflow-cudnn-version/" >Check failed stream->parent()->GetConvolveAlgorithms(&algorithms)解决办法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/16</span><a class="archive-post-title" href= "/2017/02/16/tensorflow-1.0/" >TensorFlow 1.0 发布</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/28</span><a class="archive-post-title" href= "/2016/12/28/tensorflow-cnn-no-tensorboard/" >TensorFlow 中的卷积神经网络 CNN - 无TensorBoard版</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/10</span><a class="archive-post-title" href= "/2016/12/10/tensorboard-mnist-pca/" >在 TensorBoard 中用 PCA 可视化 MNIST 手写数字识别数据集</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span><a class="archive-post-title" href= "/2016/12/01/installing-tensorflow/" >Windows10 64 位下安装 TensorFlow - 官方原生支持</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/30</span><a class="archive-post-title" href= "/2016/11/30/numpy-array-memory/" >小谈 Numpy 数组占用内存空间问题</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/19</span><a class="archive-post-title" href= "/2016/11/19/tensorflow-mlp/" >TensorFlow 中的多层感知器（MLP）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/06</span><a class="archive-post-title" href= "/2016/11/06/tensorflow-logistic-regression/" >TensorFlow 中的 Logistic Regression</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2016/11/03/pandas-apply/" >Pandas 中的 apply 函数使用示例</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/29</span><a class="archive-post-title" href= "/2016/10/29/tensorflow-hyperparams/" >学习率、迭代次数和初始化方式对模型准确率的影响</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/26</span><a class="archive-post-title" href= "/2016/10/26/sklearn-logistics-regression-parameters/" >sklearn 中 Logistics Regression 的 coef_ 和 intercept_ 的具体意义</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/24</span><a class="archive-post-title" href= "/2016/10/24/plt-save/" >解决使用 plt.savefig 保存图片时一片空白</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/22</span><a class="archive-post-title" href= "/2016/10/22/tensorflow-linear-regression/" >TensorFlow 中的线性回归</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/16</span><a class="archive-post-title" href= "/2016/10/16/python-email/" >用 Python 发电子邮件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/27</span><a class="archive-post-title" href= "/2016/08/27/ubuntu-installing-tensorflow/" >Ubuntu 14.04 64 位安装 Google 的 TensorFlow</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/22</span><a class="archive-post-title" href= "/2016/08/22/ubuntu-update-issue/" >Ubuntu 14.04 64 位系统更新重启后无法进入系统，光标不停闪烁</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/27</span><a class="archive-post-title" href= "/2016/06/27/python-strptime/" >Python 中 strptime 的简单使用</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/03</span><a class="archive-post-title" href= "/2016/04/03/python-numpy/" >Python NumPy 基础</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/24</span><a class="archive-post-title" href= "/2016/03/24/ubuntu-gedit-change-theme/" >Ubuntu14.04 gnome3 下 gedit 首选项消失时如何修改 gedit 主题</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/18</span><a class="archive-post-title" href= "/2016/01/18/matlab-plot-parallelepiped/" >MATLAB绘制平行六面体</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span><a class="archive-post-title" href= "/2016/01/11/matlab-mat2cell/" >MATLAB 矩阵分块函数 mat2cell 及 cellfun 函数</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/11</span><a class="archive-post-title" href= "/2016/01/11/machine-learning-book-note/" >《机器学习》学习笔记1——绪论 机器学习概述</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2015 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/21</span><a class="archive-post-title" href= "/2015/12/21/matlab-fitlm/" >使用 MATLAB 的 fitlm 函数进行线性回归</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/19</span><a class="archive-post-title" href= "/2015/11/19/matlab-rename-files/" >MATLAB 批量文件重命名（详细解释）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/05</span><a class="archive-post-title" href= "/2015/11/05/matlab-callback/" >MATLAB GUI 中 Edit Text 的 Callback 函数何时执行</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2015/09/22/matlab-move-files/" >用 MATLAB 将多个文件夹内的某些文件汇总到另一个文件夹</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="Python"><span class="iconfont-archer">&#xe606;</span>Python</span>
    
        <span class="sidebar-tag-name" data-tags="Data Science"><span class="iconfont-archer">&#xe606;</span>Data Science</span>
    
        <span class="sidebar-tag-name" data-tags="Machine Learning"><span class="iconfont-archer">&#xe606;</span>Machine Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Translation"><span class="iconfont-archer">&#xe606;</span>Translation</span>
    
        <span class="sidebar-tag-name" data-tags="hexo"><span class="iconfont-archer">&#xe606;</span>hexo</span>
    
        <span class="sidebar-tag-name" data-tags="TensorFlow"><span class="iconfont-archer">&#xe606;</span>TensorFlow</span>
    
        <span class="sidebar-tag-name" data-tags="MATLAB"><span class="iconfont-archer">&#xe606;</span>MATLAB</span>
    
        <span class="sidebar-tag-name" data-tags="Living"><span class="iconfont-archer">&#xe606;</span>Living</span>
    
        <span class="sidebar-tag-name" data-tags="sklearn"><span class="iconfont-archer">&#xe606;</span>sklearn</span>
    
        <span class="sidebar-tag-name" data-tags="NLP"><span class="iconfont-archer">&#xe606;</span>NLP</span>
    
        <span class="sidebar-tag-name" data-tags="Ubuntu"><span class="iconfont-archer">&#xe606;</span>Ubuntu</span>
    
        <span class="sidebar-tag-name" data-tags="IDE"><span class="iconfont-archer">&#xe606;</span>IDE</span>
    
        <span class="sidebar-tag-name" data-tags="Windows"><span class="iconfont-archer">&#xe606;</span>Windows</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Alan Lee"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


